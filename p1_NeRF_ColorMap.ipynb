{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R12942159/NTU_DLCV/blob/Hw4/p1_NeRF_ColorMap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQosyvi9_SUi"
      },
      "outputs": [],
      "source": [
        "!gdown 1hF4z9U-xaoV4qaq9DbhTP-KKJTlwOUv_ -O hw4_data.zip\n",
        "!unzip hw4_data.zip\n",
        "!rm hw4_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1a-1prjFoFaY_ZK4KriFCKWcqwkdDt5tW -O hw4_gt.zip\n",
        "!unzip hw4_gt.zip\n",
        "!rm hw4_gt.zip"
      ],
      "metadata": {
        "id": "rp2Or8hcyBPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kornia"
      ],
      "metadata": {
        "id": "W92bIA_qcnUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6PyjHE8SwNC",
        "outputId": "c15f85fc-4cc5-417c-9fb3-60e484d25996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NeRF_pl"
      ],
      "metadata": {
        "id": "g1rArMiy0uM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recursive https://github.com/kwea123/nerf_pl"
      ],
      "metadata": {
        "id": "ljGMzAXH0wQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### blender.py"
      ],
      "metadata": {
        "id": "ofIMmxXMGFQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "import random\n",
        "from kornia import create_meshgrid\n",
        "from scipy.spatial import transform\n",
        "\n",
        "\n",
        "def calculate_near_and_far(rays_o, rays_d, bbox_min=[-1.,-1.,-1.], bbox_max=[1.,1.,1.]):\n",
        "    '''\n",
        "    rays_o, (len(self.split_ids)*h*w, 3)\n",
        "    rays_d, (len(self.split_ids)*h*w, 3)\n",
        "    bbox_min=[-1,-1,-1],\n",
        "    bbox_max=[1,1,1]\n",
        "    '''\n",
        "    # map all shape to same (len(self.split_ids)*h*w, 3, 2)\n",
        "    corners = torch.stack((torch.tensor(bbox_min),torch.tensor(bbox_max)), dim=-1)\n",
        "    corners = corners.unsqueeze(0).repeat(rays_o.shape[0],1,1) # (len(self.split_ids)*h*w, 3, 2)\n",
        "    corners -= torch.unsqueeze(rays_o, -1).repeat(1,1,2)\n",
        "    intersections = (corners / (torch.unsqueeze(rays_d, -1).repeat(1,1,2)))\n",
        "\n",
        "    min_intersections = torch.amax(torch.amin(intersections, dim=-1), dim=-1, keepdim=True)\n",
        "    max_intersections = torch.amin(torch.amax(intersections, dim=-1), dim=-1, keepdim=True)\n",
        "    epsilon = 1e-10*torch.ones_like(min_intersections)\n",
        "    near = torch.maximum(epsilon, min_intersections)\n",
        "    # tmp = near\n",
        "    near = torch.where((near > max_intersections), epsilon, near)\n",
        "    far = torch.where(near < max_intersections, max_intersections, near+epsilon)\n",
        "\n",
        "    return near, far\n",
        "\n",
        "def get_ray_directions(H, W, focal):\n",
        "    \"\"\"\n",
        "    Get ray directions for all pixels in camera coordinate.\n",
        "    Reference: https://www.scratchapixel.com/lessons/3d-basic-rendering/\n",
        "               ray-tracing-generating-camera-rays/standard-coordinate-systems\n",
        "\n",
        "    Inputs:\n",
        "        H, W, focal: image height, width and focal length\n",
        "\n",
        "    Outputs:\n",
        "        directions: (H, W, 3), the direction of the rays in camera coordinate\n",
        "    \"\"\"\n",
        "    grid = create_meshgrid(H, W, normalized_coordinates=False)[0]\n",
        "    i, j = grid.unbind(-1)\n",
        "    # the direction here is without +0.5 pixel centering as calibration is not so accurate\n",
        "    # see https://github.com/bmild/nerf/issues/24\n",
        "    directions = \\\n",
        "        torch.stack([(i-W/2)/focal, -(j-H/2)/focal, -torch.ones_like(i)], -1) # (H, W, 3)\n",
        "\n",
        "    return directions\n",
        "\n",
        "def get_rays(directions, c2w):\n",
        "    \"\"\"\n",
        "    Get ray origin and normalized directions in world coordinate for all pixels in one image.\n",
        "    Reference: https://www.scratchapixel.com/lessons/3d-basic-rendering/\n",
        "               ray-tracing-generating-camera-rays/standard-coordinate-systems\n",
        "\n",
        "    Inputs:\n",
        "        directions: (H, W, 3) precomputed ray directions in camera coordinate\n",
        "        c2w: (3, 4) transformation matrix from camera coordinate to world coordinate\n",
        "\n",
        "    Outputs:\n",
        "        rays_o: (H*W, 3), the origin of the rays in world coordinate\n",
        "        rays_d: (H*W, 3), the normalized direction of the rays in world coordinate\n",
        "    \"\"\"\n",
        "    # Rotate ray directions from camera coordinate to the world coordinate\n",
        "    rays_d = directions @ c2w[:, :3].T # (H, W, 3)\n",
        "    rays_d = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
        "    # The origin of all rays is the camera origin in world coordinate\n",
        "    rays_o = c2w[:, 3].expand(rays_d.shape) # (H, W, 3)\n",
        "\n",
        "    rays_d = rays_d.view(-1, 3)\n",
        "    rays_o = rays_o.view(-1, 3)\n",
        "\n",
        "    return rays_o, rays_d\n",
        "\n",
        "def blender_quat2rot(quaternion):\n",
        "    \"\"\"Convert quaternion to rotation matrix.\n",
        "    Equivalent to, but support batched case:\n",
        "    ```python\n",
        "    rot3x3 = mathutils.Quaternion(quaternion).to_matrix()\n",
        "    ```\n",
        "    Args:\n",
        "    quaternion:\n",
        "    Returns:\n",
        "    rotation matrix\n",
        "    \"\"\"\n",
        "\n",
        "    # Note: Blender first cast to double values for numerical precision while\n",
        "    # we're using float32.\n",
        "    q = np.sqrt(2) * quaternion\n",
        "\n",
        "    q0 = q[..., 0]\n",
        "    q1 = q[..., 1]\n",
        "    q2 = q[..., 2]\n",
        "    q3 = q[..., 3]\n",
        "\n",
        "    qda = q0 * q1\n",
        "    qdb = q0 * q2\n",
        "    qdc = q0 * q3\n",
        "    qaa = q1 * q1\n",
        "    qab = q1 * q2\n",
        "    qac = q1 * q3\n",
        "    qbb = q2 * q2\n",
        "    qbc = q2 * q3\n",
        "    qcc = q3 * q3\n",
        "\n",
        "    # Note: idx are inverted as blender and numpy convensions do not\n",
        "    # match (x, y) -> (y, x)\n",
        "    rotation = np.empty((*quaternion.shape[:-1], 3, 3), dtype=np.float32)\n",
        "    rotation[..., 0, 0] = 1.0 - qbb - qcc\n",
        "    rotation[..., 1, 0] = qdc + qab\n",
        "    rotation[..., 2, 0] = -qdb + qac\n",
        "\n",
        "    rotation[..., 0, 1] = -qdc + qab\n",
        "    rotation[..., 1, 1] = 1.0 - qaa - qcc\n",
        "    rotation[..., 2, 1] = qda + qbc\n",
        "\n",
        "    rotation[..., 0, 2] = qdb + qac\n",
        "    rotation[..., 1, 2] = -qda + qbc\n",
        "    rotation[..., 2, 2] = 1.0 - qaa - qbb\n",
        "    return rotation\n",
        "\n",
        "def make_transform_matrix(positions,rotations,):\n",
        "    \"\"\"Create the 4x4 transformation matrix.\n",
        "    Note: This function uses numpy.\n",
        "    Args:\n",
        "    positions: Translation applied after the rotation.\n",
        "        Last column of the transformation matrix\n",
        "    rotations: Rotation. Top-left 3x3 matrix of the transformation matrix.\n",
        "    Returns:\n",
        "    transformation_matrix:\n",
        "    \"\"\"\n",
        "    # Create the 4x4 transformation matrix\n",
        "    rot_pos = np.broadcast_to(np.eye(4), (*positions.shape[:-1], 4, 4)).copy()\n",
        "    rot_pos[..., :3, :3] = rotations\n",
        "    rot_pos[..., :3, 3] = positions\n",
        "    return rot_pos\n",
        "\n",
        "def from_position_and_quaternion(positions, quaternions, use_unreal_axes):\n",
        "    if use_unreal_axes:\n",
        "        rotations = transform.Rotation.from_quat(quaternions).as_matrix()\n",
        "    else:\n",
        "        # Rotation matrix that rotates from world to object coordinates.\n",
        "        # Warning: Rotations should be given in blender convensions as\n",
        "        # scipy.transform uses different convensions.\n",
        "        rotations = blender_quat2rot(quaternions)\n",
        "    px2world_transform = make_transform_matrix(positions=positions,rotations=rotations)\n",
        "    return px2world_transform\n",
        "\n",
        "def scale_rays(all_rays_o, all_rays_d, scene_boundaries, img_wh):\n",
        "    \"\"\"Rescale scene boundaries.\n",
        "    rays_o: (len(image_paths)*h*w, 3)\n",
        "    rays_d: (len(image_paths)*h*w, 3)\n",
        "    scene_boundaries: np.array(2 ,3), [min, max]\n",
        "    img_wh: (2)\n",
        "    \"\"\"\n",
        "    # Rescale (x, y, z) from [min, max] -> [-1, 1]\n",
        "    all_rays_o = all_rays_o.reshape(-1, img_wh[0], img_wh[1], 3) # (len(image_paths), h, w, 3))\n",
        "    all_rays_d = all_rays_d.reshape(-1, img_wh[0], img_wh[1], 3)\n",
        "\n",
        "    old_min = torch.from_numpy(scene_boundaries[0])\n",
        "    old_max = torch.from_numpy(scene_boundaries[1])\n",
        "    new_min = torch.tensor([-1,-1,-1])\n",
        "    new_max = torch.tensor([1,1,1])\n",
        "    # scale = max(scene_boundaries[1] - scene_boundaries[0])/2\n",
        "    # all_rays_o = (all_rays_o - torch.mean(all_rays_o, dim=-1, keepdim=True)) / scale\n",
        "    # This is from jax3d.interp, kind of weird but true\n",
        "    all_rays_o = ((new_min - new_max) / (old_min - old_max))*all_rays_o + (old_min * new_max - new_min * old_max) / (old_min - old_max)\n",
        "\n",
        "    # We also need to rescale the camera direction by bbox.size.\n",
        "    # The direction can be though of a ray from a point in space (the camera\n",
        "    # origin) to another point in space (say the red light on the lego\n",
        "    # bulldozer). When we scale the scene in a certain way, this direction\n",
        "    # also needs to be scaled in the same way.\n",
        "    all_rays_d = all_rays_d * 2 / (scene_boundaries[1] - scene_boundaries[0])\n",
        "    # (re)-normalize the rays\n",
        "    all_rays_d = all_rays_d / torch.linalg.norm(all_rays_d, dim=-1, keepdims=True)\n",
        "    return all_rays_o.reshape(-1, 3), all_rays_d.reshape(-1, 3)\n",
        "\n",
        "\n",
        "# Nesf Klevr\n",
        "\n",
        "class BlenderDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', get_rgb=True) -> None:\n",
        "        # super().__init__()\n",
        "        '''\n",
        "        split: train/val/test\n",
        "        '''\n",
        "        self.root_dir = root_dir\n",
        "        self.get_rgb = get_rgb\n",
        "        self.split = split\n",
        "\n",
        "        self.define_transforms()\n",
        "        self.read_meta()\n",
        "\n",
        "    def define_transforms(self):\n",
        "        self.transform = T.ToTensor()\n",
        "\n",
        "    def read_meta(self):\n",
        "        with open(os.path.join(self.root_dir, \"metadata.json\"), \"r\") as f:\n",
        "            self.meta = json.load(f)\n",
        "\n",
        "        w, h = self.meta['metadata']['width'], self.meta['metadata']['width']\n",
        "        self.img_wh = (w, h)\n",
        "        self.focal = (self.meta['camera']['focal_length']*w/self.meta['camera']['sensor_width'])\n",
        "        if self.split not in ['train', 'val', 'test']:\n",
        "            raise ValueError(f\"split should be train/val/test, got {self.split}\")\n",
        "        self.split_ids = self.meta['split_ids'][self.split]\n",
        "\n",
        "        self.scene_boundaries = np.array([self.meta['scene_boundaries']['min'], self.meta['scene_boundaries']['max']])\n",
        "        self.directions = get_ray_directions(h, w, self.focal)\n",
        "\n",
        "        if self.split == 'train':\n",
        "            self.poses = []\n",
        "            self.all_rays_o = []\n",
        "            self.all_rays_d = []\n",
        "            self.all_rays = []\n",
        "            self.all_rgbs = []\n",
        "            camera_positions = np.array(self.meta['camera']['positions'])\n",
        "            camera_quaternions = np.array(self.meta['camera']['quaternions'])\n",
        "            for image_id in self.split_ids:\n",
        "                if self.get_rgb:\n",
        "                    image_path = os.path.join(self.root_dir, f'{image_id:05d}.png')\n",
        "                    img = Image.open(image_path)\n",
        "                    img = img.resize(self.img_wh, Image.Resampling.LANCZOS)\n",
        "                    img = self.transform(img) # (4, h, w)\n",
        "                    img = img.view(4, -1).permute(1,0) # (h*w, 4)\n",
        "                    # not sure, original jax implementation seems not using blend just cut it off, they also /255 to make it [0,1] which I didn't use\n",
        "                    # img = img[:, :3]*img[:, -1:]+(1-img[:,-1:]) # blend A to RGB\n",
        "                    img = img[:, :3]\n",
        "                    self.all_rgbs += [img]\n",
        "\n",
        "                pose = np.array(from_position_and_quaternion(camera_positions[image_id:image_id+1,:], camera_quaternions[image_id:image_id+1,:], False))[0,:3,:4]\n",
        "                self.poses += [pose]\n",
        "                c2w = torch.FloatTensor(pose)\n",
        "                rays_o, rays_d = get_rays(self.directions, c2w)\n",
        "                self.all_rays_o += [rays_o]\n",
        "                self.all_rays_d += [rays_d]\n",
        "\n",
        "            self.all_rays_o = torch.cat(self.all_rays_o, 0) # (len(self.split_ids)*h*w, 3)\n",
        "            self.all_rays_d = torch.cat(self.all_rays_d, 0) # (len(self.split_ids)*h*w, 3)\n",
        "            self.all_rays_o, self.all_rays_d = scale_rays(self.all_rays_o, self.all_rays_d, self.scene_boundaries, self.img_wh)\n",
        "\n",
        "\n",
        "            self.near, self.far = calculate_near_and_far(self.all_rays_o, self.all_rays_d)\n",
        "            self.all_rays = torch.cat([self.all_rays_o, self.all_rays_d, self.near, self.far],1).float()\n",
        "\n",
        "            if len(self.all_rgbs) > 0:\n",
        "                self.all_rgbs = torch.cat(self.all_rgbs, 0)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.split == 'train':\n",
        "            return len(self.all_rays)\n",
        "        elif self.split == 'val' or self.split == 'test':\n",
        "            return len(self.split_ids)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.split == 'train':\n",
        "            sample = {\n",
        "                'rays': self.all_rays[idx],\n",
        "            }\n",
        "            if self.get_rgb:\n",
        "                sample['rgbs'] = self.all_rgbs[idx]\n",
        "\n",
        "\n",
        "        # split of val/test\n",
        "        elif self.split == 'val':\n",
        "            image_id = self.split_ids[idx]\n",
        "\n",
        "            if self.get_rgb:\n",
        "                img = Image.open(os.path.join(self.root_dir, f'{image_id:05d}.png'))\n",
        "                img = img.resize(self.img_wh, Image.Resampling.LANCZOS)\n",
        "                img = self.transform(img)\n",
        "                valid_mask = (img[-1]>0).flatten()\n",
        "                img = img.view(4,-1).permute(1,0)\n",
        "                # img = img[:,:3]*img[:,-1:]+(1-img[:,-1:])\n",
        "                img = img[:, :3]\n",
        "\n",
        "            camera_position = np.array(self.meta['camera']['positions'][image_id:image_id+1])\n",
        "            camera_quaternion = np.array(self.meta['camera']['quaternions'][image_id:image_id+1])\n",
        "\n",
        "            pose = np.array(from_position_and_quaternion(camera_position, camera_quaternion, False))[0,:3,:4]\n",
        "            # pose = np.array(from_position_and_quaternion(camera_position[idx:1+idx,:], camera_quaternion[idx:1+idx,:], False))[0,:3,:4]\n",
        "            c2w = torch.FloatTensor(pose)[:3,:4]\n",
        "            rays_o, rays_d = get_rays(self.directions, c2w)\n",
        "            rays_o, rays_d = scale_rays(rays_o, rays_d, self.scene_boundaries, self.img_wh)\n",
        "\n",
        "            self.near, self.far = calculate_near_and_far(rays_o, rays_d)\n",
        "            rays = torch.cat([rays_o, rays_d,\n",
        "                              self.near,\n",
        "                              self.far],\n",
        "                              1) # (H*W, 8)\n",
        "\n",
        "            sample = {'rays': rays.float(),\n",
        "                      'id': image_id,\n",
        "                      'c2w': c2w}\n",
        "            if self.get_rgb:\n",
        "                sample['rgbs'] = img\n",
        "                sample['valid_mask'] = valid_mask\n",
        "\n",
        "        elif self.split == 'test':\n",
        "            image_id = self.split_ids[idx]\n",
        "\n",
        "            camera_position = np.array(self.meta['camera']['positions'][image_id:image_id+1])\n",
        "            camera_quaternion = np.array(self.meta['camera']['quaternions'][image_id:image_id+1])\n",
        "\n",
        "            pose = np.array(from_position_and_quaternion(camera_position, camera_quaternion, False))[0,:3,:4]\n",
        "            # pose = np.array(from_position_and_quaternion(camera_position[idx:1+idx,:], camera_quaternion[idx:1+idx,:], False))[0,:3,:4]\n",
        "            c2w = torch.FloatTensor(pose)[:3,:4]\n",
        "            rays_o, rays_d = get_rays(self.directions, c2w)\n",
        "            rays_o, rays_d = scale_rays(rays_o, rays_d, self.scene_boundaries, self.img_wh)\n",
        "\n",
        "            self.near, self.far = calculate_near_and_far(rays_o, rays_d)\n",
        "            rays = torch.cat([rays_o, rays_d,\n",
        "                              self.near,\n",
        "                              self.far],\n",
        "                              1) # (H*W, 8)\n",
        "\n",
        "            sample = {'rays': rays.float(),\n",
        "                      # 'c2w': c2w,\n",
        "                      'id': image_id}\n",
        "\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "VaJxXIKJGIaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train.py"
      ],
      "metadata": {
        "id": "XmVoGrPLtVaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from opt import get_opts\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import dataset_dict\n",
        "\n",
        "# models\n",
        "from models.nerf import Embedding, NeRF\n",
        "from models.rendering import render_rays\n",
        "\n",
        "# optimizer, scheduler, visualization\n",
        "from utils import *\n",
        "\n",
        "# losses\n",
        "from losses import loss_dict\n",
        "\n",
        "# metrics\n",
        "from metrics import *\n",
        "\n",
        "# # img save\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "# pytorch-lightning\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "class NeRFSystem(LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(NeRFSystem, self).__init__()\n",
        "        self.save_hyperparameters(hparams)\n",
        "\n",
        "        self.loss = loss_dict[hparams.loss_type]()\n",
        "\n",
        "        self.embedding_xyz = Embedding(3, 10) # 10 is the default number\n",
        "        self.embedding_dir = Embedding(3, 4) # 4 is the default number\n",
        "        self.embeddings = [self.embedding_xyz, self.embedding_dir]\n",
        "\n",
        "        self.nerf_coarse = NeRF()\n",
        "        self.models = [self.nerf_coarse]\n",
        "        if hparams.N_importance > 0:\n",
        "            self.nerf_fine = NeRF()\n",
        "            self.models += [self.nerf_fine]\n",
        "\n",
        "    def decode_batch(self, batch):\n",
        "        rays = batch['rays'] # (B, 8)\n",
        "        rgbs = batch['rgbs'] # (B, 3)\n",
        "        return rays, rgbs\n",
        "\n",
        "    def forward(self, rays):\n",
        "        \"\"\"Do batched inference on rays using chunk.\"\"\"\n",
        "        B = rays.shape[0]\n",
        "        results = defaultdict(list)\n",
        "        for i in range(0, B, self.hparams.chunk):\n",
        "            rendered_ray_chunks = \\\n",
        "                render_rays(self.models,\n",
        "                            self.embeddings,\n",
        "                            rays[i:i+self.hparams.chunk],\n",
        "                            self.hparams.N_samples,\n",
        "                            self.hparams.use_disp,\n",
        "                            self.hparams.perturb,\n",
        "                            self.hparams.noise_std,\n",
        "                            self.hparams.N_importance,\n",
        "                            self.hparams.chunk,) # chunk size is effective in val mode\n",
        "                            # self.train_dataset.white_back)\n",
        "\n",
        "            for k, v in rendered_ray_chunks.items():\n",
        "                results[k] += [v]\n",
        "\n",
        "        for k, v in results.items():\n",
        "            results[k] = torch.cat(v, 0)\n",
        "        return results\n",
        "\n",
        "    def prepare_data(self):\n",
        "        dataset = dataset_dict[self.hparams.dataset_name]\n",
        "        kwargs = {'root_dir': self.hparams.root_dir,\n",
        "                  } # 'img_wh': tuple(self.hparams.img_wh)\n",
        "        if self.hparams.dataset_name == 'llff':\n",
        "            kwargs['spheric_poses'] = self.hparams.spheric_poses\n",
        "            kwargs['val_num'] = self.hparams.num_gpus\n",
        "        self.train_dataset = dataset(split='train', **kwargs)\n",
        "        self.val_dataset = dataset(split='val', **kwargs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        self.optimizer = get_optimizer(self.hparams, self.models)\n",
        "        scheduler = get_scheduler(self.hparams, self.optimizer)\n",
        "\n",
        "        return [self.optimizer], [scheduler]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset,\n",
        "                          shuffle=True,\n",
        "                          num_workers=4,\n",
        "                          batch_size=self.hparams.batch_size,\n",
        "                          pin_memory=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset,\n",
        "                          shuffle=False,\n",
        "                          num_workers=4,\n",
        "                          batch_size=1, # validate one image (H*W rays) at a time\n",
        "                          pin_memory=True)\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        log = {'lr': get_learning_rate(self.optimizer)}\n",
        "        rays, rgbs = self.decode_batch(batch)\n",
        "        results = self(rays)\n",
        "        log['train/loss'] = loss = self.loss(results, rgbs)\n",
        "        typ = 'fine' if 'rgb_fine' in results else 'coarse'\n",
        "\n",
        "        with torch.no_grad():\n",
        "            psnr_ = psnr(results[f'rgb_{typ}'], rgbs)\n",
        "            log['train/psnr'] = psnr_\n",
        "\n",
        "        for k, v in log.items():\n",
        "            self.log_dict({k: v})\n",
        "\n",
        "        return {'loss': loss,\n",
        "                'progress_bar': {'train_psnr': psnr_},\n",
        "                'log': log\n",
        "               }\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        rays, rgbs = self.decode_batch(batch)\n",
        "        img_id = batch['id']\n",
        "        rays = rays.squeeze() # (H*W, 3)\n",
        "        rgbs = rgbs.squeeze() # (H*W, 3)\n",
        "        results = self(rays)\n",
        "        log = {'val_loss': self.loss(results, rgbs)}\n",
        "\n",
        "        typ = 'fine' if 'rgb_fine' in results else 'coarse'\n",
        "\n",
        "        if batch_nb == 0:\n",
        "            W, H = self.hparams.img_wh\n",
        "            img = results[f'rgb_{typ}'].view(H, W, 3).cpu()\n",
        "            img = img.permute(2, 0, 1) # (3, H, W)\n",
        "            img_gt = rgbs.view(H, W, 3).permute(2, 0, 1).cpu() # (3, H, W)\n",
        "            depth = visualize_depth(results[f'depth_{typ}'].view(H, W)) # (3, H, W)\n",
        "\n",
        "            output_path =  f'./depth_img/{int(img_id):05d}.png'\n",
        "            depth_numpy = depth.permute(1, 2, 0).numpy()\n",
        "            depth_numpy = (depth_numpy - depth_numpy.min()) / (depth_numpy.max() - depth_numpy.min()) * 255\n",
        "            depth_numpy = depth_numpy.astype('uint8')\n",
        "            imageio.imwrite(\n",
        "                output_path,\n",
        "                depth_numpy\n",
        "            )\n",
        "\n",
        "            stack = torch.stack([img_gt, img, depth]) # (3, 3, H, W)\n",
        "            self.logger.experiment.add_images('val/GT_pred_depth',\n",
        "                                               stack, self.global_step)\n",
        "\n",
        "        log['val_psnr'] = psnr(results[f'rgb_{typ}'], rgbs)\n",
        "        # 使用 self.log 來記錄 'val/loss'\n",
        "        for k, v in log.items():\n",
        "            self.log_dict({k: v})\n",
        "        return log\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    hparams = get_opts()\n",
        "    system = NeRFSystem(hparams)\n",
        "    checkpoint_callback = ModelCheckpoint(dirpath='./ckpts',\n",
        "                                          filename='base',\n",
        "                                          monitor='val_loss',\n",
        "                                          mode='min',\n",
        "                                          save_top_k=5,)\n",
        "\n",
        "    logger = TensorBoardLogger(\n",
        "        save_dir=\"logs\",\n",
        "        name=hparams.exp_name,\n",
        "        # debug=False,\n",
        "        # create_git_tag=False\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(max_epochs=hparams.num_epochs,\n",
        "                      callbacks=checkpoint_callback,\n",
        "                      # resume_from_checkpoint=hparams.ckpt_path,\n",
        "                      logger=logger,\n",
        "                      # early_stop_callback=None,\n",
        "                      # weights_summary=None,\n",
        "                      # progress_bar_refresh_rate=1,\n",
        "                      # gpus=hparams.num_gpus,\n",
        "                      # distributed_backend='ddp' if hparams.num_gpus>1 else None,\n",
        "                      num_sanity_val_steps=1,\n",
        "                      benchmark=True,)\n",
        "                      # profiler=hparams.num_gpus==1)\n",
        "\n",
        "    trainer.fit(system)"
      ],
      "metadata": {
        "id": "xvVZGRM2tXDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### training"
      ],
      "metadata": {
        "id": "hOhJ70qh2Fhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "rendering: torch.searchsorted"
      ],
      "metadata": {
        "id": "fMxcqFAoNb59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd nerf_pl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71Fa1rOU2PGq",
        "outputId": "5ddc0906-e1f5-4634-ddf8-250d83cbe47b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nerf_pl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir ckpts"
      ],
      "metadata": {
        "id": "48022wkzCE7m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir imgout"
      ],
      "metadata": {
        "id": "XTzS4fzhCGvP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir depth_img"
      ],
      "metadata": {
        "id": "1E2Y2xMUXL5i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning==2.1.2"
      ],
      "metadata": {
        "id": "wJix88eq2jyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "   --dataset_name blender \\\n",
        "   --root_dir '/content/dataset' \\\n",
        "   --N_importance 64 --img_wh 256 256 --noise_std 0 \\\n",
        "   --num_epochs 8 --batch_size 1024 \\\n",
        "   --optimizer adam --lr 5e-4 \\\n",
        "   --lr_scheduler steplr --decay_step 2 4 8 --decay_gamma 0.5 \\\n",
        "   --exp_name exp"
      ],
      "metadata": {
        "id": "XXadp9Ty2G8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### logs"
      ],
      "metadata": {
        "id": "cF4533bM_-OP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard --logdir /content/nerf_pl/logs  # logs是你的日誌目錄"
      ],
      "metadata": {
        "id": "VoTdq6nWAALi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1-SbMC1KUZ7FMotoQvJP8boJqvqBRDw-o -O base.ckpt"
      ],
      "metadata": {
        "id": "nRhXAFtoxX38",
        "outputId": "d0faf2e6-2531-461a-9e15-571855db89eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-SbMC1KUZ7FMotoQvJP8boJqvqBRDw-o\n",
            "To: /content/nerf_pl/base.ckpt\n",
            "\r  0% 0.00/14.4M [00:00<?, ?B/s]\r 95% 13.6M/14.4M [00:00<00:00, 135MB/s]\r100% 14.4M/14.4M [00:00<00:00, 138MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### eval.py"
      ],
      "metadata": {
        "id": "ZgZceL5oxr2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from opt import get_opts\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import dataset_dict\n",
        "\n",
        "# models\n",
        "from models.nerf import Embedding, NeRF\n",
        "from models.rendering import render_rays\n",
        "\n",
        "# optimizer, scheduler, visualization\n",
        "from utils import *\n",
        "\n",
        "# losses\n",
        "from losses import loss_dict\n",
        "\n",
        "# metrics\n",
        "from metrics import *\n",
        "\n",
        "# img save\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "# pytorch-lightning\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "class NeRFSystem(LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(NeRFSystem, self).__init__()\n",
        "        self.save_hyperparameters(hparams)\n",
        "\n",
        "        self.loss = loss_dict[hparams.loss_type]()\n",
        "\n",
        "        self.embedding_xyz = Embedding(3, 10) # 10 is the default number\n",
        "        self.embedding_dir = Embedding(3, 4) # 4 is the default number\n",
        "        self.embeddings = [self.embedding_xyz, self.embedding_dir]\n",
        "\n",
        "        self.nerf_coarse = NeRF()\n",
        "        self.models = [self.nerf_coarse]\n",
        "        if hparams.N_importance > 0:\n",
        "            self.nerf_fine = NeRF()\n",
        "            self.models += [self.nerf_fine]\n",
        "\n",
        "    # def decode_batch(self, batch):\n",
        "    #     rays = batch['rays'] # (B, 8)\n",
        "    #     rgbs = batch['rgbs'] # (B, 3)\n",
        "    #     return rays, rgbs\n",
        "\n",
        "    def forward(self, rays):\n",
        "        \"\"\"Do batched inference on rays using chunk.\"\"\"\n",
        "        B = rays.shape[0]\n",
        "        results = defaultdict(list)\n",
        "        for i in range(0, B, self.hparams.chunk):\n",
        "            rendered_ray_chunks = \\\n",
        "                render_rays(self.models,\n",
        "                            self.embeddings,\n",
        "                            rays[i:i+self.hparams.chunk],\n",
        "                            self.hparams.N_samples,\n",
        "                            self.hparams.use_disp,\n",
        "                            self.hparams.perturb,\n",
        "                            self.hparams.noise_std,\n",
        "                            self.hparams.N_importance,\n",
        "                            self.hparams.chunk,) # chunk size is effective in val mode\n",
        "                            # self.train_dataset.white_back)\n",
        "\n",
        "            for k, v in rendered_ray_chunks.items():\n",
        "                results[k] += [v]\n",
        "\n",
        "        for k, v in results.items():\n",
        "            results[k] = torch.cat(v, 0)\n",
        "        return results\n",
        "\n",
        "    def prepare_data(self):\n",
        "        dataset = dataset_dict[self.hparams.dataset_name]\n",
        "        kwargs = {'root_dir': self.hparams.root_dir,\n",
        "                  } # 'img_wh': tuple(self.hparams.img_wh)\n",
        "        if self.hparams.dataset_name == 'llff':\n",
        "            kwargs['spheric_poses'] = self.hparams.spheric_poses\n",
        "        self.test_dataset = dataset(split='val', **kwargs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        self.optimizer = get_optimizer(self.hparams, self.models)\n",
        "        scheduler = get_scheduler(self.hparams, self.optimizer)\n",
        "\n",
        "        return [self.optimizer], [scheduler]\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset,\n",
        "                          shuffle=False,\n",
        "                          num_workers=4,\n",
        "                          batch_size=1,\n",
        "                          pin_memory=True)\n",
        "\n",
        "    def test_step(self, batch, batch_nb):\n",
        "        rays = batch['rays']\n",
        "        img_id = batch['id']\n",
        "        rays = rays.squeeze() # (H*W, 3)\n",
        "        results = self(rays)\n",
        "        typ = 'fine' if 'rgb_fine' in results else 'coarse'\n",
        "\n",
        "        W, H = self.hparams.img_wh\n",
        "        img = results[f'rgb_{typ}'].view(H, W, 3).cpu()\n",
        "\n",
        "        # Save the rendered image for each test batch\n",
        "        output_path =  f'./imgout/{int(img_id):05d}.png'\n",
        "        img_vis = (\n",
        "            img\n",
        "            .clip(0, 1)\n",
        "            .numpy()\n",
        "        )\n",
        "        imageio.imwrite(\n",
        "            output_path,\n",
        "            (img_vis * 255).astype('uint8')\n",
        "        )\n",
        "\n",
        "        # Return any relevant metrics or values\n",
        "        return {'output_image_path': output_path}\n",
        "\n",
        "\n",
        "    # def validation_epoch_end(self, outputs):\n",
        "    #     mean_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "    #     mean_psnr = torch.stack([x['val_psnr'] for x in outputs]).mean()\n",
        "\n",
        "    #     return {'progress_bar': {'val_loss': mean_loss,\n",
        "    #                              'val_psnr': mean_psnr},\n",
        "    #             'log': {'val/loss': mean_loss,\n",
        "    #                     'val/psnr': mean_psnr}\n",
        "    #            }\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    hparams = get_opts()\n",
        "    system = NeRFSystem(hparams)\n",
        "    checkpoint_callback = ModelCheckpoint(dirpath='./ckpts',\n",
        "                                          filename='base.ckpt',\n",
        "                                          monitor='val/loss',\n",
        "                                          mode='min',\n",
        "                                          save_top_k=5,)\n",
        "\n",
        "    logger = TensorBoardLogger(\n",
        "        save_dir=\"logs\",\n",
        "        name=hparams.exp_name,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(max_epochs=hparams.num_epochs,\n",
        "                      callbacks=checkpoint_callback,\n",
        "                      # resume_from_checkpoint=hparams.ckpt_path,\n",
        "                      logger=logger,\n",
        "                      # early_stop_callback=None,\n",
        "                      # weights_summary=None,\n",
        "                      # progress_bar_refresh_rate=1,\n",
        "                      # gpus=hparams.num_gpus,\n",
        "                      # distributed_backend='ddp' if hparams.num_gpus>1 else None,\n",
        "                      num_sanity_val_steps=1,\n",
        "                      benchmark=True,)\n",
        "                      # profiler=hparams.num_gpus==1)\n",
        "\n",
        "    # Load the best checkpoint for testing\n",
        "    trainer.test(system, ckpt_path='./ckpts/base.ckpt')"
      ],
      "metadata": {
        "id": "wb64zpMdxs7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### testing"
      ],
      "metadata": {
        "id": "FIPcSh9F0Jgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 eval.py \\\n",
        "   --root_dir '/content/dataset' \\\n",
        "   --dataset_name blender \\\n",
        "   --img_wh 256 256 --N_importance 64 --noise_std 0"
      ],
      "metadata": {
        "id": "X-XJgzVhx0Va",
        "outputId": "7e48bf65-db7d-4d93-9de2-788211f95564",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "2023-12-10 16:01:47.707063: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-10 16:01:47.707120: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-10 16:01:47.707162: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-10 16:01:48.839275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Restoring states from the checkpoint path at ./ckpts/base.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from the checkpoint at ./ckpts/base.ckpt\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Testing DataLoader 0: 100% 20/20 [02:11<00:00,  6.58s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLkHKjAHF3U-",
        "outputId": "8bf54ae6-9332-484e-ee58-62165aa8117a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 grade.py /content/nerf_pl/imgout /content/val_gt"
      ],
      "metadata": {
        "id": "d6sQOY7Oj5Vu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b8b175-6909-4a71-8ae4-c75e0015a732"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 20/20 [00:02<00:00,  7.15it/s]\n",
            "Testing psnr 42.183295011520386 (avg)\n",
            "Testing ssim 0.9924355388185437 (avg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import imageio\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "image = Image.open('/content/nerf_pl/imgout/00279.png')\n",
        "\n",
        "# 将深度图映射为彩色图\n",
        "colored_depth = cv2.applyColorMap(np.uint8(np.array(image) * 255), cv2.COLORMAP_JET)\n",
        "cv2_imshow(colored_depth)\n",
        "cv2.imwrite('/content/nerf_pl/depth_img/00279.png', colored_depth)"
      ],
      "metadata": {
        "id": "UDJ4ZFc8d27V",
        "outputId": "e390f3a6-11e4-411a-f009-040a06121433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=256x256>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O1EHE3-Aeyjo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}