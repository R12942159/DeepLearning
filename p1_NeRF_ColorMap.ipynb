{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R12942159/NTU_DLCV/blob/Hw4/p1_NeRF_ColorMap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQosyvi9_SUi"
      },
      "outputs": [],
      "source": [
        "!gdown 1hF4z9U-xaoV4qaq9DbhTP-KKJTlwOUv_ -O hw4_data.zip\n",
        "!unzip hw4_data.zip\n",
        "!rm hw4_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1a-1prjFoFaY_ZK4KriFCKWcqwkdDt5tW -O hw4_gt.zip\n",
        "!unzip hw4_gt.zip\n",
        "!rm hw4_gt.zip"
      ],
      "metadata": {
        "id": "rp2Or8hcyBPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kornia"
      ],
      "metadata": {
        "id": "W92bIA_qcnUM",
        "outputId": "3216ccbc-fbd8-4758-9df3-94ed970d3022",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kornia\n",
            "  Downloading kornia-0.7.0-py2.py3-none-any.whl (705 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.7/705.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (23.2)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.1->kornia) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6PyjHE8SwNC",
        "outputId": "c15f85fc-4cc5-417c-9fb3-60e484d25996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NeRF_pl"
      ],
      "metadata": {
        "id": "g1rArMiy0uM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recursive https://github.com/kwea123/nerf_pl"
      ],
      "metadata": {
        "id": "ljGMzAXH0wQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### blender.py"
      ],
      "metadata": {
        "id": "ofIMmxXMGFQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "import random\n",
        "from kornia import create_meshgrid\n",
        "from scipy.spatial import transform\n",
        "\n",
        "\n",
        "def calculate_near_and_far(rays_o, rays_d, bbox_min=[-1.,-1.,-1.], bbox_max=[1.,1.,1.]):\n",
        "    '''\n",
        "    rays_o, (len(self.split_ids)*h*w, 3)\n",
        "    rays_d, (len(self.split_ids)*h*w, 3)\n",
        "    bbox_min=[-1,-1,-1],\n",
        "    bbox_max=[1,1,1]\n",
        "    '''\n",
        "    # map all shape to same (len(self.split_ids)*h*w, 3, 2)\n",
        "    corners = torch.stack((torch.tensor(bbox_min),torch.tensor(bbox_max)), dim=-1)\n",
        "    corners = corners.unsqueeze(0).repeat(rays_o.shape[0],1,1) # (len(self.split_ids)*h*w, 3, 2)\n",
        "    corners -= torch.unsqueeze(rays_o, -1).repeat(1,1,2)\n",
        "    intersections = (corners / (torch.unsqueeze(rays_d, -1).repeat(1,1,2)))\n",
        "\n",
        "    min_intersections = torch.amax(torch.amin(intersections, dim=-1), dim=-1, keepdim=True)\n",
        "    max_intersections = torch.amin(torch.amax(intersections, dim=-1), dim=-1, keepdim=True)\n",
        "    epsilon = 1e-10*torch.ones_like(min_intersections)\n",
        "    near = torch.maximum(epsilon, min_intersections)\n",
        "    # tmp = near\n",
        "    near = torch.where((near > max_intersections), epsilon, near)\n",
        "    far = torch.where(near < max_intersections, max_intersections, near+epsilon)\n",
        "\n",
        "    return near, far\n",
        "\n",
        "def get_ray_directions(H, W, focal):\n",
        "    \"\"\"\n",
        "    Get ray directions for all pixels in camera coordinate.\n",
        "    Reference: https://www.scratchapixel.com/lessons/3d-basic-rendering/\n",
        "               ray-tracing-generating-camera-rays/standard-coordinate-systems\n",
        "\n",
        "    Inputs:\n",
        "        H, W, focal: image height, width and focal length\n",
        "\n",
        "    Outputs:\n",
        "        directions: (H, W, 3), the direction of the rays in camera coordinate\n",
        "    \"\"\"\n",
        "    grid = create_meshgrid(H, W, normalized_coordinates=False)[0]\n",
        "    i, j = grid.unbind(-1)\n",
        "    # the direction here is without +0.5 pixel centering as calibration is not so accurate\n",
        "    # see https://github.com/bmild/nerf/issues/24\n",
        "    directions = \\\n",
        "        torch.stack([(i-W/2)/focal, -(j-H/2)/focal, -torch.ones_like(i)], -1) # (H, W, 3)\n",
        "\n",
        "    return directions\n",
        "\n",
        "def get_rays(directions, c2w):\n",
        "    \"\"\"\n",
        "    Get ray origin and normalized directions in world coordinate for all pixels in one image.\n",
        "    Reference: https://www.scratchapixel.com/lessons/3d-basic-rendering/\n",
        "               ray-tracing-generating-camera-rays/standard-coordinate-systems\n",
        "\n",
        "    Inputs:\n",
        "        directions: (H, W, 3) precomputed ray directions in camera coordinate\n",
        "        c2w: (3, 4) transformation matrix from camera coordinate to world coordinate\n",
        "\n",
        "    Outputs:\n",
        "        rays_o: (H*W, 3), the origin of the rays in world coordinate\n",
        "        rays_d: (H*W, 3), the normalized direction of the rays in world coordinate\n",
        "    \"\"\"\n",
        "    # Rotate ray directions from camera coordinate to the world coordinate\n",
        "    rays_d = directions @ c2w[:, :3].T # (H, W, 3)\n",
        "    rays_d = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
        "    # The origin of all rays is the camera origin in world coordinate\n",
        "    rays_o = c2w[:, 3].expand(rays_d.shape) # (H, W, 3)\n",
        "\n",
        "    rays_d = rays_d.view(-1, 3)\n",
        "    rays_o = rays_o.view(-1, 3)\n",
        "\n",
        "    return rays_o, rays_d\n",
        "\n",
        "def blender_quat2rot(quaternion):\n",
        "    \"\"\"Convert quaternion to rotation matrix.\n",
        "    Equivalent to, but support batched case:\n",
        "    ```python\n",
        "    rot3x3 = mathutils.Quaternion(quaternion).to_matrix()\n",
        "    ```\n",
        "    Args:\n",
        "    quaternion:\n",
        "    Returns:\n",
        "    rotation matrix\n",
        "    \"\"\"\n",
        "\n",
        "    # Note: Blender first cast to double values for numerical precision while\n",
        "    # we're using float32.\n",
        "    q = np.sqrt(2) * quaternion\n",
        "\n",
        "    q0 = q[..., 0]\n",
        "    q1 = q[..., 1]\n",
        "    q2 = q[..., 2]\n",
        "    q3 = q[..., 3]\n",
        "\n",
        "    qda = q0 * q1\n",
        "    qdb = q0 * q2\n",
        "    qdc = q0 * q3\n",
        "    qaa = q1 * q1\n",
        "    qab = q1 * q2\n",
        "    qac = q1 * q3\n",
        "    qbb = q2 * q2\n",
        "    qbc = q2 * q3\n",
        "    qcc = q3 * q3\n",
        "\n",
        "    # Note: idx are inverted as blender and numpy convensions do not\n",
        "    # match (x, y) -> (y, x)\n",
        "    rotation = np.empty((*quaternion.shape[:-1], 3, 3), dtype=np.float32)\n",
        "    rotation[..., 0, 0] = 1.0 - qbb - qcc\n",
        "    rotation[..., 1, 0] = qdc + qab\n",
        "    rotation[..., 2, 0] = -qdb + qac\n",
        "\n",
        "    rotation[..., 0, 1] = -qdc + qab\n",
        "    rotation[..., 1, 1] = 1.0 - qaa - qcc\n",
        "    rotation[..., 2, 1] = qda + qbc\n",
        "\n",
        "    rotation[..., 0, 2] = qdb + qac\n",
        "    rotation[..., 1, 2] = -qda + qbc\n",
        "    rotation[..., 2, 2] = 1.0 - qaa - qbb\n",
        "    return rotation\n",
        "\n",
        "def make_transform_matrix(positions,rotations,):\n",
        "    \"\"\"Create the 4x4 transformation matrix.\n",
        "    Note: This function uses numpy.\n",
        "    Args:\n",
        "    positions: Translation applied after the rotation.\n",
        "        Last column of the transformation matrix\n",
        "    rotations: Rotation. Top-left 3x3 matrix of the transformation matrix.\n",
        "    Returns:\n",
        "    transformation_matrix:\n",
        "    \"\"\"\n",
        "    # Create the 4x4 transformation matrix\n",
        "    rot_pos = np.broadcast_to(np.eye(4), (*positions.shape[:-1], 4, 4)).copy()\n",
        "    rot_pos[..., :3, :3] = rotations\n",
        "    rot_pos[..., :3, 3] = positions\n",
        "    return rot_pos\n",
        "\n",
        "def from_position_and_quaternion(positions, quaternions, use_unreal_axes):\n",
        "    if use_unreal_axes:\n",
        "        rotations = transform.Rotation.from_quat(quaternions).as_matrix()\n",
        "    else:\n",
        "        # Rotation matrix that rotates from world to object coordinates.\n",
        "        # Warning: Rotations should be given in blender convensions as\n",
        "        # scipy.transform uses different convensions.\n",
        "        rotations = blender_quat2rot(quaternions)\n",
        "    px2world_transform = make_transform_matrix(positions=positions,rotations=rotations)\n",
        "    return px2world_transform\n",
        "\n",
        "def scale_rays(all_rays_o, all_rays_d, scene_boundaries, img_wh):\n",
        "    \"\"\"Rescale scene boundaries.\n",
        "    rays_o: (len(image_paths)*h*w, 3)\n",
        "    rays_d: (len(image_paths)*h*w, 3)\n",
        "    scene_boundaries: np.array(2 ,3), [min, max]\n",
        "    img_wh: (2)\n",
        "    \"\"\"\n",
        "    # Rescale (x, y, z) from [min, max] -> [-1, 1]\n",
        "    all_rays_o = all_rays_o.reshape(-1, img_wh[0], img_wh[1], 3) # (len(image_paths), h, w, 3))\n",
        "    all_rays_d = all_rays_d.reshape(-1, img_wh[0], img_wh[1], 3)\n",
        "\n",
        "    old_min = torch.from_numpy(scene_boundaries[0])\n",
        "    old_max = torch.from_numpy(scene_boundaries[1])\n",
        "    new_min = torch.tensor([-1,-1,-1])\n",
        "    new_max = torch.tensor([1,1,1])\n",
        "    # scale = max(scene_boundaries[1] - scene_boundaries[0])/2\n",
        "    # all_rays_o = (all_rays_o - torch.mean(all_rays_o, dim=-1, keepdim=True)) / scale\n",
        "    # This is from jax3d.interp, kind of weird but true\n",
        "    all_rays_o = ((new_min - new_max) / (old_min - old_max))*all_rays_o + (old_min * new_max - new_min * old_max) / (old_min - old_max)\n",
        "\n",
        "    # We also need to rescale the camera direction by bbox.size.\n",
        "    # The direction can be though of a ray from a point in space (the camera\n",
        "    # origin) to another point in space (say the red light on the lego\n",
        "    # bulldozer). When we scale the scene in a certain way, this direction\n",
        "    # also needs to be scaled in the same way.\n",
        "    all_rays_d = all_rays_d * 2 / (scene_boundaries[1] - scene_boundaries[0])\n",
        "    # (re)-normalize the rays\n",
        "    all_rays_d = all_rays_d / torch.linalg.norm(all_rays_d, dim=-1, keepdims=True)\n",
        "    return all_rays_o.reshape(-1, 3), all_rays_d.reshape(-1, 3)\n",
        "\n",
        "\n",
        "# Nesf Klevr\n",
        "\n",
        "class BlenderDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', get_rgb=True) -> None:\n",
        "        # super().__init__()\n",
        "        '''\n",
        "        split: train/val/test\n",
        "        '''\n",
        "        self.root_dir = root_dir\n",
        "        self.get_rgb = get_rgb\n",
        "        self.split = split\n",
        "\n",
        "        self.define_transforms()\n",
        "        self.read_meta()\n",
        "\n",
        "    def define_transforms(self):\n",
        "        self.transform = T.ToTensor()\n",
        "\n",
        "    def read_meta(self):\n",
        "        with open(os.path.join(self.root_dir, \"metadata.json\"), \"r\") as f:\n",
        "            self.meta = json.load(f)\n",
        "\n",
        "        w, h = self.meta['metadata']['width'], self.meta['metadata']['width']\n",
        "        self.img_wh = (w, h)\n",
        "        self.focal = (self.meta['camera']['focal_length']*w/self.meta['camera']['sensor_width'])\n",
        "        if self.split not in ['train', 'val', 'test']:\n",
        "            raise ValueError(f\"split should be train/val/test, got {self.split}\")\n",
        "        self.split_ids = self.meta['split_ids'][self.split]\n",
        "\n",
        "        self.scene_boundaries = np.array([self.meta['scene_boundaries']['min'], self.meta['scene_boundaries']['max']])\n",
        "        self.directions = get_ray_directions(h, w, self.focal)\n",
        "\n",
        "        if self.split == 'train':\n",
        "            self.poses = []\n",
        "            self.all_rays_o = []\n",
        "            self.all_rays_d = []\n",
        "            self.all_rays = []\n",
        "            self.all_rgbs = []\n",
        "            camera_positions = np.array(self.meta['camera']['positions'])\n",
        "            camera_quaternions = np.array(self.meta['camera']['quaternions'])\n",
        "            for image_id in self.split_ids:\n",
        "                if self.get_rgb:\n",
        "                    image_path = os.path.join(self.root_dir, f'{image_id:05d}.png')\n",
        "                    img = Image.open(image_path)\n",
        "                    img = img.resize(self.img_wh, Image.Resampling.LANCZOS)\n",
        "                    img = self.transform(img) # (4, h, w)\n",
        "                    img = img.view(4, -1).permute(1,0) # (h*w, 4)\n",
        "                    # not sure, original jax implementation seems not using blend just cut it off, they also /255 to make it [0,1] which I didn't use\n",
        "                    # img = img[:, :3]*img[:, -1:]+(1-img[:,-1:]) # blend A to RGB\n",
        "                    img = img[:, :3]\n",
        "                    self.all_rgbs += [img]\n",
        "\n",
        "                pose = np.array(from_position_and_quaternion(camera_positions[image_id:image_id+1,:], camera_quaternions[image_id:image_id+1,:], False))[0,:3,:4]\n",
        "                self.poses += [pose]\n",
        "                c2w = torch.FloatTensor(pose)\n",
        "                rays_o, rays_d = get_rays(self.directions, c2w)\n",
        "                self.all_rays_o += [rays_o]\n",
        "                self.all_rays_d += [rays_d]\n",
        "\n",
        "            self.all_rays_o = torch.cat(self.all_rays_o, 0) # (len(self.split_ids)*h*w, 3)\n",
        "            self.all_rays_d = torch.cat(self.all_rays_d, 0) # (len(self.split_ids)*h*w, 3)\n",
        "            self.all_rays_o, self.all_rays_d = scale_rays(self.all_rays_o, self.all_rays_d, self.scene_boundaries, self.img_wh)\n",
        "\n",
        "\n",
        "            self.near, self.far = calculate_near_and_far(self.all_rays_o, self.all_rays_d)\n",
        "            self.all_rays = torch.cat([self.all_rays_o, self.all_rays_d, self.near, self.far],1).float()\n",
        "\n",
        "            if len(self.all_rgbs) > 0:\n",
        "                self.all_rgbs = torch.cat(self.all_rgbs, 0)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.split == 'train':\n",
        "            return len(self.all_rays)\n",
        "        elif self.split == 'val' or self.split == 'test':\n",
        "            return len(self.split_ids)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.split == 'train':\n",
        "            sample = {\n",
        "                'rays': self.all_rays[idx],\n",
        "            }\n",
        "            if self.get_rgb:\n",
        "                sample['rgbs'] = self.all_rgbs[idx]\n",
        "\n",
        "\n",
        "        # split of val/test\n",
        "        elif self.split == 'val':\n",
        "            image_id = self.split_ids[idx]\n",
        "\n",
        "            if self.get_rgb:\n",
        "                img = Image.open(os.path.join(self.root_dir, f'{image_id:05d}.png'))\n",
        "                img = img.resize(self.img_wh, Image.Resampling.LANCZOS)\n",
        "                img = self.transform(img)\n",
        "                valid_mask = (img[-1]>0).flatten()\n",
        "                img = img.view(4,-1).permute(1,0)\n",
        "                # img = img[:,:3]*img[:,-1:]+(1-img[:,-1:])\n",
        "                img = img[:, :3]\n",
        "\n",
        "            camera_position = np.array(self.meta['camera']['positions'][image_id:image_id+1])\n",
        "            camera_quaternion = np.array(self.meta['camera']['quaternions'][image_id:image_id+1])\n",
        "\n",
        "            pose = np.array(from_position_and_quaternion(camera_position, camera_quaternion, False))[0,:3,:4]\n",
        "            # pose = np.array(from_position_and_quaternion(camera_position[idx:1+idx,:], camera_quaternion[idx:1+idx,:], False))[0,:3,:4]\n",
        "            c2w = torch.FloatTensor(pose)[:3,:4]\n",
        "            rays_o, rays_d = get_rays(self.directions, c2w)\n",
        "            rays_o, rays_d = scale_rays(rays_o, rays_d, self.scene_boundaries, self.img_wh)\n",
        "\n",
        "            self.near, self.far = calculate_near_and_far(rays_o, rays_d)\n",
        "            rays = torch.cat([rays_o, rays_d,\n",
        "                              self.near,\n",
        "                              self.far],\n",
        "                              1) # (H*W, 8)\n",
        "\n",
        "            sample = {'rays': rays.float(),\n",
        "                      'id': image_id,\n",
        "                      'c2w': c2w}\n",
        "            if self.get_rgb:\n",
        "                sample['rgbs'] = img\n",
        "                sample['valid_mask'] = valid_mask\n",
        "\n",
        "        elif self.split == 'test':\n",
        "            image_id = self.split_ids[idx]\n",
        "\n",
        "            camera_position = np.array(self.meta['camera']['positions'][image_id:image_id+1])\n",
        "            camera_quaternion = np.array(self.meta['camera']['quaternions'][image_id:image_id+1])\n",
        "\n",
        "            pose = np.array(from_position_and_quaternion(camera_position, camera_quaternion, False))[0,:3,:4]\n",
        "            # pose = np.array(from_position_and_quaternion(camera_position[idx:1+idx,:], camera_quaternion[idx:1+idx,:], False))[0,:3,:4]\n",
        "            c2w = torch.FloatTensor(pose)[:3,:4]\n",
        "            rays_o, rays_d = get_rays(self.directions, c2w)\n",
        "            rays_o, rays_d = scale_rays(rays_o, rays_d, self.scene_boundaries, self.img_wh)\n",
        "\n",
        "            self.near, self.far = calculate_near_and_far(rays_o, rays_d)\n",
        "            rays = torch.cat([rays_o, rays_d,\n",
        "                              self.near,\n",
        "                              self.far],\n",
        "                              1) # (H*W, 8)\n",
        "\n",
        "            sample = {'rays': rays.float(),\n",
        "                      # 'c2w': c2w,\n",
        "                      'id': image_id}\n",
        "\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "VaJxXIKJGIaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train.py"
      ],
      "metadata": {
        "id": "XmVoGrPLtVaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from opt import get_opts\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import dataset_dict\n",
        "\n",
        "# models\n",
        "from models.nerf import Embedding, NeRF\n",
        "from models.rendering import render_rays\n",
        "\n",
        "# optimizer, scheduler, visualization\n",
        "from utils import *\n",
        "\n",
        "# losses\n",
        "from losses import loss_dict\n",
        "\n",
        "# metrics\n",
        "from metrics import *\n",
        "\n",
        "# # img save\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "# pytorch-lightning\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "class NeRFSystem(LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(NeRFSystem, self).__init__()\n",
        "        self.save_hyperparameters(hparams)\n",
        "\n",
        "        self.loss = loss_dict[hparams.loss_type]()\n",
        "\n",
        "        self.embedding_xyz = Embedding(3, 10) # 10 is the default number\n",
        "        self.embedding_dir = Embedding(3, 4) # 4 is the default number\n",
        "        self.embeddings = [self.embedding_xyz, self.embedding_dir]\n",
        "\n",
        "        self.nerf_coarse = NeRF()\n",
        "        self.models = [self.nerf_coarse]\n",
        "        if hparams.N_importance > 0:\n",
        "            self.nerf_fine = NeRF()\n",
        "            self.models += [self.nerf_fine]\n",
        "\n",
        "    def decode_batch(self, batch):\n",
        "        rays = batch['rays'] # (B, 8)\n",
        "        rgbs = batch['rgbs'] # (B, 3)\n",
        "        return rays, rgbs\n",
        "\n",
        "    def forward(self, rays):\n",
        "        \"\"\"Do batched inference on rays using chunk.\"\"\"\n",
        "        B = rays.shape[0]\n",
        "        results = defaultdict(list)\n",
        "        for i in range(0, B, self.hparams.chunk):\n",
        "            rendered_ray_chunks = \\\n",
        "                render_rays(self.models,\n",
        "                            self.embeddings,\n",
        "                            rays[i:i+self.hparams.chunk],\n",
        "                            self.hparams.N_samples,\n",
        "                            self.hparams.use_disp,\n",
        "                            self.hparams.perturb,\n",
        "                            self.hparams.noise_std,\n",
        "                            self.hparams.N_importance,\n",
        "                            self.hparams.chunk,) # chunk size is effective in val mode\n",
        "                            # self.train_dataset.white_back)\n",
        "\n",
        "            for k, v in rendered_ray_chunks.items():\n",
        "                results[k] += [v]\n",
        "\n",
        "        for k, v in results.items():\n",
        "            results[k] = torch.cat(v, 0)\n",
        "        return results\n",
        "\n",
        "    def prepare_data(self):\n",
        "        dataset = dataset_dict[self.hparams.dataset_name]\n",
        "        kwargs = {'root_dir': self.hparams.root_dir,\n",
        "                  } # 'img_wh': tuple(self.hparams.img_wh)\n",
        "        if self.hparams.dataset_name == 'llff':\n",
        "            kwargs['spheric_poses'] = self.hparams.spheric_poses\n",
        "            kwargs['val_num'] = self.hparams.num_gpus\n",
        "        self.train_dataset = dataset(split='train', **kwargs)\n",
        "        self.val_dataset = dataset(split='val', **kwargs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        self.optimizer = get_optimizer(self.hparams, self.models)\n",
        "        scheduler = get_scheduler(self.hparams, self.optimizer)\n",
        "\n",
        "        return [self.optimizer], [scheduler]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset,\n",
        "                          shuffle=True,\n",
        "                          num_workers=4,\n",
        "                          batch_size=self.hparams.batch_size,\n",
        "                          pin_memory=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset,\n",
        "                          shuffle=False,\n",
        "                          num_workers=4,\n",
        "                          batch_size=1, # validate one image (H*W rays) at a time\n",
        "                          pin_memory=True)\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        log = {'lr': get_learning_rate(self.optimizer)}\n",
        "        rays, rgbs = self.decode_batch(batch)\n",
        "        results = self(rays)\n",
        "        log['train/loss'] = loss = self.loss(results, rgbs)\n",
        "        typ = 'fine' if 'rgb_fine' in results else 'coarse'\n",
        "\n",
        "        with torch.no_grad():\n",
        "            psnr_ = psnr(results[f'rgb_{typ}'], rgbs)\n",
        "            log['train/psnr'] = psnr_\n",
        "\n",
        "        for k, v in log.items():\n",
        "            self.log_dict({k: v})\n",
        "\n",
        "        return {'loss': loss,\n",
        "                'progress_bar': {'train_psnr': psnr_},\n",
        "                'log': log\n",
        "               }\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        rays, rgbs = self.decode_batch(batch)\n",
        "        img_id = batch['id']\n",
        "        rays = rays.squeeze() # (H*W, 3)\n",
        "        rgbs = rgbs.squeeze() # (H*W, 3)\n",
        "        results = self(rays)\n",
        "        log = {'val_loss': self.loss(results, rgbs)}\n",
        "\n",
        "        typ = 'fine' if 'rgb_fine' in results else 'coarse'\n",
        "\n",
        "        if batch_nb == 0:\n",
        "            W, H = self.hparams.img_wh\n",
        "            img = results[f'rgb_{typ}'].view(H, W, 3).cpu()\n",
        "            img = img.permute(2, 0, 1) # (3, H, W)\n",
        "            img_gt = rgbs.view(H, W, 3).permute(2, 0, 1).cpu() # (3, H, W)\n",
        "            depth = visualize_depth(results[f'depth_{typ}'].view(H, W)) # (3, H, W)\n",
        "\n",
        "            output_path =  f'./depth_img/{int(img_id):05d}.png'\n",
        "            depth_numpy = depth.permute(1, 2, 0).numpy()\n",
        "            depth_numpy = (depth_numpy - depth_numpy.min()) / (depth_numpy.max() - depth_numpy.min()) * 255\n",
        "            depth_numpy = depth_numpy.astype('uint8')\n",
        "            imageio.imwrite(\n",
        "                output_path,\n",
        "                depth_numpy\n",
        "            )\n",
        "\n",
        "            stack = torch.stack([img_gt, img, depth]) # (3, 3, H, W)\n",
        "            self.logger.experiment.add_images('val/GT_pred_depth',\n",
        "                                               stack, self.global_step)\n",
        "\n",
        "        log['val_psnr'] = psnr(results[f'rgb_{typ}'], rgbs)\n",
        "        # 使用 self.log 來記錄 'val/loss'\n",
        "        for k, v in log.items():\n",
        "            self.log_dict({k: v})\n",
        "        return log\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    hparams = get_opts()\n",
        "    system = NeRFSystem(hparams)\n",
        "    checkpoint_callback = ModelCheckpoint(dirpath='./ckpts',\n",
        "                                          filename='base',\n",
        "                                          monitor='val_loss',\n",
        "                                          mode='min',\n",
        "                                          save_top_k=5,)\n",
        "\n",
        "    logger = TensorBoardLogger(\n",
        "        save_dir=\"logs\",\n",
        "        name=hparams.exp_name,\n",
        "        # debug=False,\n",
        "        # create_git_tag=False\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(max_epochs=hparams.num_epochs,\n",
        "                      callbacks=checkpoint_callback,\n",
        "                      # resume_from_checkpoint=hparams.ckpt_path,\n",
        "                      logger=logger,\n",
        "                      # early_stop_callback=None,\n",
        "                      # weights_summary=None,\n",
        "                      # progress_bar_refresh_rate=1,\n",
        "                      # gpus=hparams.num_gpus,\n",
        "                      # distributed_backend='ddp' if hparams.num_gpus>1 else None,\n",
        "                      num_sanity_val_steps=1,\n",
        "                      benchmark=True,)\n",
        "                      # profiler=hparams.num_gpus==1)\n",
        "\n",
        "    trainer.fit(system)"
      ],
      "metadata": {
        "id": "xvVZGRM2tXDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### training"
      ],
      "metadata": {
        "id": "hOhJ70qh2Fhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "rendering: torch.searchsorted"
      ],
      "metadata": {
        "id": "fMxcqFAoNb59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd nerf_pl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71Fa1rOU2PGq",
        "outputId": "da7ed132-b07b-44b0-d2f9-59c05f3dc9fa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nerf_pl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir ckpts"
      ],
      "metadata": {
        "id": "48022wkzCE7m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir imgout"
      ],
      "metadata": {
        "id": "XTzS4fzhCGvP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir depth_img"
      ],
      "metadata": {
        "id": "1E2Y2xMUXL5i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning==2.1.2"
      ],
      "metadata": {
        "id": "wJix88eq2jyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "   --dataset_name blender \\\n",
        "   --root_dir '/content/dataset' \\\n",
        "   --N_importance 64 --img_wh 256 256 --noise_std 0 \\\n",
        "   --num_epochs 8 --batch_size 1024 \\\n",
        "   --optimizer adam --lr 5e-4 \\\n",
        "   --lr_scheduler steplr --decay_step 2 4 8 --decay_gamma 0.5 \\\n",
        "   --exp_name exp"
      ],
      "metadata": {
        "id": "XXadp9Ty2G8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### logs"
      ],
      "metadata": {
        "id": "cF4533bM_-OP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard --logdir /content/nerf_pl/logs  # logs是你的日誌目錄"
      ],
      "metadata": {
        "id": "VoTdq6nWAALi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1-SbMC1KUZ7FMotoQvJP8boJqvqBRDw-o -O base.ckpt"
      ],
      "metadata": {
        "id": "nRhXAFtoxX38",
        "outputId": "ed8e6746-aa60-42b0-b3c3-25cc7656841f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-SbMC1KUZ7FMotoQvJP8boJqvqBRDw-o\n",
            "To: /content/nerf_pl/base.ckpt\n",
            "\r  0% 0.00/14.4M [00:00<?, ?B/s]\r 62% 8.91M/14.4M [00:00<00:00, 85.9MB/s]\r100% 14.4M/14.4M [00:00<00:00, 106MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### eval.py"
      ],
      "metadata": {
        "id": "ZgZceL5oxr2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from opt import get_opts\n",
        "import torch\n",
        "import cv2\n",
        "from collections import defaultdict\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import dataset_dict\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# models\n",
        "from models.nerf import Embedding, NeRF\n",
        "from models.rendering import render_rays\n",
        "\n",
        "# optimizer, scheduler, visualization\n",
        "from utils import *\n",
        "\n",
        "# losses\n",
        "from losses import loss_dict\n",
        "\n",
        "# metrics\n",
        "from metrics import *\n",
        "\n",
        "# img save\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "# pytorch-lightning\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "class NeRFSystem(LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(NeRFSystem, self).__init__()\n",
        "        self.save_hyperparameters(hparams)\n",
        "\n",
        "        self.loss = loss_dict[hparams.loss_type]()\n",
        "\n",
        "        self.embedding_xyz = Embedding(3, 10) # 10 is the default number\n",
        "        self.embedding_dir = Embedding(3, 4) # 4 is the default number\n",
        "        self.embeddings = [self.embedding_xyz, self.embedding_dir]\n",
        "\n",
        "        self.nerf_coarse = NeRF()\n",
        "        self.models = [self.nerf_coarse]\n",
        "        if hparams.N_importance > 0:\n",
        "            self.nerf_fine = NeRF()\n",
        "            self.models += [self.nerf_fine]\n",
        "\n",
        "    # def decode_batch(self, batch):\n",
        "    #     rays = batch['rays'] # (B, 8)\n",
        "    #     rgbs = batch['rgbs'] # (B, 3)\n",
        "    #     return rays, rgbs\n",
        "\n",
        "    def forward(self, rays):\n",
        "        \"\"\"Do batched inference on rays using chunk.\"\"\"\n",
        "        B = rays.shape[0]\n",
        "        results = defaultdict(list)\n",
        "        for i in range(0, B, self.hparams.chunk):\n",
        "            rendered_ray_chunks = \\\n",
        "                render_rays(self.models,\n",
        "                            self.embeddings,\n",
        "                            rays[i:i+self.hparams.chunk],\n",
        "                            self.hparams.N_samples,\n",
        "                            self.hparams.use_disp,\n",
        "                            self.hparams.perturb,\n",
        "                            self.hparams.noise_std,\n",
        "                            self.hparams.N_importance,\n",
        "                            self.hparams.chunk,) # chunk size is effective in val mode\n",
        "                            # self.train_dataset.white_back)\n",
        "\n",
        "            for k, v in rendered_ray_chunks.items():\n",
        "                results[k] += [v]\n",
        "\n",
        "        for k, v in results.items():\n",
        "            results[k] = torch.cat(v, 0)\n",
        "        return results\n",
        "\n",
        "    def prepare_data(self):\n",
        "        dataset = dataset_dict[self.hparams.dataset_name]\n",
        "        kwargs = {'root_dir': self.hparams.root_dir,\n",
        "                  } # 'img_wh': tuple(self.hparams.img_wh)\n",
        "        if self.hparams.dataset_name == 'llff':\n",
        "            kwargs['spheric_poses'] = self.hparams.spheric_poses\n",
        "        self.test_dataset = dataset(split='val', **kwargs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        self.optimizer = get_optimizer(self.hparams, self.models)\n",
        "        scheduler = get_scheduler(self.hparams, self.optimizer)\n",
        "\n",
        "        return [self.optimizer], [scheduler]\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset,\n",
        "                          shuffle=False,\n",
        "                          num_workers=4,\n",
        "                          batch_size=1,\n",
        "                          pin_memory=True)\n",
        "\n",
        "    def test_step(self, batch, batch_nb):\n",
        "        rays = batch['rays']\n",
        "        img_id = batch['id']\n",
        "        rays = rays.squeeze() # (H*W, 3)\n",
        "        results = self(rays)\n",
        "        typ = 'fine' if 'rgb_fine' in results else 'coarse'\n",
        "\n",
        "        W, H = self.hparams.img_wh\n",
        "\n",
        "        depth = visualize_depth(results[f'depth_{typ}'].view(H, W)) # (3, H, W)\n",
        "        save_image(depth, f'/content/nerf_pl/depth_img/{int(img_id):05d}.png')\n",
        "\n",
        "        img = results[f'rgb_{typ}'].view(H, W, 3).cpu()\n",
        "\n",
        "        # Save the rendered image for each test batch\n",
        "        output_path =  f'./imgout/{int(img_id):05d}.png'\n",
        "        img_vis = (\n",
        "            img\n",
        "            .clip(0, 1)\n",
        "            .numpy()\n",
        "        )\n",
        "        imageio.imwrite(\n",
        "            output_path,\n",
        "            (img_vis * 255).astype('uint8')\n",
        "        )\n",
        "\n",
        "        # Return any relevant metrics or values\n",
        "        return {'output_image_path': output_path}\n",
        "\n",
        "\n",
        "    # def validation_epoch_end(self, outputs):\n",
        "    #     mean_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "    #     mean_psnr = torch.stack([x['val_psnr'] for x in outputs]).mean()\n",
        "\n",
        "    #     return {'progress_bar': {'val_loss': mean_loss,\n",
        "    #                              'val_psnr': mean_psnr},\n",
        "    #             'log': {'val/loss': mean_loss,\n",
        "    #                     'val/psnr': mean_psnr}\n",
        "    #            }\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    hparams = get_opts()\n",
        "    system = NeRFSystem(hparams)\n",
        "    checkpoint_callback = ModelCheckpoint(dirpath='./',\n",
        "                                          filename='base.ckpt',\n",
        "                                          monitor='val/loss',\n",
        "                                          mode='min',\n",
        "                                          save_top_k=5,)\n",
        "\n",
        "    logger = TensorBoardLogger(\n",
        "        save_dir=\"logs\",\n",
        "        name=hparams.exp_name,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(max_epochs=hparams.num_epochs,\n",
        "                      callbacks=checkpoint_callback,\n",
        "                      # resume_from_checkpoint=hparams.ckpt_path,\n",
        "                      logger=logger,\n",
        "                      # early_stop_callback=None,\n",
        "                      # weights_summary=None,\n",
        "                      # progress_bar_refresh_rate=1,\n",
        "                      # gpus=hparams.num_gpus,\n",
        "                      # distributed_backend='ddp' if hparams.num_gpus>1 else None,\n",
        "                      num_sanity_val_steps=1,\n",
        "                      benchmark=True,)\n",
        "                      # profiler=hparams.num_gpus==1)\n",
        "\n",
        "    # Load the best checkpoint for testing\n",
        "    trainer.test(system, ckpt_path='./base.ckpt')"
      ],
      "metadata": {
        "id": "wb64zpMdxs7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### testing"
      ],
      "metadata": {
        "id": "FIPcSh9F0Jgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 eval.py \\\n",
        "   --root_dir '/content/dataset' \\\n",
        "   --dataset_name blender \\\n",
        "   --img_wh 256 256 --N_importance 64 --noise_std 0"
      ],
      "metadata": {
        "id": "X-XJgzVhx0Va",
        "outputId": "1944361b-09e5-4c95-b410-3ff9d6d80050",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "2023-12-11 08:13:04.926465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-11 08:13:04.926524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-11 08:13:04.926562: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-11 08:13:06.103425: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Restoring states from the checkpoint path at ./base.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from the checkpoint at ./base.ckpt\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Testing DataLoader 0: 100% 20/20 [02:00<00:00,  6.01s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import imageio\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "image = Image.open('/content/nerf_pl/depth_img/00156.png')\n",
        "\n",
        "# 将深度图映射为彩色图\n",
        "colored_depth = cv2.applyColorMap(np.uint8(np.array(image) * 255), cv2.COLORMAP_JET)\n",
        "cv2_imshow(colored_depth)\n",
        "# cv2.imwrite('/content/nerf_pl/depth_img/00005.png', colored_depth)"
      ],
      "metadata": {
        "id": "UDJ4ZFc8d27V",
        "outputId": "dc0abc5e-e7a4-4ee6-f430-66742a032bf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=256x256>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAIAAADTED8xAABDu0lEQVR4nO19b6gc2XXnr62WXhFKVhNaHrM0wbHFosRC6INRhmUThpDNmiEsy2CsEEJYjMkHY5awmLDkQzbshxDCsoTF5EMIYTEhZIwtQjCDCcYMwxKEWIIQWq8Iz86wNGEiN6FHbkzNUw+9H27d0+eeP7dudfd7I82rg3iqunXvrVtVv3PO75x7q3oEfAeDDHJe5SMf9AAGGeSDlEEBBjnXMijAIOdaBgUY5FzLoACDnGsZFGCQcy2DAgxyrmVQgEHOtQwKMMi5lkEBBjnXMgbeAcbAOv4NhYjbu/W5tnZ5uSg0zzjeYwyDFErJTT7Ug9i5n0xD75AJNqNO8ABr9tc7Df0VG2O1se5qyEGvz5IvHLNzlTQ/GxlbN2fnrg4ihUMqR+T+A9MGrryhdy0CSHTV67SOO56PsE49YCHVkHFavk7riH6o4diqSfW9/nVloasZfRCnKC8xoTO2auptYXjEofzA6Mn1Uu+SEeZN0jgt1BZHd2KezuzEA27eGxTqxti5gborF+EfUVcoetFoWKc1tRNAfIpjdTvWqolmR5wRadfBOxd9auTpJ5p/3mPr1LqOqKxHIppo0YZKmBWNm7Uatrgb5m3hpzNvmh6YV9980Ejtl9iGDxV9dhMVAn762sUwzIb80sRlbm+cKfqoeX/BCjUm+AWsVZ9j9QiNUVp8TnQlnIypzyaYzB48xGfwuk6vRVfTJeIoH4mnS/ruadEg0zYlPzA9qsyJoC5fVFg79y1zUl1NozRzW/T4vVOMP+LU01quL6ZEOGiEbaDezMfpmQeoqzVtjDi1vhy9bZo3fTkk4nK4RgkCmr9jpuPSusG1wgvp9NjMOtp8ZkSrOtQzFT2LYYzZP3Oc3ulEJ+Xj5IUdqj4C/tJR391EQLOkw0y1gwwJ6ZMzNWe3wawdfBQOpm/DQQ4vFARnlLuXaMfUq0n5IS4eHxP9ZJj9boPJOOX9Izyzn90eyiCuEAXSPOQgNulsDFshlM2SU4JUxo2UV9aHBkdxYOHzAOdTzhhSA4KfL+HzADjfmjDIeRSxFmiwT4OcLxkWww1yrmVQgEHOtQwKMMi5lkEBBjnXMijAIOdaBgUY5FzLoACDnGsZFGCQcy2DAgxyrmVQgEHOtQwKMMi5lkEBBjnXMqx+O3t5J30/MGxU6UuVANZAk74MOf0ABvthl0EBDiN3N18HUKEBUGMVNsLfm6PXGLirnboPWvFOVAkA64ebuw0qAA2qFeqw8dro8we4mPMkgwLk5AubHxOmEQFtovzQUvRiRoWmSTXqa5s3ADSoQvkKtVaSPxv9xOHH+8LKuVOAT25uCxyDgdgs3Edujl7bvxNPH5o+/qTGKujAFzY/5vpAf6nke6OH+4/4RZEPiwIs7wBbfnHp6BkcWJ+NNKjCSR9u7jo6oD9UId7Zb1R9c3t7xvyQAr5L5Gc3N00NOXnv4nZck9cLe3vO5flWgDfvbB8rPb5MiSXCkB+QsQSUh78r1FkFI3A3QGWBvomX4QG94bsPN3dpDBzZ2qKjp6MoleWd9kau4ujENoBXXgAlCd8FOkP50zvtBj0U/nQ0pnspQAVE84+UsqOM7WQYP1Uw/UnQhIC5GqsJliFICH85TENJhebm6AvAOKJmDUxiUmgVQdQ83NydY7bAdInJHLMKTdimE61QLzEBsMB0igUNj5+IBsljAypBGiFAMSIIDwAaHYCsAvBC0YrKf+sDVpIDeYCv3EkcbOVsl9nswwqxEXM3U8iFG3gy+bx52FhgOscMQI3VHLMQgzao5pidvHexPcMCADDFpc2zCZZgChNOFLAe0Hbz+/8lnGCLLYGqOt2eAFOgBipcuvIsKACxfwBBSabtIJLBl0qnB62ydcTRP7rToTAAvnqKSpJVgC/eSXbFjTKRrauZdfIVTk09NFExqYuGOx0i2hN2F5gGiAes0+4C06fvXsYcWAANsADmwHEC1hNcfFJdBfCkuooZMI0X3sRWtLFKQUMY4mCqogLMEHo7mV48mV58Wl1eXpnQJSwwnWGeUfhSfchAfE+OSSpNV/dlpiRaPcLGt3dUEqYAv5rCXeA1g/6MnKGlJzl57yKxIC7c75uPn6CMyHACrIlazDCvsSKIUIfBIQRL/+Tdq1gAx8AcmAMrYBkVgJRBS912FMZBI97K+n2ML8iaFXtMEwYdUo8KqDDDnHM8SuDCYjv80jj/seXs0gq+BD357B0gHU+jNh4ZSvJBB8FnqyHc2IfnWqEhrvL0vcut3Q21wsTrIm5UMVJdAhMAuHT0LCjbyfdjemQFzIFl3JgDDfu7fD8ac3L2dft3iXiOOLqtxMIQAy8Z6icX2sHUlJxJuwEAzDELOiDQ30uSFNBpiO45f6480TJr3jCUhCnAX76+dQJ5uiJQ61X2wN0J+iZFQuUf7ehGWnriNvxQ2GhQXTp6hiOc4CLeBsAQDHbXlnFjgRNcxAIn04tbS7yKFVaxB9KoFVIX0KR/q4jiiqlg7djYeAuWNQCsLrRnnwE1MAWmwHVg1oYBUywmWDaoBNkTAbF3DzNHt8MpF6+yBnQ5xPcQ5QH03dB3oBiCtnj4XhXECVyovhqPYEH5GDeYRmL28ysz/BxO3r24BeoCeByZTJMa8QXj7oglnKWAgXlZMYhrKzLZjqj9x6tN5Q0KjAjMQYW/IQZYAQucVBdx1NIeE/3eLpWE27LN/3BZZXcbZ/dMYF0qlVCAEvyVQ58/vl7INk9Xoh4KV53pHZIpFsE/zDBfoW6uVLiCxaemS0xO3r2IW9EhPAaOWVwLxukDEAOSm7i7AmbAMeIxLWS9gUnE9OoKANRXAGBJ7P/KFuWh/0m096uoh7PW9mOGjx79KNh+ng+lTGh7nnRI2ieEkkD2eD17W++SEO4L6+8mnsvkWpfWycYAGuu6pBDZfa07ielt+rsg0oSAcqEVwTp6y3uaK9XT6jJmwOOIuTkwAa4Bx8wAV0mPALPp14EHwPwCFlewro20zvhCUIEtlaLLDFpBLIn6nAIz4BowY0ev4aOf+hFF7WGDAnp9W3iG1JScBwjX4Vn0Qkvft5oH8V49s058BehEv3nfMnx9Zx3Qp8ug39GN8jQ/lxnmABaY4gjVUfPkE1fbbgJYHwBT4BpQAwtgBiyAWynNmcb8zyRmQucX0FwBriQnDMn7GTCPurQArjGiRbYfUQ1q4BpwC7iOSy89C0mqGebXoruBw+CJ7IW/QQdoSpua6JV2HZLXh14USKM838Qz8JV/3ljNUYAqu4uyUMErRIEJ1xVM/dGFldiTd44HwWKS2Gx1DcdTLEKC/wmuJow/bEyjDQ6uYAIs8cmX/qGdCgAwAx7E+GEeGcsSmAHL+GCutbNXrRYFHl/FhBI5mZrRnhvADB/7uR9ex2OaULuGY57i1AslBNy9wtCqtw5wETe+BPd9w4MMvkvq1MBKL4X4cjobgAL0F0I/00qTByiceR5A5MWj5NfDla+SoIz4AtNjXHv67uVLV56dfP9iC8rQgvKZFAGvgE+0ygAAbwPHcSKM8qTH8ZHcAgC8zDRqxYjNPF7SNBL9W/jYlR++jHu38ICbc1IDPmyR6ReTGObiiNxKOLAEANTaBzIjjVXepEdFZbMf82hJBa+OqpaNAVBAe/JMqbN+Sf8BB6YHyHKqPPpZH7b5D7yIhwc1VqsrNYDlpyYr1E/+z9XWKs/wsaMfPplc/einfvT03ctY4NKnn528d/GTR/+weqkGUL+0Wv1c3aB6+n8ut5F0AyyiH5gAAH4JqHDpU89O3r146cqzELyuUAf/s8TkFbwZLPQtPJhhTnCnBRehMlKjTrv8GqkCopnnwQB3AkVCllVQF1HeaeDNfsxWZoVM/74f6BkE5ytk0JzXk4wQF9KewRdzJrivcIVZYMq9RIPqGo6bT1dzzG7gUYNqhvn8aHYNx82VqrrSNKimR4sQQgSuFVb43Pv0yw8+fQv/Fk/fu9z6AQDAJz/1DwtMScemWFzD8RKTazgOoH8Tr4SUznU8DqwMLFMZtvWELo2/hMmIgFhMGoZbevLexW1PjbVsgbAOFguZYatuax5ll9H2mRHeFU8nwNWBshgAytZmoL+n1c9X1q1UBUI/J/ra/GeWSWtzGABHJnOKxRSLOWav4M2QaQFwDce0rI2McThvmIdaYDrB8gYezTGbH82WL01WL7WJVwAv415AatgNBv4WHryJV67h+BYehJ5Dt0ThKHIlM68XL1FNXkGrBHcIogKfK9xyoSrqACyIcxaU9wxIFUa34oW0zc8rKvTxA5YC9KU9hejfWan62H7WuoFCuRaT/VNSiAeUtPCYJs6CJaaVNpRwpEUHoW2A8u3Rq+Fu39/8zwe4dYxrweTPMbuOx5TDCb5lickEy5ujLwC/cH/zjbDU+Xb7Yk3Df88zvBnM0/xhQoO/qwAnHSQukKs9TxBRSejw0tGzk+YiKjYpLtDPCY/Hgng5LDUwj2oT7nEhWD1Ynagg+Ct9lsTBR3+mZi+Tb6LfPFEFpORHm3+UhcXaXdB6T4o1KcUeEi+8CZ9nCBvxvXhubhoAb22+G8BNlW+Pfi0+ogkA9pGIhjfkP4UddIBiWR3C6njXOySOwloVlywKokGZ0bAYsi4X2yVhsdm2M971KtgvxHzFWRFUiFR91MRuvjezZq1CYXUW8TYMUkxr/qPBqivzTvjC+vCXlvIT9Mn0xpchx/FvFQFNIA44Ht/f/PXt0a/FBxUSQJN48lVamfSBSrY6QHjlyNb6IBQAqXqIhryCnBfjalCYEdI1M63MQzschXJT8ZATA+xs+HVl0/Bn0J9Pd3pJUgBd74JpMUmz7oFbd5574ZYbqS7dHr0KgEEfEf1gH0cZhynhh5uvAwBWwMeBVXxPchnz/+PIHsapDkD8znHFXtPhukrbRGzEtpn159kwRHfKV0ZcOnp2gotb4s6lYdyjUuQ+nmBbYlIa+IeqbEN9VPccD1kKcBqGf586cPxGWkfbfljkx4t9taMQsBbqRBRIsywAFvTXrAQB+vc3bzSo6BXHaPjrqAB0nVU0+dSPLkl4vEhi6nXgdNVe9ByOLjERKynaOkdsdLyR4Pom9ffi5vTc7f1A2ptuuJ8O9JwHyBh+flTDOo/gjBZlGwrGLzZM6s86bp/6ZAs1PeqV3vXC66p905e+ASEMf5VqxccB3B7dABBxXLGrbYCPp3SfhCLgKrqL7QAI3GLki/SrcnQhPOGTyf8ISQJinhgFwzff9cJfkTCFUg8Tx1AlGQciaiod8BVAgziDfs+Qi5paQ7QWeadAN+7Dg9ETWzwfAivGNZOkJnHSCR9EL5GSfgF6Eirn1cbs6Jg9rjGz8VX66LhLWd8cvXZ/8wa/FrGkR3iAKn2nmXJfhH4+TQaGeD6/Fsq38wNCOO411gVeM0DXolt1HvV1wPk4rjb8hejP1OxEf6Wa88KMjllWOfD1CZaCzxBY6Z/uRCQ0RVqTH6UO/aGtI6wJ5VorxH0Zp9tUWZuE9tDDzV0zaOFXQbsUxojPRlDlcNOmWORTZLznS0fP2iHzh1Wzy6rTXY0BAQ9eXqUb3lFYR2Ft19s6zmdR/kB9vER0bfbbCf2SgXptK8Cy/V6i04wETDafSYyandNUlMj0p+SHR7rc0lMhVDnH+prRmyaWgCV/GnUo+VooImsn8qMzQuIob8h3qSuvkEq2X76InYZ6chvMHpvldFTX3+GoPh22dbIxQB79JdAvqeadRXuMbYENWaSo1fNZGuKizwz6ufKIhg0qK+0Dx9JDKYCo6Ya5rDKnUg1VEL7I+7xFjVWA/hSLoBWc1fB+9IxYpWaRiSZtRwRGdTQXoosjcVI0SSoJfuanUsgWfVasLVjlunApRHmqpzPXuXe86yUo+YZ4wQXRYNPbMISGDMo95RF0wsyiAnDMfLikdUqEKlWzSfvhakC7QbYp0Zujzwc/wO7oNh4IM8ShXEwYh0JO+vk2Uq3Q17s9egRQPFAlONti0YxTkepJpjnSo2YUgWxeKN3OegA6vTi3uQ0f/VVXBTrUH/q8nHtnnb5EmtPMoFyU65rUT4oGilYJ0GCHCP1Ioa+dxpoZ+IbtjtU8Gq8WSnBz9BrXARK9Fog7BzBtEToglER4ABFzQ7w/acKOY12YZF7NPASlIfmot0sHsgpQPiFQoiSmgfeyQBXQn/FrpGboft578B5MyhQkMh8org+2oXkODxKgCk04ENzNrGgClhCNUEhg5oV0VsecEBAvi9XZ18eoH/kOMVQiSFxclSZANVILZwD664CjAOXQR0/Dbza0+jdznXDIPR0y64vCKk1iIoW+9ga6t0omPekvXRK/sRUr1NGwudEoA88pUJ3+fgxXjIQvcQ4T3ivQeUxRzZwy0zoAJTo22Np4DURN7j3Fh+rhoH7A/ziu/oqt3u1l+L22VitzWrdKc/NVmsQsWdbmGf6StrXxuy/iAiqL9/PyiilGldKhcXp0zZYAEfqpZK0SRKKQtmH+kEzJGrjMeiFYKaCOZXO0sUJSKHZFOZVw3Sg5ZPZpVcj+SF6l0OmhX9c0e6OGxPg5eBT6WdOG41VAHyp7I45WVi4//BXZbjP5I5L96WwXIbtmJRzTwsDTX+4i6tRLIO2KSugUVeyhsipXscO2rVD+Ss1yIE0Y8FuEOGOgC80S3kn7HAUYRLCnYz9ezm+YaVLzh3gPukIF/NXrfgyQAfQO+dCqaHzeWk6kvJ8f8tY7mBxJVzPZlEj18MEE9LOf/eIXME5hXTH2Ok4pTYDpOjUASD1AHa2+7hOM8FTRRVBDTpbWlBqiCeAaK4/NE4nXNw2MCFEdftP0EiM5ahp7rYg+r4Nshkf0AOcQnKO8AoCin0nNoN8z/HWqwaKHihX6Vj89p7TclfraWaUmNfkh3Se3fNxF0BteuvPU9q/jpQr2Mo7Wmp6n8BL84rXrICuO2A91Tv3Aqqyub9sEN0ev0UVRDODZCOEi+C01OScvEfdz+1jTx50UckaAdEPzZH2tmVZeQ1ahDwXKwNpzQ6ZK6KMA2HccOtMyaR/JogaP0ENhXfdsLiKymA8YNHmJ8AmVojFaBMnhd0rEBhz9lWqlFSlIkjIyL5NEZM/yJEerkGllAFw6epZwIQI92C7UbqcO5NmEpwMSkGNfAXaDvod+UZm1ohukiT7SrAuchwH2PDjKdTm7oFWGIyFFibL92uhqHeBMnZAtVKJKW1XsTtVpcxPxogRpyTg9XavAecYvdIP8oQ6iMrdOdBuq2TpQp7tgu3wjowNmMIn0KNLHlTy6ta8An3td9iIbu25lOyBzEAr6aZcuk6FCL1+pt83KIgTkYW4+rgDAMi1jB/HCQoxTZBN8BYJrpUVr1jndNYF4pE0qa0gQOkC3UcT3mhOaN0HPr+sekD5HVwewtw7kD4mjVgXPNaeNRaeVU62knKE/Fhg32pul8ra1Oa/8ZH9lzWVqUhSqWcl+ujZt+MHiXdN4c+8htpFit46B25jFG6s0nhaqkhCe7Z1g/dB16TsQpDMC1ssiGvXdoSB8is0eUTz9Niau+8x25Q8hOwlQtb+8lFUAauPtdub4YaA//xYLHDQjfU66UBshUQhmvQTTNdGPZKIXPvpFBdNII9UHUUH4h/Qqt0meFatDaaIm7vL6XNa8VVgowW9ISPnX6hVK4fr0J4P0sgghtfq6DI7YSqEg/CRcBziC++qA2RB6pmyM3ecBMtTIczrqRmn01+oz9vlgILNIASnKhaeu0hkA3T/cfD+/DEHKeYlm4cLeh12dCb2ggmnBcMAq8KvuJEIQREjff323dTVNFDsDYr4rSW8YmohoC/NClTrUjw6tszGAGGWmIz0CB/qc9JvoZydMUJ5ZueCVmw8P6cMwNSGtQ4CjyxinKkGHBM+BofFJh4hWHFuMji8wHyBAbPbAT7FWNcHGnAyG6wBduzYBXDI5U6g7b0ZTVG0bDGhQCTjxEs/4mq14ffNolC4KZOI+f6gytvenPZrMeOWmi2jUV+JMK8XM/+fjEUKPwDcdGjMGItDPdSYxw+lGlJbMX2iH3PqHhlWuIJWzSv3AOi1cs+HxmmtO4ukWESOS1CWK+Ii0KGnST46KVmAxw3a1HN2VxmI4oqSKNekQLzFbmUcRKoyBddcPZd+zvhHUB/rIzu8iG+/mtSJj+M1CHgTr81rMBzqRosrpatespm4SNsIz5BMInP0fsVD2/VgfbBVQw9bAIf1SEBVuIZ4uClqrthBvkNHbYZ3fxtILgfibZfprQtSt6MdeLARnvVCmZv6QefT32l+M7PIAKIM+P5SFPhS+xVHWvYH+8gUR+eDYOy9b4yBoN1hhkIYd5egXlQP63wHeSVkKz3tOAAAfxxosNhA6xsvXbIkE1WycdBB3UFWqIeBZGtoOd8lcAi1uV2UlhcKhzBtkJMlHJbiBF9ZdW3TzkGvp06NIXGaXApipzAwXitsm9OHjVdTMZPpp11SkjOEXTXTnPE2uAC1oT8Mumx9aM2QHODbx58Q4KBvW/5r95UIaWAGfUcqgk6T0kCkJCycjBIqGw6sz4b02NrgW05rqpL1JJYk3LldSqTcHQB+d5ggWyPbQzA9p9YBPh9K7nBWhvXvT/bTvRhzNh7awYG3275Vo3VBf8iHo6HTneNtZEgqz4QMMgkvgbeAd69LXqUPQCkCHpsCS/V4SrJeGkQYM69Qz8JHXsfkKGNOrM1X6dgsYueevBQtzrnfBtEKUoD2rjAfA2rvI1mjW9r6XK0hvsS+e4begj2LOA4Vv+NCHhVqvvmf7dT9BrHkuofFQoS2vTFhPjGukPQ/e2nyXd8QpBy8xDUTkY3X83Un19LY6gEiQKn8BKeKA482I2iImBwij3kwWHeVXpCcHzOkCUWjHxE0KX0GH4Lw9nNEBKFcQpcADoB/0YSEYBfg2j3o2voTiiz5Fk5TwwE81jrOKQX/rtPLi/ub355iF6FDMIq3Ub/JpBaixeri5e3P0BfZ4yWOsIWWsskNcISt1FLo3DtwAUJ3PIeGIR6oGZgl1ov0J6UCosKVDGWTXDufppEOQM81dWSAA71ivhu0EfV3Bw3dnYoeXVAWvOJo+xDH/mvPwQzreHbPbQYWP727eEK8Ocu4rGLaW8MsDN0dfAqbAZ+KHcom9iETQiu2u4wNvWBMzfUR3co34OYlGfUXd/EyQ+fpYvg4/milEZ3aISlaqgldTbH/xddL8Mg/gQB/FX+fMMyJ91DOQGdpj9tCF/vHDzdfpvt8evapeW6HdJga4DeM8wjPIkEB7JG4OhVGs0sU22M5FjFM9XCnGtWYlYkj8+VcMF1yTtzck6ABn8MGE84hWvArTpF+XEPxH1+HVzMJkvMi6gl4hgdiON7BAASq14b+xXsh5Oo/qXdHE9AYmp8pEvW9s/vYeXqYc9u9uPlLhx7HdUygXhPizABWa8EmpXxj9IvAJ4BMqF5QMhrsC+qg6WUF9B6r2+3PjlIAR1ychG8+fLSdIFQuUeZwtEqbttvikiogBNLkHUxKP54iG5kclqA6nQ8n3hTI6AD8kQEqHxM0DgHFvDyDWchyE82S4je5To79KZ+87bT+Ah5u7D3DrHl7mvliIl73m5b+9mf/haKmMdEuK+CqD8OApABB+QJwrNqxVt+wOtb5ozcx5EwvBSjjQeSsw1aJq0AExDVJAXHgGUV/7AWql+6HOG/PbKnkdoBLuMZBNg25l3cMDeNDn23vmeUS35iEzPOA1M+oRnsr9zRtv4pXHuC5WBQvp1IoQJn5tc+83Rp8BkFKgsXkrEL9GSEfFooOUj7EO2vURQdapw6FCOODmorUCQjE4F+KDNM08GNyhmA+/A5lJA9G54QqqFNmeDvAS7Qqs+1HkAUqsPrL41kczOtN5qJD0I0V/eK4Nqu/glx7hhojAuJRoRY1Vgyr8rB2Lg9dIAU3CJ5toVGZykLUN3U7azsfAmtSAGDyn9fR4xyzSpaNwpo11PLCm+QFYxl6nR02tgGPgvUkx4U9IN7YTxhzZ+VkCdLmC9L5kpXodm+3bdOKCUWz1oTABZcg9JsPvVGZOIFMhPNGA/jfxSpMuWeFSqBINqvC7qL85+ikrT5oYM7o6QoBgGiTElFgAsAbGoKw9LqTGnoMYDNxIR7VSXIgKeTxAaEpcAVQgS1QnXI75/oDgOXohHb+ZOpbgcYKxaILDmnsGdLmCz77OT13kAXpBHxb6O0OFTCth+83wwNSE8JdynQ83X/8WfuUb+BzPu4lQDA7zIaFwcIJleF/2/uZbt0evxDRlReBT8wytPNzcDfSGKvC4k0GER8DR8LdAPfIRL0A8bq9JBsFQu1SIVFUSvaUSPb2VX/jggdusBuUxwD85WvWhQ1CuIJ1IKQuC44XR9g7Q10f1oU461Jf0M1kvMQm2n5fyXZHpE8ILww9lhxPdHr3KolXEm11ZmAPa5GbyDET+MR38uDVcnOC0a0XBbPyKuQKkG2DPf63KGyeFuvWcpJ88JMhEt41a+CAuqmSNkIg3EMNieAuHUOYKAMEMe3uAg0Cfl+S1ojMMMHdhrPBpj3K7Iu6vTtvTITBKSv3E+oQ8TamFImkXjgBKsSanauvE5nXsuO3gAtbvp8AdKycAZvgrK1WqqT/Y0a1j4Z/a5Ua6UfO+UOEBBzfdQ2pO5/OqiS9Uu3QIviuAyoS20oZJPTzAwaEPH76azeuuPPWwoD9G/B0XXlNcoHgA/MIFL6rQ0C8E39985/bo17d1W2nirBkcNaCntHUUBLWbo9eiDxm3nU3ZNG4FrERSiB4+t/F1miqlk3JntRZwt+YHIKYIxFwYFYaS/AIKfg95oUeQUJIhpQ0T8TL8HfOViEUK0Al9FJh282i5QzBVwmub8oH2um+OPv+FzWK5/QFq9xqRPp5GTXbyUCTmK4VFmjCLPVZ4Qtoksd/26lR+ZWvWou2HzgumKJw2kQ7U6Wo5MY0AX1uMm2OmR7mYAQCJGShDmXytGwC2P8aRdwVIFWN7f9pbWaQAPxjd/+Tmdt6uIwv9Ep+QL89wHkV7NPrbkikWc8w4+eE3ulLZSTqFLqExPNzcvTn6D+JEwHXgAWuhqfla6KfiIXS0wvr91OTH4+sLwAXgfZXXp12tA4ILCQDwhgBqtrIIPDcKRhq91D5SKHcGyjSITIZUeOluV8AP1cANngJa6+t3pZddFxU6FSMfBuhy0w+kH6wdI4Fs++AneKLH4zEipIGBeEjhkUywvDn6NYb+KqLw7VgSREfDAmqcjvPkZjy6RsS6Fq4b3PePHZWgahQkePGASDSNka6dzgdOXobHZFBQzIfX1HQITAegXQEhHl4MgB4eAA6y0V8xell93aFZLTX85kZr9maYm/RJF/KHxJ8ct0zhcxL3N3dvj341vcfr+BwE/rhoUsTRVqVpUOptnXai+dBY1aE+xRjW6f3hbcfqKNcBmbbiQBd0xdMBOKkFwXyopu6QPw5kkqRwp8D6eQC0XXUEA+VWX3ebDwO8dJDzDjs8fdAWi0/umgsThPUS47/dep4Vs7JcOJ/WbERUA7PENTvEz8i3OZnhJKBS5k67FwiGk2qg0AGIwetfIvPeF8voANL7mUmGZlwBVbZdAQQjkiaDX6FpPOhW5di8WUEfLXcIugldvyL9Ju3RagBg/Am8LZBtXqb3QrdQwjiSdTojz5M1XKp0YOLUnPasWRNSqqUFypW1rTkJNSFDLk4k7CDXAd18bOqAmbnXboEOlc+U6Q7F6boTRCA14BkC6QFc9MerL4U+FMR30Apt+BFh57zFW8VtXsiPtr+BLqy4UAOT12pNqOKPCUQXyztZF0y+CiNNQOeYo8rccgsykzNbjoh4QOSFYMUDNCruSYy1EpwLQekAUp7j6YCohnSiQPgHsKfjRgUAKmDyOrtdfdKgAN4a/eMvb34SXVzfrJCHOBTW89Usxu/RHq4Y7YYXtMF3C6JCqEO/oxHfz+IoFJZVm1IadpP+5U1owx0wuzpNhDpFU3yuGCL8hdK0tnk+HtBn7VwVp6tBZSP46cDSqR2MqL06fi19JsL2h74XH3vQp5IU+nAMPx0y44ExsL45+vxvb+QlwJq7EZGZboLtGqQxsNQRlmWntTeombnlJIquYpVerA4wVmkqSXsDzz8ItaxTTavSOTI+KpFI3S6z1WGuuGleVs2bBtbBrmjOC3UnEMFxctXbu9BDdkh9QqGfY1p32BXsgoGjUiXCA0ChsA6ZUDFCKmkMHBtCcQgb1UrdXNtkqqMc2eQNqM/KiaqDkNOgOpl0kE6JqsuS92SsyjkXkjoguqvjhxbZCQwcgzEcqDRDPjLmEV0mKgC9XyZvRR8FKAlzzaN5h+BFBVV6f9loheH3cFal9bet+DBEujNj73l5OvLKQr+OL01LTHyDDxusN80iCh8Z1+RCUsRPsbY0U+dGQeXi17nZOBqkk75IpxpNzmlyJDrqYT3PiE62l0PX2EcB8mwH/aHv7dKGk+DnnEczZjKiY+vqxgLZUyx4zkcbrSD6FZDYD9dAjmZtkj3hMRm/zLWj2F7qhh8ds6VBQkQrk9/r+ryaVo/t2ExXQMItN8d0xsBnkkhwsG4eUs+0ZxBMkgF3/qh3yMsFOT/NItBPD4NfCGXQ1yk6AcWy9HgqNPSaPKzEPyIFigEAykwsB42+7VVa02vOAddY/Xjq1+mIzFOQjFW0TfWpvG1CU8XCWJjMHhZSvdl3LzI22b/IEZ2MSC2T+1PwXSAmX9r8Mw4HfVicx4l0Y7uk3NQNU2iczd3N38wxE4Pnowq3jxuMlfrZ9Bnmt/Dg1dFtoIJcXZexpp6YZogXZmKJ3YSujlhcX5q0ZmpDxmULLHqdkv7yd1Bpm+4zP8oLuZegarxOvpwOPRm9ZT6XfnfzVKGPHPpN2rN+uPljRDtxc/Rr7OtR77y1+S51fnP0pYjLZoY5N/BcuL0xiSm/D/GKKivW1LyCEwYhWmEylTPlpogxkPMRoQXtetwm3zlR04Y68d75hHIFbBDGF1NM55BfOA32KOnQE+e+7U6BSiJgM0LIZveRopzc6yrea4J483DzZ2/ilYDmCs3XNt8Li3NWqI9x7Tv4pVB+HY/Z/OVygSl3oyQe1dEZBrAsUIHhzDOf9GzbJlp0dJERk/BonqMHaTZBduRcDeh5bXUA8TZWKtGJtvaWKXnLqrlouHvlaZpI3MB2tx8F+t3N32OP5M9OdL/52uZ7S0wCcK/jcfBooUS4OU9mmF/H4we4FbQlfHWw80YH0a62QRUo0O3Rq13cOi/l1mds6U9ngFulSVVT6CboWLxigM53om8CuYI12NclwGik5kVmoX7EZrnHfGj3e6OHCv3orQC/v3kYNnolfzLEqZPx39/8xbfwK4LnkVDP4pDm8fy8FZpbeEBfaBOiz6IfzxSL63g8wdJLemiDp4dhnh0xiKS/VC52fSnUq/X9zRvBiKxQB4MiRhhubzAWYeZb5eWgH1kU0gGQGggdQLEalBQiqzOpAmyltwLsBn0q6bL6zd3NwzlmVfttQASzbaaNvZl2KGPTdo0KQHiQ4YsmM8zNy+zUgeCLaCTeZILZs17dZNaknoWXF/WFr++UBtUcs8e4PscsvBskKpj90JjDtzBmmIfPAgQTAKBA66QOwAqLkYK4xOqXuIi/G/19OpitJvRTAAD/Y/O3hWFAGeHhG+s/2fy/B7il3Rn1kHEpIm8TSoLyeHB8Gfdexj0TrxkdCOf1HIgW7+zIoi1P6kRl3qEX3jSoHuDWA9w6xjXeuZ6O9AZGvSH6h2s45soA5r7MtuKt4v1dAbIKQxtvjf7Ru5wdg+D9oC/O23rJh5u7f45fD3ila9aexHxgvKRh6wTBfr9NSI3VAlOqlgGcieAFpjX70Ke3xo4/EqFpVfwAP1jSusZKa7IYNj+d2aFu9TY+8W18loyLfoh9pUG1xCQklCdYXsfjazi+jsdvbb7boMrMhdFQuaKasW+jpgV0YWYhHRh+mPBoao19skCmGe6a0yXZUrGHm7tLTB7g1jfwOfpFCa7BVXwJvUrfRtdjE0mG8ISEB6iSL5qAvyOfMdXmicRf8xHq+kK3ker5EpNOUuQdIhUSJXPM3sQrheg3nybXKKHGK9Qh7TbF4gYeheDKc4/aJprA1dL488ElKVEAEfoy1t9lViWzeBPW/TU/gUbXsMTkW/gVznygbNgKdfgGCe9cr+jkF7zA9DGuLzHRXYW2lAjyGFeJcDSY7D9kjSjQpL9CLYNRJ+UMVBuRa1XqNxC8oeoBNKju4eUHuLXE5OS9i+I3rLRBMV2ZUCpxlnAtIah4hBsv494NPKrZ5/q0aJPE/Tbf1Z3ohRJQrkCrgc7/7OgBBAPJr5ALcn/zhr5sMKSKhKZAZDDS9HvuJvqpfiC4XoTHJfQTDFj3ZfvCm4uYe4HpI9wIg5G/ehIHgabNnTzB1Xa7iYeAjx79KNBr4tnsPYSOwQSZY3aMawH9aHDSXDypLuKoVVdCv5nbEBLuZ61eQ+Gnfozr4YGG4Cqvt/QIkK4y5Ljn5/IUw3QFVP4Xo4+wc67FxoEpEMoMqghW6GslwjRStce4/hjXZ5iHkIsahjtCuTxNePgIub2nTnaw+ubFcgmYe4BbT9+9bGOJGjXsL9LtBgCeLi8/xWVqdenKsxnmFHqqXg0OdoxrC0wD+ul0T6vL1ZUmhK2V9T6G+Uz5DRSuj5uw4HMqNLfwwGSGJBz6/BT6ikqsPq+JhGT+hHP+MXq9EBPk1dFvvLX5YzG+fNxmCjUJM1xho1Efbea3dYnJI9wwe0tXe7cNEEeJI+OzcHRSL34VwuNdzbapziPceOu9n8c8MeTba+L64qleozbCeRY4mV/8AX76B7Of/tiVH17H4xnmdF1wMldSyVft3ye4iisGBYKlSAL6SE3YCvXT9y63A66AIwSnF7wWnT3jWDjQOzkYteoTAJiyEwUSg4Dy+/kmpnpUcXGlyNjQwzPwDUUn4ECqwQku4shIJWWGZHVT8VgW6mIXmN7Dy0++f7WdXaiAOmJiBXmSzH2q2t63dIiNFQ2wwJPp1Sezqz975f+GCQ34WYHklq5izwDmeLK4+mR29aNHP7qFB0GRyDEKgqF7DtmqFeoFpk/evcpPHu72HDOato8n36a80suVEYV3Obx+nwDA8wDYZTk0jUDnXD0keVrBkVRjNcVCx6yt744n1l3wYQVp3wDCtqH4gY8w1DALVkKBSuosMP2b9/4NjiHn1vjVVMkl6FGRbBU+6EANLCL6ATTAHFjge7d+BlfamVrz5suR12zZ2wpYAQs8xeW3pj+PKT565Uf0yesQiENNSgQrEHz1HLOTf7qIBbsoNgTSPS85lrf6nX4DzC9lAgAAwE/6a6j6UyAwJ8g9gEaJLvEeEg1ak7ktAwbAQGP7BFbh0tGzE2zriKQEnyjolE7FblA9wK0W/fxRcmRE9Hu/L8i73Q6e/EDF7HekMXiA7738M9WRPStHRqpC0xxVJ419x1p1muMpLj+tLm9HO00vIdRsmB6u4qU1ac0GOEpyDJ0kMxMqeGwTjM2azCelQGulA7tmgejcIXJdWT8iazYx7Sj3JJX6rv8KtcAxSVueeoCArW0nR+3/4WsoXrDuDdUjkaIkZDaf/tPlYJUTNAQWFNEvh+dYx7YwfPxVXOMiojB0vgCOcfzpa+GgzgsnXQso1zBA1bC/9jIR1ZvF7oIPEVwxY8uR1QGvB61aWg3+cPRT8aDQgV2zQABujn4HwP3Nfw+nCWlKzYjggCyzyoBPcG7xF78DfOnoWXILjnBSSd0Q8OK+JeNnzUJPQ/Qk/BwzvM3Qv2KwmG7Rn4e+XX4UdWABzIGlQu0CT9+7vDyacCZNty4pOULrBOp29K0O6Eijl9TtCUguHT2jXG2hlQniBQn6inj2RTh2KDVgolnQTjFAkNuj//TG5msAJljypLuOjEnygAtSpy9B05UHAMm8U3rfdbqTJg0K0d/XfYUmJ+8yO82ZOhGT2dYddYrUwCXwNnDMFAwp31hi8dKUWFC4dbZDrmITsOhCnb5D+J3g6K/bRO0EyxA/lJBM0/DrQj2bqW2lEPbEvQAAfWMA2dGro9+4u/k6gCkWC0x5CNtpR+Gnj/S8Y9g2LChDlSA5+TvIJR/jdvqBJSY2aELhI+AYmOLk2sWT2UUi1s1RYq35udrwZhmtfvibkj0xFLy0HY+XR4aKi7bjrGM/unNPeGI3qkFAf5iyNK01LNeXD4LNJp2SNqGpXzAAt7u9FMBQo9dGnwfwJ5vvUl5s55DAS7dxHRDBKyc5OhCkO1upBQLeSftOYiCfoqVodQE8Bmpg0lrKlrxNcVJdRBPtMbVaRnsvgKFHVwF1fx3mDJ7cAu/cjFpFRgvMA0SOF5INnMyIs+e5vikZamRKxdaMMdER8BgHesMavzn6xd/fPKTQx0ySosCactGBAe2K6XE4RsJcMumdLjO8jsoNs52cojTbqtsNI1tTIN64qvbfyXsX8xQr0VIam1YDGqd5hzzyE9Gveb8Jd53YKUl6er2x0bVIoAiEVRbmf01/D6MAAH5ndPN3N39fse+9CFeAbPjrScZ4wwmhMrIb29GyxVPDwFQr0JuZFrDkppZyHQxnjNNcXl7YPjvvxIO7Fu0BKgCww7Pt2XqY/EzSEw4Ygs2t2JLhcNL/OPpX6qM4WvYIgrX819G/BPClzT/zmN0j+r3M7Q7MZIezYAcYCZYSzqZ5hQCAHlRlVYNjdKuYWp04/ZeL2TBzsyu5TdMancFeIYGhrsrVhi8QbFD9zuhmYcMdJ8Ly8sejn/zC5sdg+qoD4vJJqDyCzVwb7fZAM2vcr9qKbXPQe9xaWF8hlbPLI84qxhLTaP53Rn9fUegnKUn5i0McA97a3k4d4IvkEU0wgHT1vymtc+j9SmS5/LvNTzRs5hx9Vg1lpqKEHzTdor32uFDyrcSTalhhEx1Cw8obVo03984iLkVAHyn6+7k3S2prJisvvhpoMZd7lM6BAPA/vxAWaxD6/2yUW/LJtiG04vAegOSvRz/+5c32K5BixUQnOTl576LI2YW7SbnC8OVrO6+Hve2iN0tqVuOHzMy68AarAuyKCaYqlkwi//GGBNV55laEseW5ibiiJt3OXoh+OpeOnvXlpUJEkseHfpA1w73hEE7RAwT5hc2/QFzqLL7vkFCUvng1g0heUko4LRE957tq1AY3841SkpIrrdINzf41hPJWnIZUp9slkodrYcBQVkF4DDGxE3DPGf/fjP6ZVR+rlL8w/wedCS6U8EL+z26m5AqSFApJXwUox2h5z6ts3qZkMCtVuJuxE+xfd9KomoUKv3K2Pamz157xHvmGjkiPEXO7Osnz1uhJ2tRY5+N85Y5Lz4/j7iOf3NxGdAUn7yod2Mdgo+tel3fu9bNDYGA2LB8JZ/9QKoG08PTEO8U+ht/rxMoBhBwrT+3/3egHmbRm30/0nboHIPnB6P7HNr8YLqOd/mwsk5kR7ceJUnuW2xTzROVn9/rUrGyHOCSPCSgT3it+3UE8P5Zh//zR7CeXriTzaz8Y3QfAv46sWogSTYHGghqdnQcg+ejm1ZYILdM0IvzQ02T8iIdE5bx4dUr0pxDQpjIUnqVTAbScth/onBXOS7kP4V6uatEPoELzZPRdi9n3tvdazs4DkDwdvXFp8xrAGKTnB0oCR26KBDPeAeudEC/XAX3SHYxiX5XeOeQwTxR625OdemfUy5zCGKZJlP909MZTwKL4JdCV9l7LB+ABtrK807KghY/1wrvvYctrXhjUlnd4qtK5TC1Tv1z2N+qmZCy9KKkl+lG97ud2NKzzDsGu/4EqAIB37gDxhddlLMzElKZknsfOFr0X0Hfg+iWSvy7t9xBt+W5+4CBZ0ZLK5pqimv1F+FFrdJnwvjpgyAetAACO7wAxIF76c6UlMUBJfS27eQlPdKs8YjLc42xk56TNbp3zEprTIPQD+PjrrIYAtJ3LF795YUUIrqo8BwoQ5NGdNiDO0yGd7alVnR2ErKbWukz87ckZJGcOIoXQ31kbMwF92J4y3FfAtW92wd3EdPm0lyHPjQIAuBddwYItp8FOM1P7SK+eP5CQYH85DbaTbytoD+f6NXDjdbiSB7qu5m3brZ4nBQDwZhkdMkv2kT3XTVTWNroGuYNv2VMKAb2P++rFeQC88k0rrvVsP2V+hEogbVWuAM+VByD5dqRDTTpRwOFycNN7Bi6lUrsm9Tq4YnTins7Y1+TnVUVwHo7+IJ8Vhl8Y+yCesd9tNuA5mAgrkm9EVyCigl4zx33ljO2xls7ggc+FixIuJSZceKq+0O+sz5dycPQH+ZyGfpASKJuG38wXdSdMn1cFAPDnjA417G3a06ZDu8mLEgwcJDr30K95vyA8AH79myl24fAZKNoDK9JFBtyWS5FO4zlWgCB/egdgIUEmQfQ8iBcDmCmms5TD5lU757Z0kgfAF1+3JrMQS0gyMa5QGKEJ4pCpHvIUz70CAPgjhw7hECl8ExlEiE8Jss9n7LtbczPXOU0Xsf5WJsmDLliXhLNanV7ENGhe/uDOlg41ZcFAZxJmBzkIdmlCQ09rBHlOphHyt8hMdE7Sdxj+8+sOJ9G7mq9nyH2hdE8hvzgKAOD3Ih3SrgCnkDnxOiyhWx6ynwfJB9CijhYv0Vmj/cFBavt7ZPg7M/dIDTwsP5D3AOjyD+YZXywFAPAVRofIG3DZGXaF6Ze+IpKepyEakfpEB4kBPOqvkzz/TdN9TzImHxqvXbNdutuOQy+aAgT5cjpnjOJ32IX0hcVpJF7z8fFhg9fdxBwD8RyR5Plqnu4HMSFeridmiQgAaDvfz/M2E1wuX7ZCAhNGZuH+wOIJn+chD3tY6Ux0ilmtKkC/kKnnoe+Z9sIcPyzn43KwF1YBAHzRDwl2Q6R46nvC+vRiAI+t7XDGwmjbm9MNPfwpX7pMksnrZ/I8UIjvGwSXtHpBg2BTfvXOviFBecplB4QVatHBqY44b5VGI4XC7wzlN8HQ/5ce4elFyr0egpQ4B69+9+lefAUA8O/TOePGQeoBudBpTxSUyGmcPUP3J3Gb6vyVl+TxEpqm6ASO2dDTB6TlntvRXbXbHwoFCPLZlBEF6YRIcyDrK/o5FDQ1K8uM9iDETxwSbCfIt8U7KyiIUDshruWAcwJumPEhUgAAn3WWkQbZHx879BB8EZ8WKGdcO4t2gOUnrdgG2Xsq/F/a6psKwMVErU7aeGQdqRpkdsUZM4PZyodLAQD868NNFHSC5rmd6uolZna/Sid0Kw59Lpk5pkwMEMSdnHKWRXSS+3LcbwfzoVOAIJ9RUQEsvK7flyXjCwc4O+92fAHr99u/GaHzepVDhXwnJZK/QJ3cBPC/S1L7yTkAdOmD19DLaSL1FX0jaShdag99SBUAwI077cZK/bro/jB6XkTo9K7sKmiFpvuPPKuPXQGdb+tlSL1gQO/muzX04cOrAIg6wKMCA/qFPKbu87rKcy7qKsYXjI9Ov+29rmXuetIJTXTpgxdC6NyOFwx4GVLgQ64AQT6RRgWtDmiwntKM8WFlhzA8cwk1wNBPdd/+ZplB3UE6DfZu0wU769h5UAAAH490qAGW7/f/8taHSdhiBs58gryjOU9hylK3QlksexrSOS+xLTwfCoCoA1snoLOk2E8TdvYV/KTmYow9Z7z0wKoE/UGWO0S6XuKyM02Z73DnVoUMLTnRuVGAIJMYFUgdyCAsw+zPIKUvhA+m79mJ5dTbXFC9A/S15FGYoT19E52FZ9fUX6ePzpsHIBlHOrTVAY7+/QPZQlyeTcTMB8MSnG1SteRzVJ2Furwvz8lD30z4lAwvHxavz6UHIBnfYdHwMm5oKeEe+wfKhesxvMHk29aJ7d/KNwrO2Dck5a10vjJIuYvQgIaqn7H6+Stqd8+rAgDA5wD0fMv4RREW6Sb5naAABP3TDkkLPUNnTrOT0HthLrpU61wrAIDPsVdpzPVDiEdLpK8r6Nttr0StuZTn26zCGaO/5FAnyfF64Oa/s20i51wBgnwWgKMDL5wr4HO5fF3bt90Wu0gnvd6/t/KGvVQFIss0xiD4NvAKgNTWauiXhK06Aj5ssJuPsE30v9nzFCVRpgm4zKH8uXTa1EvaIB1Y5/TCWO2uRc3BA5C8AoCtmzt4duj0JGiFXrZ/76wmnoT0SuT37Tlnzlmd/Bi26jR4AJI3AQAvx90ApmaP7NABpWs5g41+9OQGJYc6RWRjOqsV5jSDmCGydiC8Jvx+gBf7pfjTkqADFAz0TRAV5jT3FzPMBfDoEJ33JdZmHfScyepsuAPd74iqBw+g5R5wA4BagyCgn9GEPf1Dif6Y6D8I9IPkgZtHIVIznBFOyqlhuV/S82Wmc6C/Ru5oUABTApJIDVC2fk7UyQesGclPeOkwtwGO/d768pnybL1ZWUeohdJp8j18820vYnZxPihARh4B1wDsmh3qGzeXKIwg+ojQP2Cw6/VjIntn1TISMlZNrV2dfgCWosJT2kEB8hLgFdSAYHdK2SGvKy/M5VZ/h3C2XGcOol0Z471Pw7wLymvLGoMClMkxMAOgdEBDNs/+ObkvjJVrxXyCzLsa5k0s+kBwz4zQDp10W27nqN4VIbXsYVCAQplHHYDzPduSwLewibeSB2zpXqccihEJKfQGh50KMNGcH4lAvBvYDApQLsHoTgCkiETZT3T0yo0S7eENl316OCUpxPQOScyS3jJcKM/0vDTokAXqLcuoA3ASRNjvK9VVin44kfdh5ZQmxYTkk6emqS6fI6PyPO+X8pHMeAdxZJkislbkvlw4uKsPCP04UKiw5wAEa/JIlMhscgsuNEcUmkcHCrSjBFBWcbKsjoUarF9m21/1O9T5zZ1xXzhNe9rI7meJe86agekMPzpOt/OTA8CwFOJwQsEAB+6X7bpAVAYzw/MhlvwkWmdDjymJmlBcyPUngwc4lNRRB4JPyEAfHyL0Fy7OMSe2TE0oIf0ZZfBmjj21WQ8xwGGFuHuG7RD6K5VNeq6kxDjmTbhm9mb/+XkupJiGpUtUWaeGeKhgdD4owMGFYG3qwFfVJwgB1M+lB9gtSBBqs2d4LTiMPhF5A+EHxmnh2upkjeGFmFMQPjPAuRCnPTgr5nMGwW5f6ZvE1Ec7a6IspMZAgU5D6jQO/ir7W6XVTg/9Jrvw6pyxdLImLuJCMhSI1yxEPzBQoNORadzgXKiyKpyS7GP1T08xxtndTBZ/rMp1z54+mNxpTXoyZIFOSQLEF3G3SsufB/GUZLc0ZUkTHaEW1jQ7FwkfXg51SPOitvngAU5VCO4NMH2e0F8iO7iRwiZ5sztWf720j9lwzcJic2BJ/DB4gNOW2Qc9gMPKPlE1B2im53yuk0QfWqtTeF5iuz14gEF6yQ6kiLftzNuI1Q3jLO83h+fNKtj9DAowyD6Sj2vhx6a0O06hr+MEPRVgxrUZPXHXQWBQgEEAlDFhz+Jmdkvqa4oytnyCt8bBpFU6v8SVJ0kxDQowCHpOyu4gJVEvFJpFBCzC4gxx0oVuLmhQgEHOQDzlGSvTjhToXldaZ3TPa8sViObDTPAgpyUltEojMpPdX1u6oXXGTCK5k26DAgxySmLiFcp4w0mtmtEF/RXBsc6ZjpU+jBVTAgYFGOTQIsyzRr/JTOCQmXwO1OM/cMJiHRsME2GDHFi81Qr8qAdZMA3J1NR5IVN4tsebHBg8wCCHlEz+kVcwV+aAFXrcXWPaYDVpHY+JtacYFGAQkoPTAc8D0NGxVZ4RcwbA7FyPYZgIG6RD9sn0owvuvBodXSsT3pnf9PoUbQVNMj3JMBE2yCEloz/miqCx0hlSCQ3ckskBnj/VJbYMCvD8y4ubqDAZjhebwioXM775VUMlg5FtBwV4/mVPZlIip8H+kc32FJ6XUGtOGuijmX7s2zgowCA4tI6ZcBxbNKYzj7lWtl/Pf8Ga5zJHYsxCDAowyKGEx53mcjSxrA2WSvBdjwiZRz1CJerIeehBAQY5rJTnJXWFzGSwncRUHfKjprtYM2UABgUY5HBSSO5N56BF+4GxKh9brIYUJqOK2zTUoACDHEpKyL2Z9RdYJwSbWiT0Qc+smfGAZD6x7bAcepAekk+25A25rqMnwvIdllB8vZIik48CBgo0SB8pseuFTdYOV0HqB/I9m7O8sBi/OLotGRRgkP2lcw44T2bMJnrZj1c5CWqtnrXb2XY7KMAgpyqak3DhYYCOBNapMvBWZv86wDBbJWH0oACDnJmYE2E6x6/XPoBB3JjMYnXEXIQ3b7AtHBTgQyZnuXAofy5zFnadwjSIYOomvr2AQaSSNB2C5Rm25Wd5vwY5AzmDhUN9z6URKaw7FQodWKuj5hnNxRFIdUbPKrTbgwcY5DREL9TxpsmE/Rb1eYTghbnCOWRojwgVhiB4kNOSTB7GW6HAawrlQWrFza7MMYyt3eRcgwIMcljpJNXaQutEEBRpMbHOPUbJkIyk6qAA50fOJt7rDAz0OgWknEfP3WbmjPMTvZoUSR0bguDzI2cZH8Ox2YLce8zH7EHnSTMUqDNixkCBBjlVyeNvrdDvVRY5Ux1ej61qnbyolUEBBjk9kUl3tW1yIcGR1hbodWYzkwLKyaAAg5ye8FU3OqmPeEhnQnU/UOpk8ns92dwhQwwwyAElz7zHFu0xcvPZOAHqFF4utUgGDzDIASWDPHMND8/Nr9MNOLoBtWsmeUot+6AAg5yqCOpv6kC+vp7AyjQ0NTCnDIMCDHJKImZwxVIFLz5eq7Zm9Jw5I+8HXbPFgwIMcuriRQUiMDDBbZJ7zYXGKXcyz24zqEEBBjkl8dhICQUiv+Et8BQ9lES95qzcoACDlMpBEoY6rh0zE27m+/kAbBDvNMj1MBM8SC/ZfyWFub7NnBIeF8C9V8LHrTMowCBnLMLGi0LaNdd+mqGtOcWms0kw1+ENCjCIllOaHjUxbZ7dXCLBNzJzXpk1EYZeDQowiJYDsh2vML+2OaMq5gq58vEMHmCQU5fCpcgalCJlmcF3R3LTH8/gAQY5sGgIdjqQcR6UFo3xTiQ0oXAqYKtggwIMsqfkVyiQiGlaPaWlF0JnZgz09FmvqYCtgg0KMMjZiLluWVQQiqGnwPTs2L7hyqAAgxTKzqkhD+uF3XJl2D86lzIowCAlUsLsd2toznaZScxMie5hWA49yL7CQ8aDmF7N77253sw8QL5bxG5FttRt/v8BI1L/jlJ+SIsAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O1EHE3-Aeyjo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}