{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R12942159/NTU_DLCV/blob/Hw4/p1_NeRF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQosyvi9_SUi"
      },
      "outputs": [],
      "source": [
        "!gdown 1hF4z9U-xaoV4qaq9DbhTP-KKJTlwOUv_ -O hw4_data.zip\n",
        "!unzip hw4_data.zip\n",
        "!rm hw4_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kornia"
      ],
      "metadata": {
        "id": "W92bIA_qcnUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dataset.py"
      ],
      "metadata": {
        "id": "DBd6qycW8Hic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "import random\n",
        "from kornia import create_meshgrid\n",
        "from scipy.spatial import transform\n",
        "\n",
        "\n",
        "def calculate_near_and_far(rays_o, rays_d, bbox_min=[-1.,-1.,-1.], bbox_max=[1.,1.,1.]):\n",
        "    '''\n",
        "    rays_o, (len(self.split_ids)*h*w, 3)\n",
        "    rays_d, (len(self.split_ids)*h*w, 3)\n",
        "    bbox_min=[-1,-1,-1],\n",
        "    bbox_max=[1,1,1]\n",
        "    '''\n",
        "    # map all shape to same (len(self.split_ids)*h*w, 3, 2)\n",
        "    corners = torch.stack((torch.tensor(bbox_min),torch.tensor(bbox_max)), dim=-1)\n",
        "    corners = corners.unsqueeze(0).repeat(rays_o.shape[0],1,1) # (len(self.split_ids)*h*w, 3, 2)\n",
        "    corners -= torch.unsqueeze(rays_o, -1).repeat(1,1,2)\n",
        "    intersections = (corners / (torch.unsqueeze(rays_d, -1).repeat(1,1,2)))\n",
        "\n",
        "    min_intersections = torch.amax(torch.amin(intersections, dim=-1), dim=-1, keepdim=True)\n",
        "    max_intersections = torch.amin(torch.amax(intersections, dim=-1), dim=-1, keepdim=True)\n",
        "    epsilon = 1e-10*torch.ones_like(min_intersections)\n",
        "    near = torch.maximum(epsilon, min_intersections)\n",
        "    # tmp = near\n",
        "    near = torch.where((near > max_intersections), epsilon, near)\n",
        "    far = torch.where(near < max_intersections, max_intersections, near+epsilon)\n",
        "\n",
        "    return near, far\n",
        "\n",
        "def get_ray_directions(H, W, focal):\n",
        "    \"\"\"\n",
        "    Get ray directions for all pixels in camera coordinate.\n",
        "    Reference: https://www.scratchapixel.com/lessons/3d-basic-rendering/\n",
        "               ray-tracing-generating-camera-rays/standard-coordinate-systems\n",
        "\n",
        "    Inputs:\n",
        "        H, W, focal: image height, width and focal length\n",
        "\n",
        "    Outputs:\n",
        "        directions: (H, W, 3), the direction of the rays in camera coordinate\n",
        "    \"\"\"\n",
        "    grid = create_meshgrid(H, W, normalized_coordinates=False)[0]\n",
        "    i, j = grid.unbind(-1)\n",
        "    # the direction here is without +0.5 pixel centering as calibration is not so accurate\n",
        "    # see https://github.com/bmild/nerf/issues/24\n",
        "    directions = \\\n",
        "        torch.stack([(i-W/2)/focal, -(j-H/2)/focal, -torch.ones_like(i)], -1) # (H, W, 3)\n",
        "\n",
        "    return directions\n",
        "\n",
        "def get_rays(directions, c2w):\n",
        "    \"\"\"\n",
        "    Get ray origin and normalized directions in world coordinate for all pixels in one image.\n",
        "    Reference: https://www.scratchapixel.com/lessons/3d-basic-rendering/\n",
        "               ray-tracing-generating-camera-rays/standard-coordinate-systems\n",
        "\n",
        "    Inputs:\n",
        "        directions: (H, W, 3) precomputed ray directions in camera coordinate\n",
        "        c2w: (3, 4) transformation matrix from camera coordinate to world coordinate\n",
        "\n",
        "    Outputs:\n",
        "        rays_o: (H*W, 3), the origin of the rays in world coordinate\n",
        "        rays_d: (H*W, 3), the normalized direction of the rays in world coordinate\n",
        "    \"\"\"\n",
        "    # Rotate ray directions from camera coordinate to the world coordinate\n",
        "    rays_d = directions @ c2w[:, :3].T # (H, W, 3)\n",
        "    rays_d = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
        "    # The origin of all rays is the camera origin in world coordinate\n",
        "    rays_o = c2w[:, 3].expand(rays_d.shape) # (H, W, 3)\n",
        "\n",
        "    rays_d = rays_d.view(-1, 3)\n",
        "    rays_o = rays_o.view(-1, 3)\n",
        "\n",
        "    return rays_o, rays_d\n",
        "\n",
        "def blender_quat2rot(quaternion):\n",
        "    \"\"\"Convert quaternion to rotation matrix.\n",
        "    Equivalent to, but support batched case:\n",
        "    ```python\n",
        "    rot3x3 = mathutils.Quaternion(quaternion).to_matrix()\n",
        "    ```\n",
        "    Args:\n",
        "    quaternion:\n",
        "    Returns:\n",
        "    rotation matrix\n",
        "    \"\"\"\n",
        "\n",
        "    # Note: Blender first cast to double values for numerical precision while\n",
        "    # we're using float32.\n",
        "    q = np.sqrt(2) * quaternion\n",
        "\n",
        "    q0 = q[..., 0]\n",
        "    q1 = q[..., 1]\n",
        "    q2 = q[..., 2]\n",
        "    q3 = q[..., 3]\n",
        "\n",
        "    qda = q0 * q1\n",
        "    qdb = q0 * q2\n",
        "    qdc = q0 * q3\n",
        "    qaa = q1 * q1\n",
        "    qab = q1 * q2\n",
        "    qac = q1 * q3\n",
        "    qbb = q2 * q2\n",
        "    qbc = q2 * q3\n",
        "    qcc = q3 * q3\n",
        "\n",
        "    # Note: idx are inverted as blender and numpy convensions do not\n",
        "    # match (x, y) -> (y, x)\n",
        "    rotation = np.empty((*quaternion.shape[:-1], 3, 3), dtype=np.float32)\n",
        "    rotation[..., 0, 0] = 1.0 - qbb - qcc\n",
        "    rotation[..., 1, 0] = qdc + qab\n",
        "    rotation[..., 2, 0] = -qdb + qac\n",
        "\n",
        "    rotation[..., 0, 1] = -qdc + qab\n",
        "    rotation[..., 1, 1] = 1.0 - qaa - qcc\n",
        "    rotation[..., 2, 1] = qda + qbc\n",
        "\n",
        "    rotation[..., 0, 2] = qdb + qac\n",
        "    rotation[..., 1, 2] = -qda + qbc\n",
        "    rotation[..., 2, 2] = 1.0 - qaa - qbb\n",
        "    return rotation\n",
        "\n",
        "def make_transform_matrix(positions,rotations,):\n",
        "    \"\"\"Create the 4x4 transformation matrix.\n",
        "    Note: This function uses numpy.\n",
        "    Args:\n",
        "    positions: Translation applied after the rotation.\n",
        "        Last column of the transformation matrix\n",
        "    rotations: Rotation. Top-left 3x3 matrix of the transformation matrix.\n",
        "    Returns:\n",
        "    transformation_matrix:\n",
        "    \"\"\"\n",
        "    # Create the 4x4 transformation matrix\n",
        "    rot_pos = np.broadcast_to(np.eye(4), (*positions.shape[:-1], 4, 4)).copy()\n",
        "    rot_pos[..., :3, :3] = rotations\n",
        "    rot_pos[..., :3, 3] = positions\n",
        "    return rot_pos\n",
        "\n",
        "def from_position_and_quaternion(positions, quaternions, use_unreal_axes):\n",
        "    if use_unreal_axes:\n",
        "        rotations = transform.Rotation.from_quat(quaternions).as_matrix()\n",
        "    else:\n",
        "        # Rotation matrix that rotates from world to object coordinates.\n",
        "        # Warning: Rotations should be given in blender convensions as\n",
        "        # scipy.transform uses different convensions.\n",
        "        rotations = blender_quat2rot(quaternions)\n",
        "    px2world_transform = make_transform_matrix(positions=positions,rotations=rotations)\n",
        "    return px2world_transform\n",
        "\n",
        "def scale_rays(all_rays_o, all_rays_d, scene_boundaries, img_wh):\n",
        "    \"\"\"Rescale scene boundaries.\n",
        "    rays_o: (len(image_paths)*h*w, 3)\n",
        "    rays_d: (len(image_paths)*h*w, 3)\n",
        "    scene_boundaries: np.array(2 ,3), [min, max]\n",
        "    img_wh: (2)\n",
        "    \"\"\"\n",
        "    # Rescale (x, y, z) from [min, max] -> [-1, 1]\n",
        "    all_rays_o = all_rays_o.reshape(-1, img_wh[0], img_wh[1], 3) # (len(image_paths), h, w, 3))\n",
        "    all_rays_d = all_rays_d.reshape(-1, img_wh[0], img_wh[1], 3)\n",
        "\n",
        "    old_min = torch.from_numpy(scene_boundaries[0])\n",
        "    old_max = torch.from_numpy(scene_boundaries[1])\n",
        "    new_min = torch.tensor([-1,-1,-1])\n",
        "    new_max = torch.tensor([1,1,1])\n",
        "    # scale = max(scene_boundaries[1] - scene_boundaries[0])/2\n",
        "    # all_rays_o = (all_rays_o - torch.mean(all_rays_o, dim=-1, keepdim=True)) / scale\n",
        "    # This is from jax3d.interp, kind of weird but true\n",
        "    all_rays_o = ((new_min - new_max) / (old_min - old_max))*all_rays_o + (old_min * new_max - new_min * old_max) / (old_min - old_max)\n",
        "\n",
        "    # We also need to rescale the camera direction by bbox.size.\n",
        "    # The direction can be though of a ray from a point in space (the camera\n",
        "    # origin) to another point in space (say the red light on the lego\n",
        "    # bulldozer). When we scale the scene in a certain way, this direction\n",
        "    # also needs to be scaled in the same way.\n",
        "    all_rays_d = all_rays_d * 2 / (scene_boundaries[1] - scene_boundaries[0])\n",
        "    # (re)-normalize the rays\n",
        "    all_rays_d = all_rays_d / torch.linalg.norm(all_rays_d, dim=-1, keepdims=True)\n",
        "    return all_rays_o.reshape(-1, 3), all_rays_d.reshape(-1, 3)\n",
        "\n",
        "\n",
        "# Nesf Klevr\n",
        "\n",
        "class KlevrDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', get_rgb=True) -> None:\n",
        "        # super().__init__()\n",
        "        '''\n",
        "        split: train/val/test\n",
        "        '''\n",
        "        self.root_dir = root_dir\n",
        "        self.get_rgb = get_rgb\n",
        "        self.split = split\n",
        "\n",
        "        self.define_transforms()\n",
        "        self.read_meta()\n",
        "\n",
        "    def define_transforms(self):\n",
        "        self.transform = T.ToTensor()\n",
        "\n",
        "    def read_meta(self):\n",
        "        with open(os.path.join(self.root_dir, \"metadata.json\"), \"r\") as f:\n",
        "            self.meta = json.load(f)\n",
        "\n",
        "        w, h = self.meta['metadata']['width'], self.meta['metadata']['width']\n",
        "        self.img_wh = (w, h)\n",
        "        self.focal = (self.meta['camera']['focal_length']*w/self.meta['camera']['sensor_width'])\n",
        "        if self.split not in ['train', 'val', 'test']:\n",
        "            raise ValueError(f\"split should be train/val/test, got {self.split}\")\n",
        "        self.split_ids = self.meta['split_ids'][self.split]\n",
        "\n",
        "        self.scene_boundaries = np.array([self.meta['scene_boundaries']['min'], self.meta['scene_boundaries']['max']])\n",
        "        self.directions = get_ray_directions(h, w, self.focal)\n",
        "\n",
        "        if self.split == 'train':\n",
        "            self.poses = []\n",
        "            self.all_rays_o = []\n",
        "            self.all_rays_d = []\n",
        "            self.all_rays = []\n",
        "            self.all_rgbs = []\n",
        "            camera_positions = np.array(self.meta['camera']['positions'])\n",
        "            camera_quaternions = np.array(self.meta['camera']['quaternions'])\n",
        "            for image_id in self.split_ids:\n",
        "                if self.get_rgb:\n",
        "                    image_path = os.path.join(self.root_dir, f'{image_id:05d}.png')\n",
        "                    img = Image.open(image_path)\n",
        "                    img = img.resize(self.img_wh, Image.Resampling.LANCZOS)\n",
        "                    img = self.transform(img) # (4, h, w)\n",
        "                    img = img.view(4, -1).permute(1,0) # (h*w, 4)\n",
        "                    # not sure, original jax implementation seems not using blend just cut it off, they also /255 to make it [0,1] which I didn't use\n",
        "                    # img = img[:, :3]*img[:, -1:]+(1-img[:,-1:]) # blend A to RGB\n",
        "                    img = img[:, :3]\n",
        "                    self.all_rgbs += [img]\n",
        "\n",
        "                pose = np.array(from_position_and_quaternion(camera_positions[image_id:image_id+1,:], camera_quaternions[image_id:image_id+1,:], False))[0,:3,:4]\n",
        "                self.poses += [pose]\n",
        "                c2w = torch.FloatTensor(pose)\n",
        "                rays_o, rays_d = get_rays(self.directions, c2w)\n",
        "                self.all_rays_o += [rays_o]\n",
        "                self.all_rays_d += [rays_d]\n",
        "\n",
        "            self.all_rays_o = torch.cat(self.all_rays_o, 0) # (len(self.split_ids)*h*w, 3)\n",
        "            self.all_rays_d = torch.cat(self.all_rays_d, 0) # (len(self.split_ids)*h*w, 3)\n",
        "            self.all_rays_o, self.all_rays_d = scale_rays(self.all_rays_o, self.all_rays_d, self.scene_boundaries, self.img_wh)\n",
        "\n",
        "\n",
        "            self.near, self.far = calculate_near_and_far(self.all_rays_o, self.all_rays_d)\n",
        "            self.all_rays = torch.cat([self.all_rays_o, self.all_rays_d, self.near, self.far],1).float()\n",
        "\n",
        "            if len(self.all_rgbs) > 0:\n",
        "                self.all_rgbs = torch.cat(self.all_rgbs, 0)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.split == 'train':\n",
        "            return len(self.all_rays)\n",
        "        elif self.split == 'val' or self.split == 'test':\n",
        "            return len(self.split_ids)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.split == 'train':\n",
        "            sample = {\n",
        "                'rays': self.all_rays[idx],\n",
        "            }\n",
        "            if self.get_rgb:\n",
        "                sample['rgbs'] = self.all_rgbs[idx]\n",
        "\n",
        "\n",
        "        # split of val/test\n",
        "        elif self.split == 'val':\n",
        "            image_id = self.split_ids[idx]\n",
        "\n",
        "            if self.get_rgb:\n",
        "                img = Image.open(os.path.join(self.root_dir, f'{image_id:05d}.png'))\n",
        "                img = img.resize(self.img_wh, Image.Resampling.LANCZOS)\n",
        "                img = self.transform(img)\n",
        "                valid_mask = (img[-1]>0).flatten()\n",
        "                img = img.view(4,-1).permute(1,0)\n",
        "                # img = img[:,:3]*img[:,-1:]+(1-img[:,-1:])\n",
        "                img = img[:, :3]\n",
        "\n",
        "            camera_position = np.array(self.meta['camera']['positions'][image_id:image_id+1])\n",
        "            camera_quaternion = np.array(self.meta['camera']['quaternions'][image_id:image_id+1])\n",
        "\n",
        "            pose = np.array(from_position_and_quaternion(camera_position, camera_quaternion, False))[0,:3,:4]\n",
        "            # pose = np.array(from_position_and_quaternion(camera_position[idx:1+idx,:], camera_quaternion[idx:1+idx,:], False))[0,:3,:4]\n",
        "            c2w = torch.FloatTensor(pose)[:3,:4]\n",
        "            rays_o, rays_d = get_rays(self.directions, c2w)\n",
        "            rays_o, rays_d = scale_rays(rays_o, rays_d, self.scene_boundaries, self.img_wh)\n",
        "\n",
        "            self.near, self.far = calculate_near_and_far(rays_o, rays_d)\n",
        "            rays = torch.cat([rays_o, rays_d,\n",
        "                              self.near,\n",
        "                              self.far],\n",
        "                              1) # (H*W, 8)\n",
        "\n",
        "            sample = {'rays': rays.float(),\n",
        "                      'c2w': c2w}\n",
        "            if self.get_rgb:\n",
        "                sample['rgbs'] = img\n",
        "                sample['valid_mask'] = valid_mask\n",
        "\n",
        "        elif self.split == 'test':\n",
        "            image_id = self.split_ids[idx]\n",
        "\n",
        "            camera_position = np.array(self.meta['camera']['positions'][image_id:image_id+1])\n",
        "            camera_quaternion = np.array(self.meta['camera']['quaternions'][image_id:image_id+1])\n",
        "\n",
        "            pose = np.array(from_position_and_quaternion(camera_position, camera_quaternion, False))[0,:3,:4]\n",
        "            # pose = np.array(from_position_and_quaternion(camera_position[idx:1+idx,:], camera_quaternion[idx:1+idx,:], False))[0,:3,:4]\n",
        "            c2w = torch.FloatTensor(pose)[:3,:4]\n",
        "            rays_o, rays_d = get_rays(self.directions, c2w)\n",
        "            rays_o, rays_d = scale_rays(rays_o, rays_d, self.scene_boundaries, self.img_wh)\n",
        "\n",
        "            self.near, self.far = calculate_near_and_far(rays_o, rays_d)\n",
        "            rays = torch.cat([rays_o, rays_d,\n",
        "                              self.near,\n",
        "                              self.far],\n",
        "                              1) # (H*W, 8)\n",
        "\n",
        "            sample = {'rays': rays.float(),\n",
        "                      'c2w': c2w}\n",
        "\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "sAjZ7kTb8JpV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dataloader"
      ],
      "metadata": {
        "id": "j9z7L1DD8Q91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = KlevrDataset(root_dir='/content/dataset', split='train', get_rgb=True)"
      ],
      "metadata": {
        "id": "Q1KYAdCu8Tcp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "dataloader = DataLoader(\n",
        "    ds,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LViJO_28kKa",
        "outputId": "25e00d99-11ee-4760-fa26-ff50b179b11d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### model"
      ],
      "metadata": {
        "id": "5LqFChf39zZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recursive https://github.com/kwea123/nerf_pl"
      ],
      "metadata": {
        "id": "WgBKUBi1-wdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "5bLCeLMF99ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd nerf_pl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bsr9OQDi-0zD",
        "outputId": "7e2b8db6-4593-47cb-9876-07f0ff8866b8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nerf_pl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import *\n",
        "from metrics import *\n",
        "from losses import loss_dict\n",
        "from models.nerf import Embedding, NeRF\n",
        "from models.rendering import render_rays"
      ],
      "metadata": {
        "id": "RPYrIp14-6T-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.loggers import TensorBoardLogger # TestTubeLogger"
      ],
      "metadata": {
        "id": "Zbj7WcC197-j"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from opt import get_opts"
      ],
      "metadata": {
        "id": "WWuHX9XXAAVK"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = get_opts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "zOfp_AFfDkHr",
        "outputId": "ef6f9c15-ae1f-47ed-894d-603a429dec9e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--root_dir ROOT_DIR] [--dataset_name {blender,llff}]\n",
            "                                [--img_wh IMG_WH [IMG_WH ...]] [--spheric_poses]\n",
            "                                [--N_samples N_SAMPLES] [--N_importance N_IMPORTANCE] [--use_disp]\n",
            "                                [--perturb PERTURB] [--noise_std NOISE_STD] [--loss_type {mse}]\n",
            "                                [--batch_size BATCH_SIZE] [--chunk CHUNK]\n",
            "                                [--num_epochs NUM_EPOCHS] [--num_gpus NUM_GPUS]\n",
            "                                [--ckpt_path CKPT_PATH]\n",
            "                                [--prefixes_to_ignore PREFIXES_TO_IGNORE [PREFIXES_TO_IGNORE ...]]\n",
            "                                [--optimizer {sgd,adam,radam,ranger}] [--lr LR]\n",
            "                                [--momentum MOMENTUM] [--weight_decay WEIGHT_DECAY]\n",
            "                                [--lr_scheduler {steplr,cosine,poly}]\n",
            "                                [--warmup_multiplier WARMUP_MULTIPLIER]\n",
            "                                [--warmup_epochs WARMUP_EPOCHS]\n",
            "                                [--decay_step DECAY_STEP [DECAY_STEP ...]]\n",
            "                                [--decay_gamma DECAY_GAMMA] [--poly_exp POLY_EXP]\n",
            "                                [--exp_name EXP_NAME]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-3aad4c5b-f9ba-4dbd-b0ee-0926a8f63a45.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeRFSystem(LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(NeRFSystem, self).__init__()\n",
        "        self.hparams = hparams\n",
        "\n",
        "        self.loss = loss_dict[hparams.loss_type]()\n",
        "\n",
        "        self.embedding_xyz = Embedding(3, 10) # 10 is the default number\n",
        "        self.embedding_dir = Embedding(3, 4) # 4 is the default number\n",
        "        self.embeddings = [self.embedding_xyz, self.embedding_dir]\n",
        "\n",
        "        self.nerf_coarse = NeRF()\n",
        "        self.models = [self.nerf_coarse]\n",
        "        if hparams.N_importance > 0:\n",
        "            self.nerf_fine = NeRF()\n",
        "            self.models += [self.nerf_fine]\n",
        "\n",
        "    def decode_batch(self, batch):\n",
        "        rays = batch['rays'] # (B, 8)\n",
        "        rgbs = batch['rgbs'] # (B, 3)\n",
        "        return rays, rgbs\n",
        "\n",
        "    def forward(self, rays):\n",
        "        \"\"\"Do batched inference on rays using chunk.\"\"\"\n",
        "        B = rays.shape[0]\n",
        "        results = defaultdict(list)\n",
        "        for i in range(0, B, self.hparams.chunk):\n",
        "            rendered_ray_chunks = \\\n",
        "                render_rays(self.models,\n",
        "                            self.embeddings,\n",
        "                            rays[i:i+self.hparams.chunk],\n",
        "                            self.hparams.N_samples,\n",
        "                            self.hparams.use_disp,\n",
        "                            self.hparams.perturb,\n",
        "                            self.hparams.noise_std,\n",
        "                            self.hparams.N_importance,\n",
        "                            self.hparams.chunk, # chunk size is effective in val mode\n",
        "                            self.train_dataset.white_back)\n",
        "\n",
        "            for k, v in rendered_ray_chunks.items():\n",
        "                results[k] += [v]\n",
        "\n",
        "        for k, v in results.items():\n",
        "            results[k] = torch.cat(v, 0)\n",
        "        return results"
      ],
      "metadata": {
        "id": "nIolE0t590qC"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}