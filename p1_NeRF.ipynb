{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R12942159/NTU_DLCV/blob/Hw4/p1_NeRF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQosyvi9_SUi"
      },
      "outputs": [],
      "source": [
        "!gdown 1hF4z9U-xaoV4qaq9DbhTP-KKJTlwOUv_ -O hw4_data.zip\n",
        "!unzip hw4_data.zip\n",
        "!rm hw4_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kornia"
      ],
      "metadata": {
        "id": "W92bIA_qcnUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NeRF_pl"
      ],
      "metadata": {
        "id": "g1rArMiy0uM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recursive https://github.com/kwea123/nerf_pl"
      ],
      "metadata": {
        "id": "ljGMzAXH0wQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### blender.py"
      ],
      "metadata": {
        "id": "ofIMmxXMGFQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "import random\n",
        "from kornia import create_meshgrid\n",
        "from scipy.spatial import transform\n",
        "\n",
        "\n",
        "def calculate_near_and_far(rays_o, rays_d, bbox_min=[-1.,-1.,-1.], bbox_max=[1.,1.,1.]):\n",
        "    '''\n",
        "    rays_o, (len(self.split_ids)*h*w, 3)\n",
        "    rays_d, (len(self.split_ids)*h*w, 3)\n",
        "    bbox_min=[-1,-1,-1],\n",
        "    bbox_max=[1,1,1]\n",
        "    '''\n",
        "    # map all shape to same (len(self.split_ids)*h*w, 3, 2)\n",
        "    corners = torch.stack((torch.tensor(bbox_min),torch.tensor(bbox_max)), dim=-1)\n",
        "    corners = corners.unsqueeze(0).repeat(rays_o.shape[0],1,1) # (len(self.split_ids)*h*w, 3, 2)\n",
        "    corners -= torch.unsqueeze(rays_o, -1).repeat(1,1,2)\n",
        "    intersections = (corners / (torch.unsqueeze(rays_d, -1).repeat(1,1,2)))\n",
        "\n",
        "    min_intersections = torch.amax(torch.amin(intersections, dim=-1), dim=-1, keepdim=True)\n",
        "    max_intersections = torch.amin(torch.amax(intersections, dim=-1), dim=-1, keepdim=True)\n",
        "    epsilon = 1e-10*torch.ones_like(min_intersections)\n",
        "    near = torch.maximum(epsilon, min_intersections)\n",
        "    # tmp = near\n",
        "    near = torch.where((near > max_intersections), epsilon, near)\n",
        "    far = torch.where(near < max_intersections, max_intersections, near+epsilon)\n",
        "\n",
        "    return near, far\n",
        "\n",
        "def get_ray_directions(H, W, focal):\n",
        "    \"\"\"\n",
        "    Get ray directions for all pixels in camera coordinate.\n",
        "    Reference: https://www.scratchapixel.com/lessons/3d-basic-rendering/\n",
        "               ray-tracing-generating-camera-rays/standard-coordinate-systems\n",
        "\n",
        "    Inputs:\n",
        "        H, W, focal: image height, width and focal length\n",
        "\n",
        "    Outputs:\n",
        "        directions: (H, W, 3), the direction of the rays in camera coordinate\n",
        "    \"\"\"\n",
        "    grid = create_meshgrid(H, W, normalized_coordinates=False)[0]\n",
        "    i, j = grid.unbind(-1)\n",
        "    # the direction here is without +0.5 pixel centering as calibration is not so accurate\n",
        "    # see https://github.com/bmild/nerf/issues/24\n",
        "    directions = \\\n",
        "        torch.stack([(i-W/2)/focal, -(j-H/2)/focal, -torch.ones_like(i)], -1) # (H, W, 3)\n",
        "\n",
        "    return directions\n",
        "\n",
        "def get_rays(directions, c2w):\n",
        "    \"\"\"\n",
        "    Get ray origin and normalized directions in world coordinate for all pixels in one image.\n",
        "    Reference: https://www.scratchapixel.com/lessons/3d-basic-rendering/\n",
        "               ray-tracing-generating-camera-rays/standard-coordinate-systems\n",
        "\n",
        "    Inputs:\n",
        "        directions: (H, W, 3) precomputed ray directions in camera coordinate\n",
        "        c2w: (3, 4) transformation matrix from camera coordinate to world coordinate\n",
        "\n",
        "    Outputs:\n",
        "        rays_o: (H*W, 3), the origin of the rays in world coordinate\n",
        "        rays_d: (H*W, 3), the normalized direction of the rays in world coordinate\n",
        "    \"\"\"\n",
        "    # Rotate ray directions from camera coordinate to the world coordinate\n",
        "    rays_d = directions @ c2w[:, :3].T # (H, W, 3)\n",
        "    rays_d = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
        "    # The origin of all rays is the camera origin in world coordinate\n",
        "    rays_o = c2w[:, 3].expand(rays_d.shape) # (H, W, 3)\n",
        "\n",
        "    rays_d = rays_d.view(-1, 3)\n",
        "    rays_o = rays_o.view(-1, 3)\n",
        "\n",
        "    return rays_o, rays_d\n",
        "\n",
        "def blender_quat2rot(quaternion):\n",
        "    \"\"\"Convert quaternion to rotation matrix.\n",
        "    Equivalent to, but support batched case:\n",
        "    ```python\n",
        "    rot3x3 = mathutils.Quaternion(quaternion).to_matrix()\n",
        "    ```\n",
        "    Args:\n",
        "    quaternion:\n",
        "    Returns:\n",
        "    rotation matrix\n",
        "    \"\"\"\n",
        "\n",
        "    # Note: Blender first cast to double values for numerical precision while\n",
        "    # we're using float32.\n",
        "    q = np.sqrt(2) * quaternion\n",
        "\n",
        "    q0 = q[..., 0]\n",
        "    q1 = q[..., 1]\n",
        "    q2 = q[..., 2]\n",
        "    q3 = q[..., 3]\n",
        "\n",
        "    qda = q0 * q1\n",
        "    qdb = q0 * q2\n",
        "    qdc = q0 * q3\n",
        "    qaa = q1 * q1\n",
        "    qab = q1 * q2\n",
        "    qac = q1 * q3\n",
        "    qbb = q2 * q2\n",
        "    qbc = q2 * q3\n",
        "    qcc = q3 * q3\n",
        "\n",
        "    # Note: idx are inverted as blender and numpy convensions do not\n",
        "    # match (x, y) -> (y, x)\n",
        "    rotation = np.empty((*quaternion.shape[:-1], 3, 3), dtype=np.float32)\n",
        "    rotation[..., 0, 0] = 1.0 - qbb - qcc\n",
        "    rotation[..., 1, 0] = qdc + qab\n",
        "    rotation[..., 2, 0] = -qdb + qac\n",
        "\n",
        "    rotation[..., 0, 1] = -qdc + qab\n",
        "    rotation[..., 1, 1] = 1.0 - qaa - qcc\n",
        "    rotation[..., 2, 1] = qda + qbc\n",
        "\n",
        "    rotation[..., 0, 2] = qdb + qac\n",
        "    rotation[..., 1, 2] = -qda + qbc\n",
        "    rotation[..., 2, 2] = 1.0 - qaa - qbb\n",
        "    return rotation\n",
        "\n",
        "def make_transform_matrix(positions,rotations,):\n",
        "    \"\"\"Create the 4x4 transformation matrix.\n",
        "    Note: This function uses numpy.\n",
        "    Args:\n",
        "    positions: Translation applied after the rotation.\n",
        "        Last column of the transformation matrix\n",
        "    rotations: Rotation. Top-left 3x3 matrix of the transformation matrix.\n",
        "    Returns:\n",
        "    transformation_matrix:\n",
        "    \"\"\"\n",
        "    # Create the 4x4 transformation matrix\n",
        "    rot_pos = np.broadcast_to(np.eye(4), (*positions.shape[:-1], 4, 4)).copy()\n",
        "    rot_pos[..., :3, :3] = rotations\n",
        "    rot_pos[..., :3, 3] = positions\n",
        "    return rot_pos\n",
        "\n",
        "def from_position_and_quaternion(positions, quaternions, use_unreal_axes):\n",
        "    if use_unreal_axes:\n",
        "        rotations = transform.Rotation.from_quat(quaternions).as_matrix()\n",
        "    else:\n",
        "        # Rotation matrix that rotates from world to object coordinates.\n",
        "        # Warning: Rotations should be given in blender convensions as\n",
        "        # scipy.transform uses different convensions.\n",
        "        rotations = blender_quat2rot(quaternions)\n",
        "    px2world_transform = make_transform_matrix(positions=positions,rotations=rotations)\n",
        "    return px2world_transform\n",
        "\n",
        "def scale_rays(all_rays_o, all_rays_d, scene_boundaries, img_wh):\n",
        "    \"\"\"Rescale scene boundaries.\n",
        "    rays_o: (len(image_paths)*h*w, 3)\n",
        "    rays_d: (len(image_paths)*h*w, 3)\n",
        "    scene_boundaries: np.array(2 ,3), [min, max]\n",
        "    img_wh: (2)\n",
        "    \"\"\"\n",
        "    # Rescale (x, y, z) from [min, max] -> [-1, 1]\n",
        "    all_rays_o = all_rays_o.reshape(-1, img_wh[0], img_wh[1], 3) # (len(image_paths), h, w, 3))\n",
        "    all_rays_d = all_rays_d.reshape(-1, img_wh[0], img_wh[1], 3)\n",
        "\n",
        "    old_min = torch.from_numpy(scene_boundaries[0])\n",
        "    old_max = torch.from_numpy(scene_boundaries[1])\n",
        "    new_min = torch.tensor([-1,-1,-1])\n",
        "    new_max = torch.tensor([1,1,1])\n",
        "    # scale = max(scene_boundaries[1] - scene_boundaries[0])/2\n",
        "    # all_rays_o = (all_rays_o - torch.mean(all_rays_o, dim=-1, keepdim=True)) / scale\n",
        "    # This is from jax3d.interp, kind of weird but true\n",
        "    all_rays_o = ((new_min - new_max) / (old_min - old_max))*all_rays_o + (old_min * new_max - new_min * old_max) / (old_min - old_max)\n",
        "\n",
        "    # We also need to rescale the camera direction by bbox.size.\n",
        "    # The direction can be though of a ray from a point in space (the camera\n",
        "    # origin) to another point in space (say the red light on the lego\n",
        "    # bulldozer). When we scale the scene in a certain way, this direction\n",
        "    # also needs to be scaled in the same way.\n",
        "    all_rays_d = all_rays_d * 2 / (scene_boundaries[1] - scene_boundaries[0])\n",
        "    # (re)-normalize the rays\n",
        "    all_rays_d = all_rays_d / torch.linalg.norm(all_rays_d, dim=-1, keepdims=True)\n",
        "    return all_rays_o.reshape(-1, 3), all_rays_d.reshape(-1, 3)\n",
        "\n",
        "\n",
        "# Nesf Klevr\n",
        "\n",
        "class BlenderDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', get_rgb=True) -> None:\n",
        "        # super().__init__()\n",
        "        '''\n",
        "        split: train/val/test\n",
        "        '''\n",
        "        self.root_dir = root_dir\n",
        "        self.get_rgb = get_rgb\n",
        "        self.split = split\n",
        "\n",
        "        self.define_transforms()\n",
        "        self.read_meta()\n",
        "\n",
        "    def define_transforms(self):\n",
        "        self.transform = T.ToTensor()\n",
        "\n",
        "    def read_meta(self):\n",
        "        with open(os.path.join(self.root_dir, \"metadata.json\"), \"r\") as f:\n",
        "            self.meta = json.load(f)\n",
        "\n",
        "        w, h = self.meta['metadata']['width'], self.meta['metadata']['width']\n",
        "        self.img_wh = (w, h)\n",
        "        self.focal = (self.meta['camera']['focal_length']*w/self.meta['camera']['sensor_width'])\n",
        "        if self.split not in ['train', 'val', 'test']:\n",
        "            raise ValueError(f\"split should be train/val/test, got {self.split}\")\n",
        "        self.split_ids = self.meta['split_ids'][self.split]\n",
        "\n",
        "        self.scene_boundaries = np.array([self.meta['scene_boundaries']['min'], self.meta['scene_boundaries']['max']])\n",
        "        self.directions = get_ray_directions(h, w, self.focal)\n",
        "\n",
        "        if self.split == 'train':\n",
        "            self.poses = []\n",
        "            self.all_rays_o = []\n",
        "            self.all_rays_d = []\n",
        "            self.all_rays = []\n",
        "            self.all_rgbs = []\n",
        "            camera_positions = np.array(self.meta['camera']['positions'])\n",
        "            camera_quaternions = np.array(self.meta['camera']['quaternions'])\n",
        "            for image_id in self.split_ids:\n",
        "                if self.get_rgb:\n",
        "                    image_path = os.path.join(self.root_dir, f'{image_id:05d}.png')\n",
        "                    img = Image.open(image_path)\n",
        "                    img = img.resize(self.img_wh, Image.Resampling.LANCZOS)\n",
        "                    img = self.transform(img) # (4, h, w)\n",
        "                    img = img.view(4, -1).permute(1,0) # (h*w, 4)\n",
        "                    # not sure, original jax implementation seems not using blend just cut it off, they also /255 to make it [0,1] which I didn't use\n",
        "                    # img = img[:, :3]*img[:, -1:]+(1-img[:,-1:]) # blend A to RGB\n",
        "                    img = img[:, :3]\n",
        "                    self.all_rgbs += [img]\n",
        "\n",
        "                pose = np.array(from_position_and_quaternion(camera_positions[image_id:image_id+1,:], camera_quaternions[image_id:image_id+1,:], False))[0,:3,:4]\n",
        "                self.poses += [pose]\n",
        "                c2w = torch.FloatTensor(pose)\n",
        "                rays_o, rays_d = get_rays(self.directions, c2w)\n",
        "                self.all_rays_o += [rays_o]\n",
        "                self.all_rays_d += [rays_d]\n",
        "\n",
        "            self.all_rays_o = torch.cat(self.all_rays_o, 0) # (len(self.split_ids)*h*w, 3)\n",
        "            self.all_rays_d = torch.cat(self.all_rays_d, 0) # (len(self.split_ids)*h*w, 3)\n",
        "            self.all_rays_o, self.all_rays_d = scale_rays(self.all_rays_o, self.all_rays_d, self.scene_boundaries, self.img_wh)\n",
        "\n",
        "\n",
        "            self.near, self.far = calculate_near_and_far(self.all_rays_o, self.all_rays_d)\n",
        "            self.all_rays = torch.cat([self.all_rays_o, self.all_rays_d, self.near, self.far],1).float()\n",
        "\n",
        "            if len(self.all_rgbs) > 0:\n",
        "                self.all_rgbs = torch.cat(self.all_rgbs, 0)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.split == 'train':\n",
        "            return len(self.all_rays)\n",
        "        elif self.split == 'val' or self.split == 'test':\n",
        "            return len(self.split_ids)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.split == 'train':\n",
        "            sample = {\n",
        "                'rays': self.all_rays[idx],\n",
        "            }\n",
        "            if self.get_rgb:\n",
        "                sample['rgbs'] = self.all_rgbs[idx]\n",
        "\n",
        "\n",
        "        # split of val/test\n",
        "        elif self.split == 'val':\n",
        "            image_id = self.split_ids[idx]\n",
        "\n",
        "            if self.get_rgb:\n",
        "                img = Image.open(os.path.join(self.root_dir, f'{image_id:05d}.png'))\n",
        "                img = img.resize(self.img_wh, Image.Resampling.LANCZOS)\n",
        "                img = self.transform(img)\n",
        "                valid_mask = (img[-1]>0).flatten()\n",
        "                img = img.view(4,-1).permute(1,0)\n",
        "                # img = img[:,:3]*img[:,-1:]+(1-img[:,-1:])\n",
        "                img = img[:, :3]\n",
        "\n",
        "            camera_position = np.array(self.meta['camera']['positions'][image_id:image_id+1])\n",
        "            camera_quaternion = np.array(self.meta['camera']['quaternions'][image_id:image_id+1])\n",
        "\n",
        "            pose = np.array(from_position_and_quaternion(camera_position, camera_quaternion, False))[0,:3,:4]\n",
        "            # pose = np.array(from_position_and_quaternion(camera_position[idx:1+idx,:], camera_quaternion[idx:1+idx,:], False))[0,:3,:4]\n",
        "            c2w = torch.FloatTensor(pose)[:3,:4]\n",
        "            rays_o, rays_d = get_rays(self.directions, c2w)\n",
        "            rays_o, rays_d = scale_rays(rays_o, rays_d, self.scene_boundaries, self.img_wh)\n",
        "\n",
        "            self.near, self.far = calculate_near_and_far(rays_o, rays_d)\n",
        "            rays = torch.cat([rays_o, rays_d,\n",
        "                              self.near,\n",
        "                              self.far],\n",
        "                              1) # (H*W, 8)\n",
        "\n",
        "            sample = {'rays': rays.float(),\n",
        "                      'c2w': c2w}\n",
        "            if self.get_rgb:\n",
        "                sample['rgbs'] = img\n",
        "                sample['valid_mask'] = valid_mask\n",
        "\n",
        "        elif self.split == 'test':\n",
        "            image_id = self.split_ids[idx]\n",
        "\n",
        "            camera_position = np.array(self.meta['camera']['positions'][image_id:image_id+1])\n",
        "            camera_quaternion = np.array(self.meta['camera']['quaternions'][image_id:image_id+1])\n",
        "\n",
        "            pose = np.array(from_position_and_quaternion(camera_position, camera_quaternion, False))[0,:3,:4]\n",
        "            # pose = np.array(from_position_and_quaternion(camera_position[idx:1+idx,:], camera_quaternion[idx:1+idx,:], False))[0,:3,:4]\n",
        "            c2w = torch.FloatTensor(pose)[:3,:4]\n",
        "            rays_o, rays_d = get_rays(self.directions, c2w)\n",
        "            rays_o, rays_d = scale_rays(rays_o, rays_d, self.scene_boundaries, self.img_wh)\n",
        "\n",
        "            self.near, self.far = calculate_near_and_far(rays_o, rays_d)\n",
        "            rays = torch.cat([rays_o, rays_d,\n",
        "                              self.near,\n",
        "                              self.far],\n",
        "                              1) # (H*W, 8)\n",
        "\n",
        "            sample = {'rays': rays.float(),\n",
        "                      'c2w': c2w}\n",
        "\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "VaJxXIKJGIaD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train.py"
      ],
      "metadata": {
        "id": "XmVoGrPLtVaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from opt import get_opts\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import dataset_dict\n",
        "\n",
        "# models\n",
        "from models.nerf import Embedding, NeRF\n",
        "from models.rendering import render_rays\n",
        "\n",
        "# optimizer, scheduler, visualization\n",
        "from utils import *\n",
        "\n",
        "# losses\n",
        "from losses import loss_dict\n",
        "\n",
        "# metrics\n",
        "from metrics import *\n",
        "\n",
        "# pytorch-lightning\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "class NeRFSystem(LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(NeRFSystem, self).__init__()\n",
        "        # self.hparams = hparams\n",
        "        self.save_hyperparameters(hparams)\n",
        "\n",
        "        self.loss = loss_dict[hparams.loss_type]()\n",
        "\n",
        "        self.embedding_xyz = Embedding(3, 10) # 10 is the default number\n",
        "        self.embedding_dir = Embedding(3, 4) # 4 is the default number\n",
        "        self.embeddings = [self.embedding_xyz, self.embedding_dir]\n",
        "\n",
        "        self.nerf_coarse = NeRF()\n",
        "        self.models = [self.nerf_coarse]\n",
        "        if hparams.N_importance > 0:\n",
        "            self.nerf_fine = NeRF()\n",
        "            self.models += [self.nerf_fine]\n",
        "\n",
        "    def decode_batch(self, batch):\n",
        "        rays = batch['rays'] # (B, 8)\n",
        "        rgbs = batch['rgbs'] # (B, 3)\n",
        "        return rays, rgbs\n",
        "\n",
        "    def forward(self, rays):\n",
        "        \"\"\"Do batched inference on rays using chunk.\"\"\"\n",
        "        B = rays.shape[0]\n",
        "        results = defaultdict(list)\n",
        "        for i in range(0, B, self.hparams.chunk):\n",
        "            rendered_ray_chunks = \\\n",
        "                render_rays(self.models,\n",
        "                            self.embeddings,\n",
        "                            rays[i:i+self.hparams.chunk],\n",
        "                            self.hparams.N_samples,\n",
        "                            self.hparams.use_disp,\n",
        "                            self.hparams.perturb,\n",
        "                            self.hparams.noise_std,\n",
        "                            self.hparams.N_importance,\n",
        "                            self.hparams.chunk,) # chunk size is effective in val mode\n",
        "                            # self.train_dataset.white_back)\n",
        "\n",
        "            for k, v in rendered_ray_chunks.items():\n",
        "                results[k] += [v]\n",
        "\n",
        "        for k, v in results.items():\n",
        "            results[k] = torch.cat(v, 0)\n",
        "        return results\n",
        "\n",
        "    def prepare_data(self):\n",
        "        dataset = dataset_dict[self.hparams.dataset_name]\n",
        "        kwargs = {'root_dir': self.hparams.root_dir,\n",
        "                  } # 'img_wh': tuple(self.hparams.img_wh)\n",
        "        if self.hparams.dataset_name == 'llff':\n",
        "            kwargs['spheric_poses'] = self.hparams.spheric_poses\n",
        "            kwargs['val_num'] = self.hparams.num_gpus\n",
        "        self.train_dataset = dataset(split='train', **kwargs)\n",
        "        self.val_dataset = dataset(split='val', **kwargs)\n",
        "        # test_dataset\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        self.optimizer = get_optimizer(self.hparams, self.models)\n",
        "        scheduler = get_scheduler(self.hparams, self.optimizer)\n",
        "\n",
        "        return [self.optimizer], [scheduler]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset,\n",
        "                          shuffle=True,\n",
        "                          num_workers=4,\n",
        "                          batch_size=self.hparams.batch_size,\n",
        "                          pin_memory=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset,\n",
        "                          shuffle=False,\n",
        "                          num_workers=4,\n",
        "                          batch_size=1, # validate one image (H*W rays) at a time\n",
        "                          pin_memory=True)\n",
        "    # def test_loader\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        log = {'lr': get_learning_rate(self.optimizer)}\n",
        "        rays, rgbs = self.decode_batch(batch)\n",
        "        results = self(rays)\n",
        "        log['train/loss'] = loss = self.loss(results, rgbs)\n",
        "        typ = 'fine' if 'rgb_fine' in results else 'coarse'\n",
        "\n",
        "        with torch.no_grad():\n",
        "            psnr_ = psnr(results[f'rgb_{typ}'], rgbs)\n",
        "            log['train/psnr'] = psnr_\n",
        "\n",
        "        return {'loss': loss,\n",
        "                'progress_bar': {'train_psnr': psnr_},\n",
        "                'log': log\n",
        "               }\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        rays, rgbs = self.decode_batch(batch)\n",
        "        rays = rays.squeeze() # (H*W, 3)\n",
        "        rgbs = rgbs.squeeze() # (H*W, 3)\n",
        "        results = self(rays)\n",
        "        log = {'val_loss': self.loss(results, rgbs)}\n",
        "        typ = 'fine' if 'rgb_fine' in results else 'coarse'\n",
        "\n",
        "        if batch_nb == 0:\n",
        "            W, H = self.hparams.img_wh\n",
        "            img = results[f'rgb_{typ}'].view(H, W, 3).cpu()\n",
        "            img = img.permute(2, 0, 1) # (3, H, W)\n",
        "            img_gt = rgbs.view(H, W, 3).permute(2, 0, 1).cpu() # (3, H, W)\n",
        "            depth = visualize_depth(results[f'depth_{typ}'].view(H, W)) # (3, H, W)\n",
        "            stack = torch.stack([img_gt, img, depth]) # (3, 3, H, W)\n",
        "            self.logger.experiment.add_images('val/GT_pred_depth',\n",
        "                                               stack, self.global_step)\n",
        "\n",
        "        log['val_psnr'] = psnr(results[f'rgb_{typ}'], rgbs)\n",
        "        return log\n",
        "\n",
        "    # def test_step(self, batch, batch_nb):\n",
        "    #     rays = self.decode_batch(batch)\n",
        "    #     rays = rays.squeeze() # (H*W, 3)\n",
        "    #     rgbs = rgbs.squeeze() # (H*W, 3)\n",
        "    #     results = self(rays)\n",
        "    #     log = {'val_loss': self.loss(results, rgbs)}\n",
        "    #     typ = 'fine' if 'rgb_fine' in results else 'coarse'\n",
        "\n",
        "    #     W, H = self.hparams.img_wh\n",
        "    #     img = results[f'rgb_{typ}'].view(H, W, 3).cpu()\n",
        "    #     img = img.permute(2, 0, 1) # (3, H, W)\n",
        "    #     img_gt = rgbs.view(H, W, 3).permute(2, 0, 1).cpu() # (3, H, W)\n",
        "    #     depth = visualize_depth(results[f'depth_{typ}'].view(H, W)) # (3, H, W)\n",
        "    #     stack = torch.stack([img_gt, img, depth]) # (3, 3, H, W)\n",
        "    #     self.logger.experiment.add_images('val/GT_pred_depth',\n",
        "    #                                          stack, self.global_step)\n",
        "\n",
        "    #     save  img\n",
        "\n",
        "    #     log['val_psnr'] = psnr(results[f'rgb_{typ}'], rgbs)\n",
        "    #     return log\n",
        "\n",
        "    # def validation_epoch_end(self, outputs):\n",
        "    #     mean_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "    #     mean_psnr = torch.stack([x['val_psnr'] for x in outputs]).mean()\n",
        "\n",
        "    #     return {'progress_bar': {'val_loss': mean_loss,\n",
        "    #                              'val_psnr': mean_psnr},\n",
        "    #             'log': {'val/loss': mean_loss,\n",
        "    #                     'val/psnr': mean_psnr}\n",
        "    #            }\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    hparams = get_opts()\n",
        "    system = NeRFSystem(hparams)\n",
        "    checkpoint_callback = ModelCheckpoint(dirpath=f'ckpts/{hparams.exp_name}',\n",
        "                                          filename='base',\n",
        "                                          monitor='val/loss',\n",
        "                                          mode='min',\n",
        "                                          save_top_k=5,)\n",
        "\n",
        "    logger = TensorBoardLogger(\n",
        "        save_dir=\"logs\",\n",
        "        name=hparams.exp_name,\n",
        "        # debug=False,\n",
        "        # create_git_tag=False\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(max_epochs=hparams.num_epochs,\n",
        "                      callbacks=checkpoint_callback,\n",
        "                      # resume_from_checkpoint=hparams.ckpt_path,\n",
        "                      logger=logger,\n",
        "                      # early_stop_callback=None,\n",
        "                      # weights_summary=None,\n",
        "                      # progress_bar_refresh_rate=1,\n",
        "                      # gpus=hparams.num_gpus,\n",
        "                      # distributed_backend='ddp' if hparams.num_gpus>1 else None,\n",
        "                      num_sanity_val_steps=1,\n",
        "                      benchmark=True,)\n",
        "                      # profiler=hparams.num_gpus==1)\n",
        "\n",
        "    trainer.fit(system)"
      ],
      "metadata": {
        "id": "xvVZGRM2tXDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### training"
      ],
      "metadata": {
        "id": "hOhJ70qh2Fhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "rendering -> torch.searchsorted \\"
      ],
      "metadata": {
        "id": "fMxcqFAoNb59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd nerf_pl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71Fa1rOU2PGq",
        "outputId": "fb3816e3-49de-4314-d57d-e1b3ce739215"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nerf_pl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning==2.1.2"
      ],
      "metadata": {
        "id": "wJix88eq2jyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "   --dataset_name blender \\\n",
        "   --root_dir '/content/dataset' \\\n",
        "   --N_importance 64 --img_wh 256 256 --noise_std 0 \\\n",
        "   --num_epochs 16 --batch_size 1024 \\\n",
        "   --optimizer adam --lr 5e-4 \\\n",
        "   --lr_scheduler steplr --decay_step 2 4 8 --decay_gamma 0.5 \\\n",
        "   --exp_name exp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXadp9Ty2G8v",
        "outputId": "a72bb118-28d8-44e4-f068-34e49ce434ec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: logs/exp\n",
            "2023-12-06 06:28:16.854394: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-06 06:28:16.854471: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-06 06:28:16.854526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-06 06:28:18.006665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name          | Type      | Params\n",
            "--------------------------------------------\n",
            "0 | loss          | MSELoss   | 0     \n",
            "1 | embedding_xyz | Embedding | 0     \n",
            "2 | embedding_dir | Embedding | 0     \n",
            "3 | nerf_coarse   | NeRF      | 595 K \n",
            "4 | nerf_fine     | NeRF      | 595 K \n",
            "--------------------------------------------\n",
            "1.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.2 M     Total params\n",
            "4.767     Total estimated model params size (MB)\n",
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch 0: 100% 5184/5184 [20:52<00:00,  4.14it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]       \u001b[A\n",
            "Validation DataLoader 0:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 20/20 [02:04<00:00,  6.20s/it]\u001b[A\n",
            "Epoch 0: 100% 5184/5184 [22:57<00:00,  3.76it/s, v_num=0]Traceback (most recent call last):\n",
            "  File \"/content/nerf_pl/train.py\", line 205, in <module>\n",
            "    trainer.fit(system)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 544, in fit\n",
            "    call._call_and_handle_interrupt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
            "    return trainer_fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 580, in _fit_impl\n",
            "    self._run(model, ckpt_path=ckpt_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 989, in _run\n",
            "    results = self._run_stage()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1035, in _run_stage\n",
            "    self.fit_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 203, in run\n",
            "    self.on_advance_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 374, in on_advance_end\n",
            "    call._call_callback_hooks(trainer, \"on_train_epoch_end\", monitoring_callbacks=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 208, in _call_callback_hooks\n",
            "    fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 313, in on_train_epoch_end\n",
            "    self._save_topk_checkpoint(trainer, monitor_candidates)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 368, in _save_topk_checkpoint\n",
            "    raise MisconfigurationException(m)\n",
            "lightning_fabric.utilities.exceptions.MisconfigurationException: `ModelCheckpoint(monitor='val/loss')` could not find the monitored key in the returned metrics: ['epoch', 'step']. HINT: Did you call `log('val/loss', value)` in the `LightningModule`?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tORYWYSKmYpd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}