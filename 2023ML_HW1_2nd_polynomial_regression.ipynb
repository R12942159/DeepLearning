{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R12942159/DeepLearning/blob/main/2023ML_HW1_2nd_polynomial_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz0_QVkxCrX3"
      },
      "source": [
        "# **2023 ML FALL HW1: PM2.5 Prediction (Regression)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeZnPAiwDRWG"
      },
      "source": [
        "Author: MLTAs\n",
        "\n",
        "Methods:\n",
        "* Training with all data\n",
        "* Training config: mini-batch=512, optimizer=Adam, learning rate=0.1 (TODO: Change the config!)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS_4-77xHk44"
      },
      "source": [
        "# **Import Some Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-onQd4JNA5H"
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import math\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iUIroiX8jBlb",
        "outputId": "cb6ae4a1-0c6d-42dc-cd30-82cce5b595c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fix random seed**\n",
        "\n",
        "\n",
        "This is for the reproduction of your result. **DO NOT modify this secton!**\n"
      ],
      "metadata": {
        "id": "aqMEWsRekx0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Edit: use np.random.seed(seed) (2022.10.12)\n",
        "seed = 9487\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "UxDA6fJb_Uem"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OVRMuTAc1_E"
      },
      "source": [
        "# **Download training data**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id \"1Hfzrcm69QwdFvdeF0uASoQlcVxKw_hHy\" --output \"train.csv\"\n",
        "\n",
        "## Edit (2022.10.14) test.csv: 7th(WS_HR) and last column(pm2.5) is correctly switched.\n",
        "!gdown --id '155N6fzI7vAFzHAGdy6jkaWIksWH6Y1G2' --output \"test.csv\"\n",
        "\n",
        "# Incase the links above die, you can use the following instead.\n",
        "#!gdown --id '11abE854Eyv4BA7qt5k8r_80sJ3KuOQUN' --output \"train.csv\"\n",
        "#!gdown --id '1uod-Z4ztluXnuHtgUbm39nMudUKqXHMl' --output \"test.csv\"\n",
        "\n",
        "# If the data is still missing, you can manually download it from kaggle, and upload the files under /content"
      ],
      "metadata": {
        "id": "s0Zo8JUp5kJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHpuZmQwXpz8"
      },
      "source": [
        "def valid(x, y):\n",
        "  # TODO: Try to filter out extreme values.\n",
        "  #  ex: If PM2.5 > 100, then we don't use the data to train (return False), otherwise return True,\n",
        "\n",
        "  return True\n",
        "\n",
        "\n",
        "# Create your dataset\n",
        "def parse2train(data, feats):\n",
        "\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  # Use data #0~#7 to predict #8 => Total data length should be decresased by 8.\n",
        "  total_length = data.shape[1] - 8\n",
        "\n",
        "  for i in range(total_length):\n",
        "    x_tmp = data[feats, i:i+8] # Use data #0~#7 to predict #8, data #1~#8 to predict #9, etc.\n",
        "    y_tmp = data[-1, i+8] # last column of (i+8)th row: PM2.5\n",
        "\n",
        "    # Filter out extreme values to train.\n",
        "    if valid(x_tmp, y_tmp):\n",
        "      x.append(x_tmp.reshape(-1,))\n",
        "      y.append(y_tmp)\n",
        "\n",
        "  # x.shape: (n, 15, 8)\n",
        "  # y.shape: (n, 1)\n",
        "  x = np.array(x)\n",
        "  y = np.array(y)\n",
        "\n",
        "  return x,y\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyEpvVVQdZ0c"
      },
      "source": [
        "#**Adam**\n",
        "* This is our gradient descent algorithm. Adam was implemented.\n",
        "* You can implement another algorithm such as SGD, which may (or may not) boost the performance.\n",
        "* However, **modules like sklearn and pytorch are not allowed**.\n",
        "* Ref: https://arxiv.org/pdf/1412.6980.pdf\n",
        "\n",
        "![](https://i.imgur.com/jRaebdf.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = np.arange(train_x.shape[0])\n",
        "np.random.shuffle(index)\n",
        "train_x = train_x[index]"
      ],
      "metadata": {
        "id": "ltZx1lika2Zy"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL_RVBoLuXvj"
      },
      "source": [
        "# TODO: Implement 2-nd polynomial regression version for the report.\n",
        "def minibatch(x, y, config):\n",
        "\n",
        "    # Randomize the data in minibatch\n",
        "    index = np.arange(x.shape[0])\n",
        "    np.random.shuffle(index)\n",
        "    x = x[index]\n",
        "    y = y[index]\n",
        "\n",
        "    # Initialization\n",
        "    batch_size = config.batch_size\n",
        "    lr = config.lr\n",
        "    lam = config.lam\n",
        "    epoch = config.epoch\n",
        "\n",
        "    beta_1 = np.full(x[0].shape, 0.9).reshape(-1, 1)\n",
        "    beta_2 = np.full(x[0].shape, 0.99).reshape(-1, 1)\n",
        "\n",
        "    # Linear regression: only contains two parameters (w, b).\n",
        "    # 2-nd polynomial regression(w1,w2,b)\n",
        "    w1 = np.full(x[0].shape, 0.1).reshape(-1, 1)\n",
        "    w2 = np.full(x[0].shape, 0.1).reshape(-1, 1)\n",
        "    bias = 0.1\n",
        "\n",
        "    m_t1 = np.full(x[0].shape, 0).reshape(-1, 1)\n",
        "    m_t2 = np.full(x[0].shape, 0).reshape(-1, 1)\n",
        "    m_t_3 = 0.0\n",
        "\n",
        "    v_t1 = np.full(x[0].shape, 0).reshape(-1, 1)\n",
        "    v_t2 = np.full(x[0].shape, 0).reshape(-1, 1)\n",
        "    v_t_3 = 0.0\n",
        "\n",
        "    t = 0\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    # Training loop\n",
        "    for num in range(epoch):\n",
        "        for b in range(int(x.shape[0]/batch_size)):\n",
        "            t+=1\n",
        "            x_batch = x[b*batch_size:(b+1)*batch_size]\n",
        "            y_batch = y[b*batch_size:(b+1)*batch_size].reshape(-1,1)\n",
        "\n",
        "            # Prediction of linear regression\n",
        "            # pred = np.dot(x_batch,w) + bias\n",
        "            # pred = w1 * x_batch**2 + w2 * x_batch + bias\n",
        "            pred = np.dot(x_batch**2, w1) + np.dot(x_batch, w2) + bias\n",
        "\n",
        "            # loss\n",
        "            loss = y_batch - pred\n",
        "\n",
        "            # Compute gradient\n",
        "            ## Edit: remove 2 * lam * np.sum(w)  (2022.10.11)\n",
        "            # https://math.stackexchange.com/questions/1962877/compute-the-gradient-of-mean-square-error\n",
        "            g_t1 = np.dot((x_batch**2).transpose(), loss) * (-2)\n",
        "            g_t2 = np.dot(x_batch.transpose(),loss) * (-2)\n",
        "            g_t_3 = loss.sum(axis=0) * (-2)\n",
        "\n",
        "            m_t1 = beta_1*m_t1 + (1-beta_1)*g_t1\n",
        "            m_t2 = beta_1*m_t2 + (1-beta_1)*g_t2\n",
        "            m_t_3 = 0.9*m_t_3 + (1-0.9)*g_t_3\n",
        "\n",
        "            v_t1 = beta_2*v_t1 + (1-beta_2)*np.multiply(g_t1, g_t1)\n",
        "            v_t2 = beta_2*v_t2 + (1-beta_2)*np.multiply(g_t2, g_t2)\n",
        "            v_t_3 = 0.99*v_t_3 + (1-0.99)*(g_t_3*g_t_3)\n",
        "\n",
        "            m_cap1 = m_t1/(1-(beta_1**t))\n",
        "            m_cap2 = m_t2/(1-(beta_1**t))\n",
        "            m_cap_3 = m_t_3/(1-(0.9**t))\n",
        "\n",
        "            v_cap1 = v_t1/(1-(beta_2**t))\n",
        "            v_cap2 = v_t2/(1-(beta_2**t))\n",
        "            v_cap_3 = v_t_3/(1-(0.99**t))\n",
        "\n",
        "            w_1 = np.copy(w1)\n",
        "            w_2 = np.copy(w2)\n",
        "\n",
        "            # Update weight & bias\n",
        "            w1 -= ((lr*m_cap1)/(np.sqrt(v_cap1)+epsilon))\n",
        "            w2 -= ((lr*m_cap2)/(np.sqrt(v_cap2)+epsilon))\n",
        "            bias -= (lr*m_cap_3)/(math.sqrt(v_cap_3)+epsilon)\n",
        "\n",
        "\n",
        "    return w1, w2, bias"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "# TODO: Tune the config to boost your performance.\n",
        "train_config = Namespace(\n",
        "    batch_size = 512,\n",
        "    lr = 1e-1,\n",
        "    lam = 0.001,\n",
        "    epoch = 1,\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZpdOsMfXLxH2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training your regression model**"
      ],
      "metadata": {
        "id": "ay-RhqqA88vS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/train.csv\")\n",
        "data"
      ],
      "metadata": {
        "id": "EoR5Q5kvJm4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose your features to train.\n",
        "# Hint:\n",
        "# 1. You can select more than one feature.\n",
        "# 2. You should select \"good\" features.\n",
        "\n",
        "# TODO: Carefully justify which feature should be chosen.\n",
        "feats = [2]"
      ],
      "metadata": {
        "id": "_Akqj5yYVGHA"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data preprocessing.\n",
        "\n",
        "data = data.values\n",
        "train_data = np.transpose(np.array(np.float64(data)))\n",
        "train_x, train_y = parse2train(train_data, feats)"
      ],
      "metadata": {
        "id": "AiEWGMQXLM99"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhfoPJUhcnH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b61c5f6b-94cf-4d3c-9d07-c5c14d90d003"
      },
      "source": [
        "# Train your regression model\n",
        "\n",
        "w1, w2, bias = minibatch(train_x, train_y, train_config)\n",
        "print(w1.shape, w2.shape, bias.shape)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 1) (8, 1) (1,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1, w2, bias"
      ],
      "metadata": {
        "id": "P1czVSvv4u88",
        "outputId": "2df5ba68-289d-4527-8fed-c1efc44ac89b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0.01593716],\n",
              "        [-0.04401034],\n",
              "        [ 0.10527231],\n",
              "        [ 0.06896598],\n",
              "        [-0.02042791],\n",
              "        [-0.05631573],\n",
              "        [ 0.03311423],\n",
              "        [ 0.11240207]]),\n",
              " array([[0.24830965],\n",
              "        [0.16806609],\n",
              "        [0.24610574],\n",
              "        [0.26935487],\n",
              "        [0.21059001],\n",
              "        [0.17554829],\n",
              "        [0.21577532],\n",
              "        [0.2831783 ]]),\n",
              " array([0.73146061]))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "019GwPMrbmrB"
      },
      "source": [
        "# **Testing:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FjQNzOb6BeQ"
      },
      "source": [
        "def parse2test(data, feats):\n",
        "  x = []\n",
        "  for i in range(90):\n",
        "    x_tmp = data[feats,8*i: 8*i+8]\n",
        "    x.append(x_tmp.reshape(-1,))\n",
        "\n",
        "  # x.shape: (n, 15, 8)\n",
        "  x = np.array(x)\n",
        "  return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('test.csv')\n",
        "print(data)\n",
        "data = data.values\n",
        "\n",
        "test_data = np.transpose(np.array(np.float64(data)))\n",
        "test_x = parse2test(test_data, feats)"
      ],
      "metadata": {
        "id": "z40o9QbAYbR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "182c0119-af21-42e5-9c81-cbddba14d168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     AMB_TEMP    CO   NO   NO2   NOx    O3  PM10  WS_HR  RAINFALL    RH  SO2  \\\n",
            "0        27.5  0.22  0.7   9.0   9.8  13.2  31.0    1.2       0.0  79.0  1.7   \n",
            "1        27.2  0.17  0.4   5.0   5.4  15.7  20.0    1.5       0.0  79.0  1.6   \n",
            "2        26.8  0.17  0.4   4.3   4.8  12.8  16.0    1.6       0.0  81.0  1.3   \n",
            "3        26.7  0.19  0.4   4.1   4.5  12.0  21.0    1.7       0.0  80.0  1.5   \n",
            "4        26.4  0.22  0.4   4.1   4.6  10.1  23.0    2.2       0.0  81.0  1.5   \n",
            "..        ...   ...  ...   ...   ...   ...   ...    ...       ...   ...  ...   \n",
            "715      16.0  0.26  0.3   3.9   4.2  47.1  34.0    2.7       0.0  70.0  0.5   \n",
            "716      15.6  0.25  0.4   3.3   3.7  44.1  27.0    3.0       0.0  74.0  0.6   \n",
            "717      15.7  0.24  0.4   3.7   4.1  44.1  29.0    2.9       0.0  73.0  0.6   \n",
            "718      15.1  0.24  0.6  10.5  11.1  29.9   9.0    0.8       0.0  95.0  0.6   \n",
            "719      15.8  0.28  0.6   6.0   6.7  40.5  28.0    3.0       0.0  74.0  0.5   \n",
            "\n",
            "     WD_HR  WIND_DIREC  WIND_SPEED  PM2.5  \n",
            "0    180.0       171.0         1.2   20.0  \n",
            "1    192.0       187.0         1.9    8.0  \n",
            "2    181.0       180.0         1.8    9.0  \n",
            "3    179.0       188.0         2.3    6.0  \n",
            "4    184.0       186.0         1.9    5.0  \n",
            "..     ...         ...         ...    ...  \n",
            "715  130.0       133.0         2.8   13.0  \n",
            "716  136.0       131.0         3.2   15.0  \n",
            "717  133.0       129.0         2.7   12.0  \n",
            "718   24.0        21.0         1.1    8.0  \n",
            "719  133.0       131.0         2.9   14.0  \n",
            "\n",
            "[720 rows x 15 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWrfEwaEdO6J"
      },
      "source": [
        "# **Write result as .csv**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqEQ1fZ9-WMO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14dcc595-b42e-4eb0-c04a-1d1e370445bb"
      },
      "source": [
        " with open('my_sol.csv', 'w', newline='') as csvf:\n",
        "    # 建立 CSV 檔寫入器\n",
        "    writer = csv.writer(csvf)\n",
        "    writer.writerow(['Id','Predicted'])\n",
        "\n",
        "    print(test_x.shape)\n",
        "    for i in range(int(test_x.shape[0])):\n",
        "      # Prediction of linear regression\n",
        "      prediction = (np.dot(np.reshape(w,-1),test_x[i]) + bias)[0]\n",
        "      writer.writerow([i, prediction] )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90, 64)\n"
          ]
        }
      ]
    }
  ]
}