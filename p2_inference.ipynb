{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R12942159/NTU_DLCV/blob/Hw2/p2_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as tr\n",
        "from torchvision.utils import save_image, make_grid"
      ],
      "metadata": {
        "id": "139SnVqXqy6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-m8GI4dOVJN",
        "outputId": "17ee705d-fd40-4cc5-d158-86bfd17557e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gsutil\n",
        "!gsutil cp /content/drive/MyDrive/NTU_DLCV/Hw2/hw2_data.zip /content/hw2_data.zip"
      ],
      "metadata": {
        "id": "sWg2FakTOZdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/hw2_data.zip"
      ],
      "metadata": {
        "id": "9H3l8O-9Oecc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoICoHcXqvLr",
        "outputId": "d04e927f-ac87-48d9-a8bd-e715bbb4ad61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOGsInhcc6co"
      },
      "outputs": [],
      "source": [
        "swish = F.silu\n",
        "\n",
        "@torch.no_grad()\n",
        "def variance_scaling_init_(tensor, scale=1, mode=\"fan_avg\", distribution=\"uniform\"):\n",
        "    fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(tensor)\n",
        "\n",
        "    if mode == \"fan_in\":\n",
        "        scale /= fan_in\n",
        "\n",
        "    elif mode == \"fan_out\":\n",
        "        scale /= fan_out\n",
        "\n",
        "    else:\n",
        "        scale /= (fan_in + fan_out) / 2\n",
        "\n",
        "    if distribution == \"normal\":\n",
        "        std = math.sqrt(scale)\n",
        "\n",
        "        return tensor.normal_(0, std)\n",
        "\n",
        "    else:\n",
        "        bound = math.sqrt(3 * scale)\n",
        "\n",
        "        return tensor.uniform_(-bound, bound)\n",
        "\n",
        "\n",
        "def conv2d(\n",
        "    in_channel,\n",
        "    out_channel,\n",
        "    kernel_size,\n",
        "    stride=1,\n",
        "    padding=0,\n",
        "    bias=True,\n",
        "    scale=1,\n",
        "    mode=\"fan_avg\",\n",
        "):\n",
        "    conv = nn.Conv2d(\n",
        "        in_channel, out_channel, kernel_size, stride=stride, padding=padding, bias=bias\n",
        "    )\n",
        "\n",
        "    variance_scaling_init_(conv.weight, scale, mode=mode)\n",
        "\n",
        "    if bias:\n",
        "        nn.init.zeros_(conv.bias)\n",
        "\n",
        "    return conv\n",
        "\n",
        "\n",
        "def linear(in_channel, out_channel, scale=1, mode=\"fan_avg\"):\n",
        "    lin = nn.Linear(in_channel, out_channel)\n",
        "\n",
        "    variance_scaling_init_(lin.weight, scale, mode=mode)\n",
        "    nn.init.zeros_(lin.bias)\n",
        "\n",
        "    return lin\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return swish(input)\n",
        "\n",
        "\n",
        "class Upsample(nn.Sequential):\n",
        "    def __init__(self, channel):\n",
        "        layers = [\n",
        "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
        "            conv2d(channel, channel, 3, padding=1),\n",
        "        ]\n",
        "\n",
        "        super().__init__(*layers)\n",
        "\n",
        "\n",
        "class Downsample(nn.Sequential):\n",
        "    def __init__(self, channel):\n",
        "        layers = [conv2d(channel, channel, 3, stride=2, padding=1)]\n",
        "\n",
        "        super().__init__(*layers)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channel, out_channel, time_dim, use_affine_time=False, dropout=0\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.use_affine_time = use_affine_time\n",
        "        time_out_dim = out_channel\n",
        "        time_scale = 1\n",
        "        norm_affine = True\n",
        "\n",
        "        if self.use_affine_time:\n",
        "            time_out_dim *= 2\n",
        "            time_scale = 1e-10\n",
        "            norm_affine = False\n",
        "\n",
        "        self.norm1 = nn.GroupNorm(32, in_channel)\n",
        "        self.activation1 = Swish()\n",
        "        self.conv1 = conv2d(in_channel, out_channel, 3, padding=1)\n",
        "\n",
        "        self.time = nn.Sequential(\n",
        "            Swish(), linear(time_dim, time_out_dim, scale=time_scale)\n",
        "        )\n",
        "\n",
        "        self.norm2 = nn.GroupNorm(32, out_channel, affine=norm_affine)\n",
        "        self.activation2 = Swish()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.conv2 = conv2d(out_channel, out_channel, 3, padding=1, scale=1e-10)\n",
        "\n",
        "        if in_channel != out_channel:\n",
        "            self.skip = conv2d(in_channel, out_channel, 1)\n",
        "\n",
        "        else:\n",
        "            self.skip = None\n",
        "\n",
        "    def forward(self, input, time):\n",
        "        batch = input.shape[0]\n",
        "\n",
        "        out = self.conv1(self.activation1(self.norm1(input)))\n",
        "\n",
        "        if self.use_affine_time:\n",
        "            gamma, beta = self.time(time).view(batch, -1, 1, 1).chunk(2, dim=1)\n",
        "            out = (1 + gamma) * self.norm2(out) + beta\n",
        "\n",
        "        else:\n",
        "            out = out + self.time(time).view(batch, -1, 1, 1)\n",
        "            out = self.norm2(out)\n",
        "\n",
        "        out = self.conv2(self.dropout(self.activation2(out)))\n",
        "\n",
        "        if self.skip is not None:\n",
        "            input = self.skip(input)\n",
        "\n",
        "        return out + input\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_channel, n_head=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "\n",
        "        self.norm = nn.GroupNorm(32, in_channel)\n",
        "        self.qkv = conv2d(in_channel, in_channel * 3, 1)\n",
        "        self.out = conv2d(in_channel, in_channel, 1, scale=1e-10)\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch, channel, height, width = input.shape\n",
        "        n_head = self.n_head\n",
        "        head_dim = channel // n_head\n",
        "\n",
        "        norm = self.norm(input)\n",
        "        qkv = self.qkv(norm).view(batch, n_head, head_dim * 3, height, width)\n",
        "        query, key, value = qkv.chunk(3, dim=2)  # bhdyx\n",
        "\n",
        "        attn = torch.einsum(\n",
        "            \"bnchw, bncyx -> bnhwyx\", query, key\n",
        "        ).contiguous() / math.sqrt(channel)\n",
        "        attn = attn.view(batch, n_head, height, width, -1)\n",
        "        attn = torch.softmax(attn, -1)\n",
        "        attn = attn.view(batch, n_head, height, width, height, width)\n",
        "\n",
        "        out = torch.einsum(\"bnhwyx, bncyx -> bnchw\", attn, value).contiguous()\n",
        "        out = self.out(out.view(batch, channel, height, width))\n",
        "\n",
        "        return out + input\n",
        "\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "\n",
        "        inv_freq = torch.exp(\n",
        "            torch.arange(0, dim, 2, dtype=torch.float32) * (-math.log(10000) / dim)\n",
        "        )\n",
        "\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "\n",
        "    def forward(self, input):\n",
        "        shape = input.shape\n",
        "        sinusoid_in = torch.ger(input.view(-1).float(), self.inv_freq)\n",
        "        pos_emb = torch.cat([sinusoid_in.sin(), sinusoid_in.cos()], dim=-1)\n",
        "        pos_emb = pos_emb.view(*shape, self.dim)\n",
        "\n",
        "        return pos_emb\n",
        "\n",
        "\n",
        "class ResBlockWithAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channel,\n",
        "        out_channel,\n",
        "        time_dim,\n",
        "        dropout,\n",
        "        use_attention=False,\n",
        "        attention_head=1,\n",
        "        use_affine_time=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.resblocks = ResBlock(\n",
        "            in_channel, out_channel, time_dim, use_affine_time, dropout\n",
        "        )\n",
        "\n",
        "        if use_attention:\n",
        "            self.attention = SelfAttention(out_channel, n_head=attention_head)\n",
        "\n",
        "        else:\n",
        "            self.attention = None\n",
        "\n",
        "    def forward(self, input, time):\n",
        "        out = self.resblocks(input, time)\n",
        "\n",
        "        if self.attention is not None:\n",
        "            out = self.attention(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def spatial_unfold(input, unfold):\n",
        "    if unfold == 1:\n",
        "        return input\n",
        "\n",
        "    batch, channel, height, width = input.shape\n",
        "    h_unfold = height * unfold\n",
        "    w_unfold = width * unfold\n",
        "\n",
        "    return (\n",
        "        input.view(batch, -1, unfold, unfold, height, width)\n",
        "        .permute(0, 1, 4, 2, 5, 3)\n",
        "        .reshape(batch, -1, h_unfold, w_unfold)\n",
        "    )\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channel = 3,\n",
        "        channel = 128,\n",
        "        attn_heads = 1,\n",
        "        use_affine_time = False,\n",
        "        dropout = 0,\n",
        "    ):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "\n",
        "        time_dim = channel * 4\n",
        "\n",
        "        self.time = nn.Sequential(\n",
        "            TimeEmbedding(channel),\n",
        "            linear(channel, time_dim),\n",
        "            Swish(),\n",
        "            linear(time_dim, time_dim),\n",
        "        )\n",
        "\n",
        "        self.down1 = conv2d(in_channel, channel, 3, padding=1)\n",
        "        self.down2 = ResBlockWithAttention(128, 128,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.down3 = ResBlockWithAttention(128, 128,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.down4 = Downsample(128)\n",
        "        self.down5 = ResBlockWithAttention(128, 128,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.down6 = ResBlockWithAttention(128, 128,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.down7 = Downsample(128)\n",
        "        self.down8 = ResBlockWithAttention(128, 256,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.down9 = ResBlockWithAttention(256, 256,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.down10 = Downsample(256)\n",
        "        self.down11 = ResBlockWithAttention(256, 256,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.down12 = ResBlockWithAttention(256, 256,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.down13 = Downsample(256)\n",
        "        self.down14 = ResBlockWithAttention(256, 512,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=True,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.down15 = ResBlockWithAttention(512, 512,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=True,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.down16 = Downsample(512)\n",
        "        self.down17 = ResBlockWithAttention(512, 512,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.down18 = ResBlockWithAttention(512, 512,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "\n",
        "        self.mid1 = ResBlockWithAttention(\n",
        "                    512,\n",
        "                    512,\n",
        "                    time_dim,\n",
        "                    dropout=dropout,\n",
        "                    use_attention=True,\n",
        "                    attention_head=attn_heads,\n",
        "                    use_affine_time=use_affine_time,\n",
        "                )\n",
        "        self.mid2 = ResBlockWithAttention(\n",
        "                    512,\n",
        "                    512,\n",
        "                    time_dim,\n",
        "                    dropout=dropout,\n",
        "                    use_affine_time=use_affine_time,\n",
        "                )\n",
        "\n",
        "        self.up1 = ResBlockWithAttention(1024, 512,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.up2 = ResBlockWithAttention(1024, 512,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.up3 = ResBlockWithAttention(1024, 512,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.up4 = Upsample(512)\n",
        "        self.up5 = ResBlockWithAttention(1024, 512,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=True,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.up6 = ResBlockWithAttention(1024, 512,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=True,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.up7 = ResBlockWithAttention(768, 512,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=True,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.up8 = Upsample(512)\n",
        "        self.up9 = ResBlockWithAttention(768, 256,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.up10 = ResBlockWithAttention(512, 256,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.up11 = ResBlockWithAttention(512, 256,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.up12 = Upsample(256)\n",
        "        self.up13 = ResBlockWithAttention(512, 256,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.up14 = ResBlockWithAttention(512, 256,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.up15 = ResBlockWithAttention(384, 256,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.up16 = Upsample(256)\n",
        "        self.up17 = ResBlockWithAttention(384, 128,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.up18 = ResBlockWithAttention(256, 128,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.up19 = ResBlockWithAttention(256, 128,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.up20 = Upsample(128)\n",
        "        self.up21 = ResBlockWithAttention(256, 128,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.up22 = ResBlockWithAttention(256, 128,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "        self.up23 = ResBlockWithAttention(256, 128,\n",
        "                time_dim,\n",
        "                dropout,\n",
        "                use_attention=False,\n",
        "                attention_head=attn_heads,\n",
        "                use_affine_time=use_affine_time,\n",
        "            )\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(32, 128),\n",
        "            Swish(),\n",
        "            conv2d(128, 3 , 3, padding=1, scale=1e-10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, time):\n",
        "        time_embed = self.time(time)\n",
        "\n",
        "        feats = []\n",
        "\n",
        "        x = self.down1(x)\n",
        "        feats.append(x)\n",
        "        x = self.down2(x, time_embed)\n",
        "        feats.append(x)\n",
        "        x = self.down3(x, time_embed)\n",
        "        feats.append(x)\n",
        "        x = self.down4(x)\n",
        "        feats.append(x)\n",
        "        x = self.down5(x, time_embed)\n",
        "        feats.append(x)\n",
        "        x = self.down6(x, time_embed)\n",
        "        feats.append(x)\n",
        "        x = self.down7(x)\n",
        "        feats.append(x)\n",
        "        x = self.down8(x, time_embed)\n",
        "        feats.append(x)\n",
        "        x = self.down9(x, time_embed)\n",
        "        feats.append(x)\n",
        "        x = self.down10(x)\n",
        "        feats.append(x)\n",
        "        x = self.down11(x, time_embed)\n",
        "        feats.append(x)\n",
        "        x = self.down12(x, time_embed)\n",
        "        feats.append(x)\n",
        "        x = self.down13(x)\n",
        "        feats.append(x)\n",
        "        x = self.down14(x, time_embed)\n",
        "        feats.append(x)\n",
        "        x = self.down15(x, time_embed)\n",
        "        feats.append(x)\n",
        "        x = self.down16(x)\n",
        "        feats.append(x)\n",
        "        x = self.down17(x, time_embed)\n",
        "        feats.append(x)\n",
        "        x = self.down18(x, time_embed)\n",
        "        feats.append(x)\n",
        "\n",
        "\n",
        "        x = self.mid1(x, time_embed)\n",
        "        x = self.mid2(x, time_embed)\n",
        "\n",
        "        x = self.up1(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "        x = self.up2(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "        x = self.up3(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "        x = self.up4(x)\n",
        "        x = self.up5(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "        x = self.up6(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "        x = self.up7(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "        x = self.up8(x)\n",
        "        x = self.up9(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "        x = self.up10(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "        x = self.up11(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "        x = self.up12(x)\n",
        "        x = self.up13(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "        x = self.up14(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "        x = self.up15(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "        x = self.up16(x)\n",
        "        x = self.up17(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "        x = self.up18(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "        x = self.up19(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "        x = self.up20(x)\n",
        "        x = self.up21(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "        x = self.up22(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "        x = self.up23(torch.cat((x, feats.pop()), 1), time_embed)\n",
        "\n",
        "        out = self.out(x)\n",
        "        out = spatial_unfold(out, 1)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### read noise path -> torch.tensor"
      ],
      "metadata": {
        "id": "yeo6wsGHCeiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def noise_read(noise_dir: str) -> torch.tensor:\n",
        "    noise_paths = sorted([os.path.join(noise_dir, i) for i in os.listdir(noise_dir) if i.endswith('.pt') ])\n",
        "\n",
        "    noise_imgs = []\n",
        "    for path in noise_paths:\n",
        "        noise_imgs.append(torch.load(path))\n",
        "    return torch.cat(noise_imgs)"
      ],
      "metadata": {
        "id": "cwW6YedXCeAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DDIM"
      ],
      "metadata": {
        "id": "Y7ISsR-6yhuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DDIM(nn.Module):\n",
        "    def __init__(self, eta=0, sample_steps=50, noise_steps=1000, beta_start=1e-4, beta_end=2e-2, img_size=256, device=device):\n",
        "        super().__init__()\n",
        "        self.eta = eta\n",
        "        self.smaple_steps = sample_steps\n",
        "        self.img_size = img_size\n",
        "\n",
        "        t_steps = torch.arange(0, noise_steps, (noise_steps // sample_steps)).long() + 1\n",
        "        t_steps = reversed(torch.cat((torch.tensor([0], dtype=torch.long), t_steps)))\n",
        "        self.t_steps = list(zip(t_steps[:-1], t_steps[1:]))\n",
        "\n",
        "        # beta_t = beta_start + (beta_end - beta_start) * torch.arange(0, noise_steps + 1, dtype=torch.long) / noise_steps\n",
        "        beta_t = torch.linspace(beta_start, beta_end, noise_steps, dtype=torch.float32)\n",
        "        alpha_t = 1 - beta_t\n",
        "        self.alpha_bar = torch.cumprod(alpha_t, dim=0).to(device)\n",
        "\n",
        "    def sample(self, noise_dir, net, device, n=10):\n",
        "        net.eval()\n",
        "        net.to(device)\n",
        "\n",
        "        x_store = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Input dim: torch.Size([n, 3, img_size, img_size])\n",
        "            # x = torch.randn((n, 3, self.img_size, self.img_size)).to(device)\n",
        "            x = noise_read(noise_dir).to(device)\n",
        "\n",
        "            for i, previous_i in tqdm(self.t_steps):\n",
        "                i, previous_i = i.to(device), previous_i.to(device)\n",
        "                # Time step, creating a tensor of size n\n",
        "                t = torch.ones(n, dtype=torch.long).to(device)\n",
        "                t *= i\n",
        "                # Previous time step, creating a tensor of size n\n",
        "                previous_t = torch.ones(n, dtype=torch.long).to(device)\n",
        "                previous_t *= previous_i\n",
        "                # Expand to a 4-dimensional tensor, and get the value according to the time step t\n",
        "                alpha_t = self.alpha_bar[t][:, None, None, None]\n",
        "                alpha_prev = self.alpha_bar[previous_t][:, None, None, None]\n",
        "\n",
        "                noise = torch.randn(n, *(3, self.img_size, self.img_size)).to(device, dtype=torch.float32) if i > 1 else 0\n",
        "\n",
        "                # Images and time steps input into the model\n",
        "                predicted_noise = net(x, t).to(device, dtype=torch.float32)\n",
        "\n",
        "                # x0_t = (x - (predicted_noise * torch.sqrt((1 - alpha_t)))) / torch.sqrt(alpha_t)\n",
        "                x0_t = torch.clamp((x - (predicted_noise * torch.sqrt((1 - alpha_t)))) / torch.sqrt(alpha_t), -1, 1)\n",
        "                c1 = self.eta * torch.sqrt((1 - alpha_t / alpha_prev) * (1 - alpha_prev) / (1 - alpha_t))\n",
        "                c2 = torch.sqrt((1 - alpha_prev) - c1 ** 2)\n",
        "                x = torch.sqrt(alpha_prev) * x0_t + c2 * predicted_noise + c1 * noise\n",
        "\n",
        "        # Return the value to the range of 0 and 1\n",
        "        # x = (x + 1) * 0.5\n",
        "        return x"
      ],
      "metadata": {
        "id": "PEQ90sv1yi0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = UNet()\n",
        "net.to(device)\n",
        "net.load_state_dict(torch.load('/content/hw2_data/face/UNet.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z59TLI5w-oDp",
        "outputId": "bd1aee96-84b6-4a0c-8820-198f6866a339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddim = DDIM(eta=0)\n",
        "ddim.eval()\n",
        "ddim.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yflEEoa4AzO4",
        "outputId": "19601d1e-ae49-46df-c814-1d97dc77bed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DDIM()"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = ddim.sample('/content/hw2_data/face/noise', net, device, n=10)\n",
        "\n",
        "for i in range(10):\n",
        "  save_image(x[i], f'/content/drive/MyDrive/NTU_DLCV/Hw2/p2_img/0{i}.png', normalize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWkj7bsQSGOt",
        "outputId": "8346ee85-326b-488d-8b40-8ff616243f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:53<00:00,  1.08s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate face images of noise 00.pt ~ 03.pt with different eta in one grid."
      ],
      "metadata": {
        "id": "kINRbhxZVM52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [0., 0.25, 0.5, 0.75, 1.]:\n",
        "    ddim = DDIM(eta=i)\n",
        "    ddim.eval()\n",
        "    ddim.to(device)\n",
        "\n",
        "    x = ddim.sample('/content/hw2_data/face/noise', net, device, n=10)\n",
        "    for j in range(10):\n",
        "        save_image(x[j], f'/content/drive/MyDrive/NTU_DLCV/Hw2/p2_img/eta{i}_0{j}.png', normalize=True)"
      ],
      "metadata": {
        "id": "n15Ma6EVVarA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check MSE value"
      ],
      "metadata": {
        "id": "9zzTdaqnVARB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "mse = 0\n",
        "\n",
        "for i in range(10):\n",
        "    gt = cv2.imread(f'/content/hw2_data/face/GT/0{i}.png').flatten()\n",
        "    pred = cv2.imread(f'/content/drive/MyDrive/NTU_DLCV/Hw2/p2_img/0{i}.png').flatten()\n",
        "    mse += mean_squared_error(gt, pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj58eAmsfrYK",
        "outputId": "78f493b2-f67f-4c7c-f0ad-aafc1acb9dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.62\n"
          ]
        }
      ]
    }
  ]
}