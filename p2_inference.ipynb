{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R12942159/NTU_DLCV/blob/Hw3/p2_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n",
        "!gdown 1SUiRrG6zQVtyrVSVh9hOBq5_fX-oV2Lh -O hw3_data.zip\n",
        "!unzip /content/hw3_data.zip\n",
        "!rm hw3_data.zip"
      ],
      "metadata": {
        "id": "qp-WCQ04LBbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Remove"
      ],
      "metadata": {
        "id": "dFNZRVfaR1ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import timm\n",
        "import json\n",
        "import torch\n",
        "import argparse\n",
        "import collections\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as tr\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms.functional import resize"
      ],
      "metadata": {
        "id": "BVhMTATlLcoD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### load model weights"
      ],
      "metadata": {
        "id": "T3ULd3iIh8Om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1-4CZb36TCfmtLJlxz06eoy-fknC0DcZb -O large_laion_weights\n",
        "!python -c \"import timm; timm.create_model('vit_large_patch14_224_clip_laion2b', pretrained=True)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_ENijHeh_7D",
        "outputId": "e4355ec1-af93-4f3e-e0b3-5d03e0736c9d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-4CZb36TCfmtLJlxz06eoy-fknC0DcZb\n",
            "To: /content/large_laion_weights\n",
            "100% 123M/123M [00:01<00:00, 84.0MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name vit_large_patch14_224_clip_laion2b to current vit_large_patch14_clip_224.laion2b.\n",
            "  model = create_fn(\n",
            "open_clip_pytorch_model.bin: 100% 1.71G/1.71G [00:12<00:00, 140MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument(\"path1\")\n",
        "# parser.add_argument(\"path2\")\n",
        "# parser.add_argument(\"path3\")\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# img_path = args.path1\n",
        "# output_json = args.path2\n",
        "# decoder_weights = args.path3\n",
        "encoder_joson_path = './encoder.json'\n",
        "vocab_bpe_path = './vocab.bpe'\n",
        "large_laion_weights = './large_laion_weights'\n",
        "\n",
        "img_path = '/content/hw3_data/p2_data/images/val'\n",
        "output_json = '/content/pred.json'\n",
        "decoder_weights = '/content/hw3_data/p2_data/decoder_model.bin'"
      ],
      "metadata": {
        "id": "ZE6YD1SbLLzQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Eg4VzCElm83",
        "outputId": "0eb23107-5fc9-400b-d275-2f9831572b07"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizer ('<|endoftext|>', 50256) -> 250dim"
      ],
      "metadata": {
        "id": "sIMyluP4OcRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BPETokenizer:\n",
        "\n",
        "    def __init__(self, encoder_file, vocab_file):\n",
        "        with open(encoder_file, 'r', encoding='utf-8') as f:\n",
        "            self.encoder = json.load(f)\n",
        "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
        "        with open(vocab_file, 'r', encoding='utf-8') as f:\n",
        "            vocab = f.read().split('\\n')[1:-1]\n",
        "        self.bpe_ranks = {tuple(line.split()): i for i, line in enumerate(vocab)}\n",
        "        assert len(self.encoder) == 50257 and len(self.bpe_ranks) == 50000\n",
        "        bs = list(range(33, 127)) + list(range(161, 256))\n",
        "        xs = list(range(0, 33)) + list(range(127, 161))\n",
        "        cs = bs[:] + [2**8 + i for i in range(len(xs))]\n",
        "        self.byte_encoder = dict(zip(bs + xs, [chr(n) for n in cs]))\n",
        "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
        "\n",
        "    def encode(self, text, allowed_special=None):\n",
        "        tokens = re.findall(r\"\"\"<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d| ?\"\"\" +\n",
        "                            r\"\"\"\\w+| ?\\d+| ?[^\\s\\w\\d]+|\\s+(?!\\S)|\\s+\"\"\", text, re.UNICODE)\n",
        "        def translate(token):\n",
        "            if token == '<|endoftext|>':\n",
        "                assert allowed_special and token in allowed_special\n",
        "                return [token]\n",
        "            word = tuple(''.join(self.byte_encoder[byte] for byte in token.encode('utf-8')))\n",
        "            while len(word) != 1:\n",
        "                pairs = set((word[i], word[i+1]) for i in range(len(word)-1))\n",
        "                bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "                if bigram not in self.bpe_ranks:\n",
        "                    break\n",
        "                a, b = bigram\n",
        "                new_word = []\n",
        "                i = 0\n",
        "                while i < len(word):\n",
        "                    j = word.index(a, i) if a in word[i:] else len(word)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                    if i < len(word):\n",
        "                        j = 2 if i < len(word)-1 and word[i] == a and word[i+1] == b else 1\n",
        "                        new_word.append(a+b if j == 2 else word[i])\n",
        "                        i += j\n",
        "                word = tuple(new_word)\n",
        "            return word\n",
        "        return [self.encoder[_] for token in tokens for _ in translate(token)]\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        tokens = [self.decoder[token] for token in tokens]\n",
        "        buffer = bytearray([self.byte_decoder[c] for c in ''.join(tokens)])\n",
        "        return buffer.decode('utf-8', errors='replace')"
      ],
      "metadata": {
        "id": "gjc6poP1OdBM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = BPETokenizer(encoder_joson_path, vocab_bpe_path)"
      ],
      "metadata": {
        "id": "6aeCuLyJOmhP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define function"
      ],
      "metadata": {
        "id": "L0JYrzNqE0mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def json_load(json_path: str):\n",
        "    with open(json_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "    return data"
      ],
      "metadata": {
        "id": "z5xRsOP3h96o"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def caption_with_id(json_path: str) -> list:\n",
        "    with open(json_path, 'r', encoding='utf-8') as file:\n",
        "        json_data = json.load(file)\n",
        "    data = [{'caption': row['caption'], 'image_id': row['image_id']} for row in json_data['annotations']]\n",
        "    return data"
      ],
      "metadata": {
        "id": "vVR-onWwmbCI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def id2file_name(json_path: str) -> dict:\n",
        "    with open(json_path, 'r', encoding='utf-8') as file:\n",
        "        json_data = json.load(file)\n",
        "    data = {row['id']: row['file_name'] for row in json_data['images']}\n",
        "    return data"
      ],
      "metadata": {
        "id": "rWW6y2Q1o_t-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build Dataset"
      ],
      "metadata": {
        "id": "7TWHXZgugWOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImgDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root: str, transform) -> None:\n",
        "        self.transform = transform\n",
        "        self.img_path = [i for i in Path(root).glob(\"*.jpg\")]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.img_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.img_path[idx]).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        return img, os.path.splitext(self.img_path[idx].name)[0]"
      ],
      "metadata": {
        "id": "lYoLLeI3gZvS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build Dataloader"
      ],
      "metadata": {
        "id": "vMk7sUiwHEPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_ds = ImgDataset(\n",
        "    root=img_path,\n",
        "    transform=tr.Compose([\n",
        "        tr.Resize(224),\n",
        "        tr.CenterCrop(224),\n",
        "        tr.ToTensor(),\n",
        "        tr.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        ")\n",
        "\n",
        "img_loader = DataLoader(\n",
        "    img_ds,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        ")"
      ],
      "metadata": {
        "id": "MMGD07vrHGiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3009a8de-9875-4b6e-d4dc-a9e282ed8947"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Config"
      ],
      "metadata": {
        "id": "Fx9an6CwEx-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "\n",
        "    def __init__(self, checkpoint=None):\n",
        "        self.n_layer = 12\n",
        "        self.n_head = 12\n",
        "        self.n_embd = 768\n",
        "        self.vocab_size = 50257\n",
        "        self.block_size = 1024\n",
        "        self.checkpoint = checkpoint"
      ],
      "metadata": {
        "id": "x39qAcvyEzgi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = Config(checkpoint=decoder_weights)"
      ],
      "metadata": {
        "id": "lLwivdnDE5eq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### decoder"
      ],
      "metadata": {
        "id": "w1QQ3sdc8GRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.c_attn = nn.Linear(cfg.n_embd, 3 * cfg.n_embd)\n",
        "        self.c_proj = nn.Linear(cfg.n_embd, cfg.n_embd)\n",
        "        self.n_head = cfg.n_head\n",
        "        self.n_embd = cfg.n_embd\n",
        "        size = cfg.block_size\n",
        "        self.register_buffer('bias', torch.tril(torch.ones(size, size)).view(1, 1, size, size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch, context, embedding\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        return self.c_proj((att @ v).transpose(1, 2).contiguous().view(B, T, C))\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.multihead_attn = nn.MultiheadAttention(cfg.n_embd, cfg.n_head, batch_first=True)\n",
        "\n",
        "    def forward(self, query, encoder_out):\n",
        "        \"\"\"\n",
        "        Q is the source from the decoder, K, V are the sources from the encoder.\n",
        "        Q: (N, L, Eq), where L is the target embedding dim, Eq is embed_dim and batch_first=True.\n",
        "        {K, V}: (N, L, E{k,v}), where L is the source embedding dim, E{k,v} is {k,v}_dim and batch_first=True.\n",
        "        \"\"\"\n",
        "        attn_output, weights = self.multihead_attn(query, encoder_out, encoder_out)\n",
        "        return attn_output\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(cfg.n_embd)\n",
        "        self.ln_2 = nn.LayerNorm(cfg.n_embd)\n",
        "        self.ln_3 = nn.LayerNorm(cfg.n_embd) # add\n",
        "        self.attn = Attention(cfg)\n",
        "        self.crs_attn = CrossAttention(cfg) # add\n",
        "        self.mlp = nn.Sequential(collections.OrderedDict([\n",
        "            ('c_fc', nn.Linear(cfg.n_embd, 4 * cfg.n_embd)),\n",
        "            ('act', nn.GELU(approximate='tanh')),\n",
        "            ('c_proj', nn.Linear(4 * cfg.n_embd, cfg.n_embd))\n",
        "        ]))\n",
        "\n",
        "    def forward(self, x, encoder_out) -> Tensor: # add\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.crs_attn(self.ln_3(x), encoder_out) # add\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.block_size = cfg.block_size\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(cfg.vocab_size, cfg.n_embd), # 文字投影\n",
        "            wpe = nn.Embedding(cfg.block_size, cfg.n_embd), # position\n",
        "            h = nn.Sequential(*[Block(cfg) for _ in range(cfg.n_layer)]), # Nx\n",
        "            ln_f = nn.LayerNorm(cfg.n_embd)\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(cfg.n_embd, cfg.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        # timm's ViT encoder\n",
        "        self.encoder = timm.create_model('vit_large_patch14_224_clip_laion2b', pretrained=True)\n",
        "        self.linear = nn.Linear(1024, cfg.n_embd) # [16, 197, 1024]\n",
        "        # load checkpoint\n",
        "        if self.cfg.checkpoint is not None:\n",
        "            state_dict = torch.load(self.cfg.checkpoint)\n",
        "            transposed = [ '.c_attn.weight', '.c_fc.weight', '.c_proj.weight' ]\n",
        "            for key, value in state_dict.items():\n",
        "                if any(key.endswith(w) for w in transposed):\n",
        "                    state_dict[key] = value.t()\n",
        "            self.transformer.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    def forward(self, x: Tensor, img: Tensor) -> Tensor: # add\n",
        "        x = torch.narrow(x, 1, 0, min(x.size(1), self.block_size))\n",
        "        pos = torch.arange(x.size()[1], dtype=torch.long, device=x.device).unsqueeze(0)\n",
        "        x = self.transformer.wte(x) + self.transformer.wpe(pos)\n",
        "        # encoder veatures\n",
        "        encoder_out = self.encoder.forward_features(img)\n",
        "        encoder_out = self.linear(encoder_out)\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x, encoder_out)\n",
        "        x = self.lm_head(self.transformer.ln_f(x)) # add\n",
        "        return x"
      ],
      "metadata": {
        "id": "TYyI157L8JB7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualization of Attention in Image Captioning"
      ],
      "metadata": {
        "id": "K4qEtiUZcjqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Decoder(cfg).to(device)\n",
        "model.load_state_dict(torch.load(large_laion_weights, map_location=device), strict=False)"
      ],
      "metadata": {
        "id": "2e0KXMqQhCPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_dict = {}\n",
        "for img, file_name in img_loader:\n",
        "\n",
        "    img = img.to(device)\n",
        "    start_token = torch.tensor([[50256]]).to(device)\n",
        "\n",
        "    for i in range(250):\n",
        "        with torch.no_grad():\n",
        "            pred = model(start_token, img)\n",
        "        out_token = pred.argmax(dim=2)[0][-1]\n",
        "        start_token = torch.cat((start_token, out_token.unsqueeze(0).unsqueeze(0)), dim=1)\n",
        "        end_token = torch.sum(start_token[0] == 50256).item()\n",
        "        if end_token == 2:\n",
        "            pred_token = start_token[start_token != 50256]\n",
        "            pred_token = pred_token.tolist()\n",
        "            pred_caption = encoding.decode(pred_token)\n",
        "            break\n",
        "    evaluation_dict[file_name[0]] = pred_caption\n",
        "    print('\\n', 'file name: ', file_name[0], '\\caption: ', evaluation_dict[file_name[0]])"
      ],
      "metadata": {
        "id": "UNewXUlkhKcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_string = json.dumps(evaluation_dict, indent=2)\n",
        "with open(output_json, 'w') as json_file:\n",
        "    json_file.write(json_string)"
      ],
      "metadata": {
        "id": "JM0VW9s3Mt8V"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}