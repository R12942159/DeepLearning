{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6985a43bf114454599404514dcb47d85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a56a4a363094925ab4495fe117e91d5",
              "IPY_MODEL_2e241cefaaf145e3b25a9456a39f3677",
              "IPY_MODEL_7986484aed364b90a2f8d15a38b63785"
            ],
            "layout": "IPY_MODEL_2f46ee9b46f945b985b485de014ca1d6"
          }
        },
        "6a56a4a363094925ab4495fe117e91d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e777cbfead14875bdc95ebc9e464ff0",
            "placeholder": "​",
            "style": "IPY_MODEL_bb1d6c7b3f8840f6946e5abdbe211043",
            "value": "model.safetensors: 100%"
          }
        },
        "2e241cefaaf145e3b25a9456a39f3677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a279b49b5a5e461a9539b64cfcfcc2ee",
            "max": 1302790986,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4bc47ce16dc4462683562c7098c64356",
            "value": 1302790986
          }
        },
        "7986484aed364b90a2f8d15a38b63785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97f9b18df3ec464eaf637c04bd028680",
            "placeholder": "​",
            "style": "IPY_MODEL_c1d69c80bf2f414cae9f4c5a07807450",
            "value": " 1.30G/1.30G [00:26&lt;00:00, 49.0MB/s]"
          }
        },
        "2f46ee9b46f945b985b485de014ca1d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e777cbfead14875bdc95ebc9e464ff0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb1d6c7b3f8840f6946e5abdbe211043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a279b49b5a5e461a9539b64cfcfcc2ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bc47ce16dc4462683562c7098c64356": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "97f9b18df3ec464eaf637c04bd028680": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1d69c80bf2f414cae9f4c5a07807450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R12942159/NTU_DLCV/blob/Hw3/p2_Image_caption_large_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n",
        "!pip install loralib"
      ],
      "metadata": {
        "id": "Xb59e3Ej2GPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import timm\n",
        "import json\n",
        "import torch\n",
        "import collections\n",
        "import numpy as np\n",
        "import loralib as lora\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as tr\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "SBi1aSqPhMuD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Eg4VzCElm83",
        "outputId": "6b711c17-0c3a-4250-8a5b-54ae91b601a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UVlQEx1nX6xD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "579147c2-cf71-4e1e-d21b-7f4622d91971"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download dataset and unzip zip file."
      ],
      "metadata": {
        "id": "Q22ZX1_VhKiK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DG6GOxEHD9l"
      },
      "outputs": [],
      "source": [
        "!gdown 1SUiRrG6zQVtyrVSVh9hOBq5_fX-oV2Lh -O hw3_data.zip # 11rP6KmR5Qwjhx0rfag0b5TZGBTRuPtQR\n",
        "!unzip /content/hw3_data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizer ('<|endoftext|>', 50256) -> 250dim"
      ],
      "metadata": {
        "id": "sIMyluP4OcRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BPETokenizer:\n",
        "\n",
        "    def __init__(self, encoder_file, vocab_file):\n",
        "        with open(encoder_file, 'r', encoding='utf-8') as f:\n",
        "            self.encoder = json.load(f)\n",
        "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
        "        with open(vocab_file, 'r', encoding='utf-8') as f:\n",
        "            vocab = f.read().split('\\n')[1:-1]\n",
        "        self.bpe_ranks = {tuple(line.split()): i for i, line in enumerate(vocab)}\n",
        "        assert len(self.encoder) == 50257 and len(self.bpe_ranks) == 49999 # len(self.bpe_ranks) == 50000\n",
        "        bs = list(range(33, 127)) + list(range(161, 256))\n",
        "        xs = list(range(0, 33)) + list(range(127, 161))\n",
        "        cs = bs[:] + [2**8 + i for i in range(len(xs))]\n",
        "        self.byte_encoder = dict(zip(bs + xs, [chr(n) for n in cs]))\n",
        "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
        "\n",
        "    def encode(self, text, allowed_special=None):\n",
        "        tokens = re.findall(r\"\"\"<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d| ?\"\"\" +\n",
        "                            r\"\"\"\\w+| ?\\d+| ?[^\\s\\w\\d]+|\\s+(?!\\S)|\\s+\"\"\", text, re.UNICODE)\n",
        "        def translate(token):\n",
        "            if token == '<|endoftext|>':\n",
        "                assert allowed_special and token in allowed_special\n",
        "                return [token]\n",
        "            word = tuple(''.join(self.byte_encoder[byte] for byte in token.encode('utf-8')))\n",
        "            while len(word) != 1:\n",
        "                pairs = set((word[i], word[i+1]) for i in range(len(word)-1))\n",
        "                bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "                if bigram not in self.bpe_ranks:\n",
        "                    break\n",
        "                a, b = bigram\n",
        "                new_word = []\n",
        "                i = 0\n",
        "                while i < len(word):\n",
        "                    j = word.index(a, i) if a in word[i:] else len(word)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                    if i < len(word):\n",
        "                        j = 2 if i < len(word)-1 and word[i] == a and word[i+1] == b else 1\n",
        "                        new_word.append(a+b if j == 2 else word[i])\n",
        "                        i += j\n",
        "                word = tuple(new_word)\n",
        "            return word\n",
        "        return [self.encoder[_] for token in tokens for _ in translate(token)]\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        tokens = [self.decoder[token] for token in tokens]\n",
        "        buffer = bytearray([self.byte_decoder[c] for c in ''.join(tokens)])\n",
        "        return buffer.decode('utf-8', errors='replace')"
      ],
      "metadata": {
        "id": "gjc6poP1OdBM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = BPETokenizer('/content/encoder.json', '/content/vocab.bpe')\n",
        "# prompt = 'a kitchen with a sink and many cooking machines and a pot of food'\n",
        "\n",
        "# text_embedding_len = 250\n",
        "\n",
        "# context = encoding.encode(prompt)\n",
        "# context = [50256] + context + [50256]*(text_embedding_len - len(context) - 1)\n",
        "# # context\n",
        "# encoding.decode(context)"
      ],
      "metadata": {
        "id": "6aeCuLyJOmhP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define function"
      ],
      "metadata": {
        "id": "L0JYrzNqE0mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def json_load(json_path: str):\n",
        "    with open(json_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "    return data"
      ],
      "metadata": {
        "id": "z5xRsOP3h96o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def caption_with_id(json_path: str) -> list:\n",
        "    with open(json_path, 'r', encoding='utf-8') as file:\n",
        "        json_data = json.load(file)\n",
        "    data = [{'caption': row['caption'], 'image_id': row['image_id']} for row in json_data['annotations']]\n",
        "    return data"
      ],
      "metadata": {
        "id": "vVR-onWwmbCI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def id2file_name(json_path: str) -> dict:\n",
        "    with open(json_path, 'r', encoding='utf-8') as file:\n",
        "        json_data = json.load(file)\n",
        "    data = {row['id']: row['file_name'] for row in json_data['images']}\n",
        "    return data"
      ],
      "metadata": {
        "id": "rWW6y2Q1o_t-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_joson_path = '/content/encoder.json'\n",
        "vocab_bpe_path = '/content/vocab.bpe'\n",
        "def collate_fn(batch, tokenizer=BPETokenizer(encoder_joson_path, vocab_bpe_path)):\n",
        "    # Get the individual elements of the batch\n",
        "    images = [item['img'] for item in batch]\n",
        "    captions = [item['caption'] for item in batch]\n",
        "    filenames = [item['filename'] for item in batch]\n",
        "\n",
        "    # Tokenize captions\n",
        "    tokenized_captions = [tokenizer.encode(caption) for caption in captions]\n",
        "\n",
        "    # Pad the vector length into stop token to dimension 250\n",
        "    text_len = 250 # text_embedding_len\n",
        "    tokenized_captions_train = [\n",
        "        [50256] + caption + [50256] * (text_len - len(caption) - 1) for caption in tokenized_captions\n",
        "    ]\n",
        "    tokenized_captions_inf = [\n",
        "        caption + [50256] + [-100] * (text_len - len(caption) - 1) for caption in tokenized_captions\n",
        "    ]\n",
        "\n",
        "    # Convert tokenized captions to PyTorch tensors\n",
        "    tokenized_captions_train = [torch.tensor(caption) for caption in tokenized_captions_train]\n",
        "    tokenized_captions_inf = [torch.tensor(caption) for caption in tokenized_captions_inf]\n",
        "\n",
        "    # Create a new batch with tokenized captions\n",
        "    tokenized_batch = {\n",
        "        'img': torch.stack(images, dim=0),\n",
        "        'tokenized_captions_train': torch.stack(tokenized_captions_train, dim=0),\n",
        "        'filename': filenames,\n",
        "        'tokenized_captions_inf': torch.stack(tokenized_captions_inf, dim=0),\n",
        "    }\n",
        "\n",
        "    return tokenized_batch"
      ],
      "metadata": {
        "id": "ZhTVXCheOYm6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build Dataset"
      ],
      "metadata": {
        "id": "7TWHXZgugWOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImgCaptionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir, json_path, transform) -> None:\n",
        "        super(ImgCaptionDataset, self).__init__()\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Connect caption -> image_id -> file_name\n",
        "        self.caption_with_id = caption_with_id(json_path)\n",
        "        self.id2file_name = id2file_name(json_path)\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.caption_with_id)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        caption_id = self.caption_with_id[idx]\n",
        "        file_name = self.id2file_name[caption_id['image_id']]\n",
        "        img = Image.open(os.path.join(self.img_dir, file_name)).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        return {'img': img, 'caption': caption_id['caption'], 'filename': os.path.splitext(file_name)[0]}"
      ],
      "metadata": {
        "id": "qJq6lmbBlbaM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build Dataloader"
      ],
      "metadata": {
        "id": "vMk7sUiwHEPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = ImgCaptionDataset(\n",
        "    img_dir='/content/hw3_data/p2_data/images/train',\n",
        "    json_path='/content/hw3_data/p2_data/train.json',\n",
        "    transform=tr.Compose([\n",
        "        tr.Resize(224),\n",
        "        tr.CenterCrop(224),\n",
        "        tr.ToTensor(),\n",
        "        tr.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        ")\n",
        "val_ds = ImgCaptionDataset(\n",
        "    img_dir='/content/hw3_data/p2_data/images/val',\n",
        "    json_path='/content/hw3_data/p2_data/val.json',\n",
        "    transform=tr.Compose([\n",
        "        tr.Resize(224),\n",
        "        tr.CenterCrop(224),\n",
        "        tr.ToTensor(),\n",
        "        tr.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=1,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        ")"
      ],
      "metadata": {
        "id": "MMGD07vrHGiB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Config"
      ],
      "metadata": {
        "id": "Fx9an6CwEx-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "\n",
        "    def __init__(self, checkpoint=None):\n",
        "        self.n_layer = 12\n",
        "        self.n_head = 12\n",
        "        self.n_embd = 768\n",
        "        self.vocab_size = 50257\n",
        "        self.block_size = 1024\n",
        "        self.checkpoint = checkpoint"
      ],
      "metadata": {
        "id": "x39qAcvyEzgi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = Config(checkpoint='/content/hw3_data/p2_data/decoder_model.bin')"
      ],
      "metadata": {
        "id": "lLwivdnDE5eq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### decoder"
      ],
      "metadata": {
        "id": "w1QQ3sdc8GRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.c_attn = lora.Linear(cfg.n_embd, 3 * cfg.n_embd, r=4)\n",
        "        self.c_proj = nn.Linear(cfg.n_embd, cfg.n_embd)\n",
        "        self.n_head = cfg.n_head\n",
        "        self.n_embd = cfg.n_embd\n",
        "        size = cfg.block_size\n",
        "        self.register_buffer('bias', torch.tril(torch.ones(size, size)).view(1, 1, size, size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch, context, embedding\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        return self.c_proj((att @ v).transpose(1, 2).contiguous().view(B, T, C))\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.multihead_attn = nn.MultiheadAttention(cfg.n_embd, cfg.n_head, batch_first=True)\n",
        "\n",
        "    def forward(self, query, encoder_out):\n",
        "        \"\"\"\n",
        "        Q is the source from the decoder, K, V are the sources from the encoder.\n",
        "        Q: (N, L, Eq), where L is the target embedding dim, Eq is embed_dim and batch_first=True.\n",
        "        {K, V}: (N, L, E{k,v}), where L is the source embedding dim, E{k,v} is {k,v}_dim and batch_first=True.\n",
        "        \"\"\"\n",
        "        attn_output, attn_output_weights = self.multihead_attn(query, encoder_out, encoder_out)\n",
        "        return attn_output #, attn_output_weights\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(cfg.n_embd)\n",
        "        self.ln_2 = nn.LayerNorm(cfg.n_embd) # add\n",
        "        self.ln_3 = nn.LayerNorm(cfg.n_embd)\n",
        "        self.attn = Attention(cfg)\n",
        "        self.crs_attn = CrossAttention(cfg) # add\n",
        "        self.mlp = nn.Sequential(collections.OrderedDict([\n",
        "            ('c_fc', nn.Linear(cfg.n_embd, 4 * cfg.n_embd)),\n",
        "            ('act', nn.GELU(approximate='tanh')),\n",
        "            ('c_proj', nn.Linear(4 * cfg.n_embd, cfg.n_embd))\n",
        "        ]))\n",
        "\n",
        "    def forward(self, x, encoder_out) -> Tensor: # add\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        cross_x = self.crs_attn(self.ln_2(x), self.ln_2(encoder_out)) # add #, weights\n",
        "        x = cross_x + x\n",
        "        x = x + self.mlp(self.ln_3(x))\n",
        "        return x #, weights\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.block_size = cfg.block_size\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(cfg.vocab_size, cfg.n_embd), # 文字投影\n",
        "            wpe = nn.Embedding(cfg.block_size, cfg.n_embd), # position\n",
        "            h = nn.Sequential(*[Block(cfg) for _ in range(cfg.n_layer)]), # Nx\n",
        "            ln_f = nn.LayerNorm(cfg.n_embd)\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(cfg.n_embd, cfg.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        # timm's ViT encoder (vit_base_patch16_224_in21k, , vit_huge_patch14_224_in21k)\n",
        "        self.encoder = timm.create_model('vit_large_patch16_224_in21k', pretrained=True)\n",
        "        self.linear = nn.Linear(1024, cfg.n_embd) # [16, 197, 1024]\n",
        "        # load checkpoint\n",
        "        if self.cfg.checkpoint is not None:\n",
        "            state_dict = torch.load(self.cfg.checkpoint)\n",
        "            transposed = [ '.c_attn.weight', '.c_fc.weight', '.c_proj.weight' ]\n",
        "            for key, value in state_dict.items():\n",
        "                if any(key.endswith(w) for w in transposed):\n",
        "                    state_dict[key] = value.t()\n",
        "            self.transformer.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    def forward(self, x: Tensor, img: Tensor) -> Tensor: # add\n",
        "        x = torch.narrow(x, 1, 0, min(x.size(1), self.block_size))\n",
        "        pos = torch.arange(x.size()[1], dtype=torch.long, device=x.device).unsqueeze(0)\n",
        "        x = self.transformer.wte(x) + self.transformer.wpe(pos)\n",
        "        with torch.no_grad():\n",
        "            encoder_out = self.encoder.forward_features(img)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x, self.linear(encoder_out)) #, weights\n",
        "        x = self.lm_head(self.transformer.ln_f(x)) # add\n",
        "        return x #, weights"
      ],
      "metadata": {
        "id": "TYyI157L8JB7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoding test"
      ],
      "metadata": {
        "id": "Nymq6RjQw0Ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder = Decoder(cfg)"
      ],
      "metadata": {
        "id": "W7ItaTQyFM3y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch in train_loader:\n",
        "#     img = batch['img']\n",
        "#     tokenized_captions_train = batch['tokenized_captions_train']\n",
        "#     tokenized_captions_inf = batch['tokenized_captions_inf']\n",
        "#     break"
      ],
      "metadata": {
        "id": "8iZRZyyLVqTa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pred = decoder(tokenized_captions_train, img)"
      ],
      "metadata": {
        "id": "C9WhUVX3XUNA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pred.size(), tokenized_captions_train.size(), img.size(), tokenized_captions_inf.size()"
      ],
      "metadata": {
        "id": "-VDTQFh1jIkn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding.decode(pred[0][0].argmax(dim=1).tolist())"
      ],
      "metadata": {
        "id": "9rVBzjCBAMC6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_fn = nn.CrossEntropyLoss() # ignore_index=50256\n",
        "\n",
        "# pred = pred[0].reshape(-1, 50257)\n",
        "# tokenized_captions_inf = tokenized_captions_inf.reshape(-1)\n",
        "# loss_fn(pred, tokenized_captions_inf)"
      ],
      "metadata": {
        "id": "mZVS0SXWk7Qh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Freeze parameters"
      ],
      "metadata": {
        "id": "Ngfxfa2fR8en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Decoder(cfg).to(device)\n",
        "\n",
        "# # Freeze parameters\n",
        "# for name, param in model.named_parameters():\n",
        "#     param.requires_grad=False\n",
        "\n",
        "# # Unfreeze some parameters\n",
        "# for i in range(12):\n",
        "#     model.transformer.h[i].ln_2.weight.requires_grad = True\n",
        "#     model.transformer.h[i].ln_2.bias.requires_grad = True\n",
        "#     model.transformer.h[i].attn.c_attn.lora_A.requires_grad = True\n",
        "#     model.transformer.h[i].attn.c_attn.lora_B.requires_grad = True\n",
        "#     model.transformer.h[i].crs_attn.multihead_attn.in_proj_weight.requires_grad = True\n",
        "#     model.transformer.h[i].crs_attn.multihead_attn.in_proj_bias.requires_grad = True\n",
        "#     model.transformer.h[i].crs_attn.multihead_attn.out_proj.weight.requires_grad = True\n",
        "#     model.transformer.h[i].crs_attn.multihead_attn.out_proj.bias.requires_grad = True\n",
        "# model.linear.weight.requires_grad = True\n",
        "# model.linear.bias.requires_grad = True\n",
        "\n",
        "# trainable_weights = [name for name, param in model.named_parameters() if param.requires_grad == True]\n",
        "# # list for True\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(f\"{name}: {param.requires_grad}\")"
      ],
      "metadata": {
        "id": "AqVX0_aSSAKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "rFEjEGPIj285"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def training(dataloader, model, loss_fn, optimizer):\n",
        "\n",
        "#     size = len(dataloader.dataset) # number of samples\n",
        "#     num_batches = len(dataloader) # batches per epoch\n",
        "#     epoch_loss = 0\n",
        "\n",
        "#     model.train() # to training mode\n",
        "#     for batch_i, data in enumerate(tqdm(dataloader)):\n",
        "#         data['img'] = data['img'].to(device, non_blocking=True)\n",
        "#         data['tokenized_captions_train'] = data['tokenized_captions_train'].to(device, non_blocking=True)\n",
        "#         data['tokenized_captions_inf'] = data['tokenized_captions_inf'].to(device, non_blocking=True)\n",
        "\n",
        "#         # zero the parameter gradients\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Compute prediction loss\n",
        "#         pred = model(data['tokenized_captions_train'], data['img'])\n",
        "#         # reshape to (B, C)\n",
        "#         data['tokenized_captions_inf'] = data['tokenized_captions_inf'].reshape(-1)\n",
        "#         pred = pred.reshape(-1, 50257)\n",
        "#         loss = loss_fn(pred, data['tokenized_captions_inf']) # tokenized captions inf\n",
        "\n",
        "#         # Optimization by gradients\n",
        "#         loss.backward() # backpropagation to compute gradients\n",
        "#         optimizer.step() # update model params\n",
        "\n",
        "#         # write to logs\n",
        "#         epoch_loss += loss.item() # tensor -> python value\n",
        "#     return epoch_loss/num_batches"
      ],
      "metadata": {
        "id": "xH8HOqOGqM47"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EPOCHS = 8\n",
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "# # logs\n",
        "# logs = {\n",
        "#     'train_loss': []\n",
        "# }\n",
        "\n",
        "# for epoch in tqdm(range(EPOCHS)):\n",
        "#     train_loss = training(train_loader, model, loss_fn, optimizer)\n",
        "\n",
        "#     print(f'EPOCH: {epoch:04d} \\train_loss: {train_loss:.4f}')\n",
        "\n",
        "#     logs['train_loss'].append(train_loss)\n",
        "\n",
        "#     # Save model\n",
        "#     save_weights = {k: v for k, v in model.state_dict().items() if k in trainable_weights}\n",
        "#     torch.save(save_weights, f'/content/drive/MyDrive/NTU_DLCV/Hw3/p2_ckpt_large_LoRA/trainable_weights_LoRA_epoch{epoch}_{train_loss:.4f}.pth')\n",
        "#     print('---------- Model Save ----------')"
      ],
      "metadata": {
        "id": "DEguocg8j4qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check the model params less than 35M"
      ],
      "metadata": {
        "id": "KvaNuVScnJLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1-A5b7or2nZfLUXxKJ66q9QdupiZLRfvf -O LoRA_trainable_weights"
      ],
      "metadata": {
        "id": "5KB_Sr0znbHb",
        "outputId": "61e5606d-8ccc-4c69-a733-fddf43e50b44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-A5b7or2nZfLUXxKJ66q9QdupiZLRfvf\n",
            "To: /content/LoRA_trainable_weights\n",
            "100% 117M/117M [00:02<00:00, 41.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Decoder(cfg)\n",
        "# Freeze parameters\n",
        "for name, param in model.named_parameters():\n",
        "    param.requires_grad=False\n",
        "    # print(f\"{name}: {param.requires_grad}\")\n",
        "# Unfreeze some parameters\n",
        "for i in range(12):\n",
        "    model.transformer.h[i].ln_2.weight.requires_grad = True\n",
        "    model.transformer.h[i].ln_2.bias.requires_grad = True\n",
        "    model.transformer.h[i].attn.c_attn.lora_A.requires_grad = True\n",
        "    model.transformer.h[i].attn.c_attn.lora_B.requires_grad = True\n",
        "    model.transformer.h[i].crs_attn.multihead_attn.in_proj_weight.requires_grad = True\n",
        "    model.transformer.h[i].crs_attn.multihead_attn.in_proj_bias.requires_grad = True\n",
        "    model.transformer.h[i].crs_attn.multihead_attn.out_proj.weight.requires_grad = True\n",
        "    model.transformer.h[i].crs_attn.multihead_attn.out_proj.bias.requires_grad = True\n",
        "model.linear.weight.requires_grad = True\n",
        "model.linear.bias.requires_grad = True"
      ],
      "metadata": {
        "id": "yews3unVnUje",
        "outputId": "4b07f265-0a1e-4a06-ac5b-9811795f5213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6985a43bf114454599404514dcb47d85",
            "6a56a4a363094925ab4495fe117e91d5",
            "2e241cefaaf145e3b25a9456a39f3677",
            "7986484aed364b90a2f8d15a38b63785",
            "2f46ee9b46f945b985b485de014ca1d6",
            "9e777cbfead14875bdc95ebc9e464ff0",
            "bb1d6c7b3f8840f6946e5abdbe211043",
            "a279b49b5a5e461a9539b64cfcfcc2ee",
            "4bc47ce16dc4462683562c7098c64356",
            "97f9b18df3ec464eaf637c04bd028680",
            "c1d69c80bf2f414cae9f4c5a07807450"
          ]
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.30G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6985a43bf114454599404514dcb47d85"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/LoRA_trainable_weights', map_location=device), strict=False)\n",
        "print('Total params: ', sum(params.numel() for params in model.parameters() if params.requires_grad))"
      ],
      "metadata": {
        "id": "rXypa7_EnXQM",
        "outputId": "b7bae70c-b6da-4bec-854f-d0433115d984",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params:  29301504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### inference"
      ],
      "metadata": {
        "id": "K4qEtiUZcjqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1-A5b7or2nZfLUXxKJ66q9QdupiZLRfvf -O trainable_weights4 # epoch4\n",
        "!gdown 1-7X5RmB1vBgglTG4Hnzt7Xrk5z6NSQCo -O trainable_weights3 # epoch3\n",
        "!gdown 1-2aI-6St6eOompVzT5OrIcbELkgTSSIl -O trainable_weights2 # epoch2\n",
        "!gdown 1-1siArvAkGcXUke88C4X9WOcw6QboFlZ -O trainable_weights1 # epoch1"
      ],
      "metadata": {
        "id": "DqSfWMRZcvbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4, 0, -1):\n",
        "    model = Decoder(cfg).to(device)\n",
        "    model.load_state_dict(torch.load(f'/content/trainable_weights{i}', map_location=device), strict=False)\n",
        "    print(f'---------- trainable weights {i} is using ----------')\n",
        "\n",
        "    evaluation_dict = {}\n",
        "    for data in tqdm(val_loader):\n",
        "        img = data['img'].to(device)\n",
        "        file_name = data['filename']\n",
        "        start_token = torch.tensor([[50256]]).to(device)\n",
        "\n",
        "        for j in range(250):\n",
        "            with torch.no_grad():\n",
        "                pred = model(start_token, img)\n",
        "                # print(weights.size())\n",
        "\n",
        "            out_token = pred.argmax(dim=2)[0][-1]\n",
        "            start_token = torch.cat((start_token, out_token.unsqueeze(0).unsqueeze(0)), dim=1)\n",
        "            end_token = torch.sum(start_token[0] == 50256).item()\n",
        "            if end_token == 2:\n",
        "                pred_token = start_token[start_token != 50256]\n",
        "                pred_token = pred_token.tolist()\n",
        "                pred_caption = encoding.decode(pred_token)\n",
        "                break\n",
        "\n",
        "        evaluation_dict[file_name[0]] = pred_caption\n",
        "        print('\\n', 'file name: ', file_name[0], '\\caption: ', evaluation_dict[file_name[0]])\n",
        "\n",
        "    json_string = json.dumps(evaluation_dict, indent=2)  # The indent parameter is optional and adds indentation for better readability\n",
        "    with open(f'/content/drive/MyDrive/NTU_DLCV/Hw3/p2_ckpt_large_LoRA/LoRA_large_epoch{i}_output.json', 'w') as json_file:\n",
        "        json_file.write(json_string)\n",
        "    print(f'---------- Epoch{i} large params Saved ----------')\n",
        "    with open('output.json', 'w') as json_file:\n",
        "        json_file.write(json_string)"
      ],
      "metadata": {
        "id": "GnNNQ6q4GO9e",
        "outputId": "9980b11d-a450-4396-b88e-1de495cbe84f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- trainable weights 4 is using ----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/8946 [00:00<2:23:23,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000402095 \\caption:  a man holding a cell phone in his hands.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 2/8946 [00:01<2:04:21,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000206587 \\caption:  a blue and blue train is flying through a grassy area.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 3/8946 [00:02<1:54:47,  1.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000484835 \\caption:  a bathroom with a sink and a sink in a bathroom.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 4/8946 [00:03<1:53:42,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000473261 \\caption:   a young girl in a blue shirt is sitting at a computer .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 5/8946 [00:03<1:52:54,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000162855 \\caption:  a young man is throwing a frisbee in a field.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 6/8946 [00:04<1:52:36,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  3052038928 \\caption:  a couple of people are playing wii while playing video games.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 7/8946 [00:05<1:44:19,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000189295 \\caption:  a traffic light is shown on a street light.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 8/8946 [00:05<1:28:52,  1.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  4439517165 \\caption:   two people walk through a building\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 9/8946 [00:06<1:33:27,  1.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000509620 \\caption:  a double decker bus is parked next to a bus.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 10/8946 [00:07<1:36:43,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000289263 \\caption:  a woman in a bikini bikini bikini holding a surfboard.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 11/8946 [00:07<1:51:41,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  2069279767 \\caption:   a man and a woman sitting on a red and red chair on a red wall .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 12/8946 [00:08<1:51:58,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000333621 \\caption:  a laptop computer sitting on a desk with a laptop on it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 13/8946 [00:09<1:47:15,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000005757 \\caption:  a school bus is parked in front of a bus.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 14/8946 [00:09<1:41:28,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000141207 \\caption:  a blue and white bus parked outside a building.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 15/8946 [00:10<1:37:44,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  2617812188 \\caption:   a woman is painting a sculpture of a sculpture .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 16/8946 [00:11<1:34:58,  1.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000092342 \\caption:  a man is playing tennis on a tennis court.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 17/8946 [00:11<1:35:29,  1.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000383605 \\caption:  a baseball player swinging a bat at a baseball game.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 18/8946 [00:12<1:38:22,  1.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000394132 \\caption:  a teddy bear with a hat sitting on a table.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 19/8946 [00:13<1:37:33,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000311904 \\caption:  a woman in a bikini top holding a tennis racket.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 20/8946 [00:13<1:34:34,  1.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000040100 \\caption:  a man holding a baseball bat in his hands.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 21/8946 [00:14<1:35:05,  1.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  4727583716 \\caption:   a group of men are standing in a rail yard .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 22/8946 [00:15<1:40:11,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000062053 \\caption:  a laptop computer, keyboard, and a keyboard on a desk.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 23/8946 [00:15<1:36:33,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000030151 \\caption:  a vase with a statue of flowers on it\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 24/8946 [00:16<1:34:10,  1.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000382171 \\caption:  a black and white dog running on a path.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 25/8946 [00:17<1:37:06,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  3787451302 \\caption:   a man in a green shirt is holding a wooden axe .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 26/8946 [00:17<1:44:11,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000306627 \\caption:  a man with a mustache and glasses sitting in front of a cake.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 27/8946 [00:18<1:39:32,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000156202 \\caption:  a man is taking a shot of a man.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 28/8946 [00:19<1:45:57,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  177222949 \\caption:  a man in a white shirt and jeans is walking down the street.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 29/8946 [00:19<1:31:04,  1.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  4439517165 \\caption:   two people walk through a building\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 30/8946 [00:20<1:37:28,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000406452 \\caption:  a pair of black and white cell phones sitting on a table.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 31/8946 [00:21<1:34:41,  1.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000182736 \\caption:  a large clock tower with a large clock tower.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 32/8946 [00:21<1:37:44,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000480797 \\caption:  a boat with a bicycle and a bicycle on the water.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 33/8946 [00:22<1:44:43,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000074411 \\caption:  a fire truck with a fire engine on the side of the road.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 34/8946 [00:23<1:37:22,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000403464 \\caption:  a busy city street with cars and pedestrians.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 35/8946 [00:23<1:32:12,  1.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  3909183873 \\caption:   two musicians are playing guitar and playing guitar .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 36/8946 [00:24<1:31:09,  1.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000459465 \\caption:  a goat is standing next to a wooden fence.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 37/8946 [00:25<1:40:04,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  8170441792 \\caption:   a man in a black shirt and jeans jumps into a rock formation .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 38/8946 [00:25<1:38:59,  1.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  2173061319 \\caption:   a person in a blue skis in the snow .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 39/8946 [00:26<1:43:28,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000091996 \\caption:   a man in a blue shirt is standing in a living room .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 40/8946 [00:27<1:43:58,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000022051 \\caption:  a group of young boys playing tennis on a tennis court.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 41/8946 [00:27<1:49:16,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000181850 \\caption:  a person is holding a fork in front of a plate with food.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 42/8946 [00:28<1:48:03,  1.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000518194 \\caption:  a slice of pizza is on a plate with a fork.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 43/8946 [00:29<1:42:24,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000573483 \\caption:  a clock on a wall with a clock on it\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 44/8946 [00:30<1:45:45,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000116557 \\caption:  a person holding a hot dog with a hot dog on it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 45/8946 [00:30<1:43:20,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000296871 \\caption:  a cat sitting on a table with a beer bottle.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 46/8946 [00:31<1:44:01,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000137573 \\caption:  a table with a computer and a computer on the floor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 47/8946 [00:32<1:39:31,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  4563139415 \\caption:  a woman is standing in front of a building .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 48/8946 [00:32<1:39:08,  1.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000217495 \\caption:  a person with a dog standing next to a person.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 49/8946 [00:33<1:43:34,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000082312 \\caption:  a woman in a red hat is standing next to a bicycle.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 50/8946 [00:34<1:39:15,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000495615 \\caption:  a person dressed in a costume with a red ribbon\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 51/8946 [00:35<1:56:00,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  4209480025 \\caption:   a man in a black hat is walking down a street with a snow covered in snow .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 52/8946 [00:36<2:10:23,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  2815256108 \\caption:   a man in a white shirt and white shirt standing in front of a yellow and white shop .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 53/8946 [00:36<2:00:31,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000459347 \\caption:   a man flying a kite on a grassy hill\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 54/8946 [00:37<2:03:32,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000014475 \\caption:  a teddy bear is sitting on a table with a large stuffed animal.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 55/8946 [00:38<2:03:17,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  177222949 \\caption:  a man in a white shirt and jeans is walking down the street.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 56/8946 [00:39<1:58:21,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000205757 \\caption:  a close up of a piece of bread and a sandwich.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 57/8946 [00:39<1:37:34,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000051530 \\caption:   a man with a hat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 58/8946 [00:40<1:37:43,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000240655 \\caption:  a snowboarder in the snow on a ramp.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 59/8946 [00:41<1:40:30,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000113897 \\caption:  a white toilet sitting in a bathroom next to a toilet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 60/8946 [00:41<1:30:01,  1.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  4014757090 \\caption:   two men walk down the street .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 61/8946 [00:41<1:25:03,  1.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000480223 \\caption:  a white plane flying through the sky.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 62/8946 [00:42<1:36:36,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000559768 \\caption:  a sandwich and a sandwich on a plate with a drink in it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 63/8946 [00:43<1:39:33,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000373792 \\caption:  a herd of sheep are in a field with a fence.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 64/8946 [00:44<1:41:40,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000031865 \\caption:  two cats looking out of a window looking out the window.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 65/8946 [00:45<1:48:13,  1.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000382557 \\caption:  a group of people standing around a table with a large framed picture.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 66/8946 [00:45<1:45:26,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000536252 \\caption:  an elephant is walking through the foliage in the jungle.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 67/8946 [00:46<1:46:00,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000334939 \\caption:  a plate of food is on a plate with a salad.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 68/8946 [00:47<1:41:22,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000417303 \\caption:   people walk down a busy street with people walking .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 69/8946 [00:47<1:48:01,  1.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000284144 \\caption:  a person is sitting in front of a tv in a large room.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 70/8946 [00:48<1:42:40,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000416165 \\caption:  a building with a clock tower and a tower.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 71/8946 [00:49<1:46:34,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000055389 \\caption:  a person holding a teddy bear with a small pink ribbon.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 72/8946 [00:49<1:44:16,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000310136 \\caption:  a baseball player is throwing a baseball on the mound.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 73/8946 [00:50<1:45:09,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000549168 \\caption:  a plate of food is on a plate with a fork.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 74/8946 [00:51<1:45:46,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000373923 \\caption:  a woman in blue skiing gear is skiing on the snow.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 75/8946 [00:52<1:48:50,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  211402278 \\caption:   a man and a woman are standing in a field with sheep .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 76/8946 [00:52<1:35:55,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  3089992945 \\caption:   a man is playing a guitar .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 77/8946 [00:53<1:44:17,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000047916 \\caption:  a small dog is playing with a frisbee in the grass.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 78/8946 [00:54<1:58:07,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000451131 \\caption:  a picture of a person in a chair with a small table in front of it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 79/8946 [00:55<1:54:52,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000063856 \\caption:   a skateboarder is doing tricks on a skateboard .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 80/8946 [00:55<1:50:17,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000081784 \\caption:  a man is on the beach holding a surfboard.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 81/8946 [00:56<1:47:06,  1.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000213023 \\caption:  a zebra is standing in a grassy area.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 82/8946 [00:57<1:49:48,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000181330 \\caption:  a large metal suitcase is sitting on a bench near a bench.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 83/8946 [00:58<1:49:09,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000283382 \\caption:  a kitchen with a sink and sink next to a sink.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 84/8946 [00:58<1:56:32,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  7130336193 \\caption:  a group of people playing a game of soccer on a grassy field.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 85/8946 [00:59<1:56:36,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000216820 \\caption:  a plate of fruit is on a plate with bananas and oranges.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 86/8946 [01:00<1:53:59,  1.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000345160 \\caption:  a woman and a child are sitting on a motorbike.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 87/8946 [01:01<1:54:54,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000307989 \\caption:  two cats are sitting on a wooden tray next to a bowl.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 88/8946 [01:01<1:50:24,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000217393 \\caption:  a man standing next to a deer in a pasture.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 89/8946 [01:02<1:42:19,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000447879 \\caption:  a bird is perched on a tree branch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 90/8946 [01:02<1:29:11,  1.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  5109882423 \\caption:   a man dressed in traditional clothing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 91/8946 [01:03<1:34:57,  1.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000556892 \\caption:  a man laying on the beach laying on a surfboard.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 92/8946 [01:04<1:46:50,  1.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000241918 \\caption:  a man in a blue jacket and ski pants standing on a snowy slope.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 93/8946 [01:05<1:42:27,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000545950 \\caption:  a man riding a horse on a dirt road.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 94/8946 [01:05<1:41:52,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  118717792 \\caption:   a group of people are playing drums in a park .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 95/8946 [01:06<1:41:31,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000361253 \\caption:  a pair of scissors sitting on top of a paper.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 96/8946 [01:07<1:43:50,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000139040 \\caption:  a man in a blue helmet is sitting on a motorcycle.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 97/8946 [01:08<1:48:16,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000305451 \\caption:  a man and a woman are playing a game on a table.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 98/8946 [01:08<1:45:54,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000330356 \\caption:  a giraffe is standing in a grassy field.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 99/8946 [01:09<1:39:17,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000169683 \\caption:  a herd of sheep grazing in a field.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 100/8946 [01:10<1:57:39,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000126263 \\caption:  a black and white photo of a black and white photo of a black and white dog.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 101/8946 [01:11<1:59:59,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000408425 \\caption:  a room with a bed, chair, chair, and a chair.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 102/8946 [01:11<1:48:59,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000100667 \\caption:  two men are holding bananas on their fingers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 103/8946 [01:12<1:48:30,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000018183 \\caption:  a clock is lit up on the wall of a building.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 104/8946 [01:13<1:46:01,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000206731 \\caption:  a blue teddy bear is sitting on a counter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 105/8946 [01:13<1:41:23,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  3315250232 \\caption:  a little girl running on a grassy field.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 106/8946 [01:14<1:43:34,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000072396 \\caption:  a woman in a blue shirt is sitting at a table.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 107/8946 [01:15<1:42:25,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file name:  000000329175 \\caption:  a skateboarder doing a trick on a ramp.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CIDEr & CLIPScore"
      ],
      "metadata": {
        "id": "VGr3qxH5quie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown 1SUiRrG6zQVtyrVSVh9hOBq5_fX-oV2Lh -O hw3_data.zip # 11rP6KmR5Qwjhx0rfag0b5TZGBTRuPtQR\n",
        "# !unzip /content/hw3_data.zip"
      ],
      "metadata": {
        "id": "L1bN1UeOP6bN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# with open('/content/large_epoch6_output.json', 'r') as file:\n",
        "#     data = json.load(file)"
      ],
      "metadata": {
        "id": "Kxn5Fa3dP61z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown 1VKQV8b5IIL6AWXnNlOXe45_njmUDyWCP -O large_epoch5_output.json"
      ],
      "metadata": {
        "id": "tuAxalXQP8bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "1OTLFea5quTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install git+https://github.com/bckim92/language-evaluation.git\n",
        "# !python -c \"import language_evaluation; language_evaluation.download('coco')\""
      ],
      "metadata": {
        "id": "OawD8CsqY1xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python3 /content/p2_evaluate.py --pred_file /content/large_epoch6_output.json --annotation_file /content/hw3_data/p2_data/val.json --images_root /content/hw3_data/p2_data/images/val"
      ],
      "metadata": {
        "id": "swSotJrRqxS2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}