{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b2a0d506e2e04803992bf87364ed43f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef59e9fb421f407681f15bb750685c1d",
              "IPY_MODEL_0332f6309ba74a8f9ba3d6570a738cec",
              "IPY_MODEL_4dde676efdd043ce883e9f3dca791e80"
            ],
            "layout": "IPY_MODEL_8a8fa90e26a74d18a750586f74deecf3"
          }
        },
        "ef59e9fb421f407681f15bb750685c1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ffe39a7f7174c5fba2834cc02cbc45b",
            "placeholder": "​",
            "style": "IPY_MODEL_5c0e5080613340f996d35dacd874e5f6",
            "value": "model.safetensors: 100%"
          }
        },
        "0332f6309ba74a8f9ba3d6570a738cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_412c03a4d8034774a9a5d9c243991d1c",
            "max": 410397786,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6829079eb2af4eeba9049ffa240d8362",
            "value": 410397786
          }
        },
        "4dde676efdd043ce883e9f3dca791e80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97f885edb8494c30b4d7e4763103d73e",
            "placeholder": "​",
            "style": "IPY_MODEL_4a1dd5145f944d3d8d2589f8679effbf",
            "value": " 410M/410M [00:02&lt;00:00, 244MB/s]"
          }
        },
        "8a8fa90e26a74d18a750586f74deecf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ffe39a7f7174c5fba2834cc02cbc45b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c0e5080613340f996d35dacd874e5f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "412c03a4d8034774a9a5d9c243991d1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6829079eb2af4eeba9049ffa240d8362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "97f885edb8494c30b4d7e4763103d73e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a1dd5145f944d3d8d2589f8679effbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R12942159/NTU_DLCV/blob/Hw3/p2_Image_caption.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "Xb59e3Ej2GPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import timm\n",
        "import json\n",
        "import torch\n",
        "import collections\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as tr\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "SBi1aSqPhMuD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Eg4VzCElm83",
        "outputId": "c16b2df9-fd36-4a9d-806f-810ed2f5cbdb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download dataset and unzip zip file."
      ],
      "metadata": {
        "id": "Q22ZX1_VhKiK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DG6GOxEHD9l"
      },
      "outputs": [],
      "source": [
        "!gdown 11rP6KmR5Qwjhx0rfag0b5TZGBTRuPtQR -O hw3_data.zip\n",
        "!unzip /content/hw3_data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizer ('<|endoftext|>', 50256) -> 250dim"
      ],
      "metadata": {
        "id": "sIMyluP4OcRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BPETokenizer:\n",
        "\n",
        "    def __init__(self, encoder_file, vocab_file):\n",
        "        with open(encoder_file, 'r', encoding='utf-8') as f:\n",
        "            self.encoder = json.load(f)\n",
        "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
        "        with open(vocab_file, 'r', encoding='utf-8') as f:\n",
        "            vocab = f.read().split('\\n')[1:-1]\n",
        "        self.bpe_ranks = {tuple(line.split()): i for i, line in enumerate(vocab)}\n",
        "        assert len(self.encoder) == 50257 and len(self.bpe_ranks) == 49999 # len(self.bpe_ranks) == 50000\n",
        "        bs = list(range(33, 127)) + list(range(161, 256))\n",
        "        xs = list(range(0, 33)) + list(range(127, 161))\n",
        "        cs = bs[:] + [2**8 + i for i in range(len(xs))]\n",
        "        self.byte_encoder = dict(zip(bs + xs, [chr(n) for n in cs]))\n",
        "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
        "\n",
        "    def encode(self, text, allowed_special=None):\n",
        "        tokens = re.findall(r\"\"\"<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d| ?\"\"\" +\n",
        "                            r\"\"\"\\w+| ?\\d+| ?[^\\s\\w\\d]+|\\s+(?!\\S)|\\s+\"\"\", text, re.UNICODE)\n",
        "        def translate(token):\n",
        "            if token == '<|endoftext|>':\n",
        "                assert allowed_special and token in allowed_special\n",
        "                return [token]\n",
        "            word = tuple(''.join(self.byte_encoder[byte] for byte in token.encode('utf-8')))\n",
        "            while len(word) != 1:\n",
        "                pairs = set((word[i], word[i+1]) for i in range(len(word)-1))\n",
        "                bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "                if bigram not in self.bpe_ranks:\n",
        "                    break\n",
        "                a, b = bigram\n",
        "                new_word = []\n",
        "                i = 0\n",
        "                while i < len(word):\n",
        "                    j = word.index(a, i) if a in word[i:] else len(word)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                    if i < len(word):\n",
        "                        j = 2 if i < len(word)-1 and word[i] == a and word[i+1] == b else 1\n",
        "                        new_word.append(a+b if j == 2 else word[i])\n",
        "                        i += j\n",
        "                word = tuple(new_word)\n",
        "            return word\n",
        "        return [self.encoder[_] for token in tokens for _ in translate(token)]\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        tokens = [self.decoder[token] for token in tokens]\n",
        "        buffer = bytearray([self.byte_decoder[c] for c in ''.join(tokens)])\n",
        "        return buffer.decode('utf-8', errors='replace')"
      ],
      "metadata": {
        "id": "gjc6poP1OdBM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = BPETokenizer('/content/encoder.json', '/content/vocab.bpe')\n",
        "prompt = 'a kitchen with a sink and many cooking machines and a pot of food'\n",
        "\n",
        "text_embedding_len = 250\n",
        "\n",
        "context = encoding.encode(prompt)\n",
        "context = [50256] + context + [50256]*(text_embedding_len - len(context) - 1)\n",
        "encoding.decode(context)"
      ],
      "metadata": {
        "id": "6aeCuLyJOmhP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "b187910d-58b9-4d15-a211-fd0f85b47523"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|endoftext|>a kitchen with a sink and many cooking machines and a pot of food<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define function"
      ],
      "metadata": {
        "id": "L0JYrzNqE0mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def json_load(json_path: str):\n",
        "    with open(json_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "    return data"
      ],
      "metadata": {
        "id": "z5xRsOP3h96o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def caption_with_id(json_path: str) -> list:\n",
        "    with open(json_path, 'r', encoding='utf-8') as file:\n",
        "        json_data = json.load(file)\n",
        "    data = [{'caption': row['caption'], 'image_id': row['image_id']} for row in json_data['annotations']]\n",
        "    return data"
      ],
      "metadata": {
        "id": "vVR-onWwmbCI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def id2file_name(json_path: str) -> dict:\n",
        "    with open(json_path, 'r', encoding='utf-8') as file:\n",
        "        json_data = json.load(file)\n",
        "    data = {row['id']: row['file_name'] for row in json_data['images']}\n",
        "    return data"
      ],
      "metadata": {
        "id": "rWW6y2Q1o_t-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_joson_path = '/content/encoder.json'\n",
        "vocab_bpe_path = '/content/vocab.bpe'\n",
        "def collate_fn(batch, tokenizer=BPETokenizer(encoder_joson_path, vocab_bpe_path)):\n",
        "    # Get the individual elements of the batch\n",
        "    images = [item['img'] for item in batch]\n",
        "    captions = [item['caption'] for item in batch]\n",
        "    filenames = [item['filename'] for item in batch]\n",
        "\n",
        "    # Tokenize captions\n",
        "    tokenized_captions = [tokenizer.encode(caption) for caption in captions]\n",
        "\n",
        "    # Pad the vector length into stop token to dimension 250\n",
        "    text_len = 250 # text_embedding_len\n",
        "    tokenized_captions = [\n",
        "        [50256] + caption + [50256] * (text_len - len(caption) - 1) for caption in tokenized_captions\n",
        "    ]\n",
        "\n",
        "    # Convert tokenized captions to PyTorch tensors\n",
        "    tokenized_captions = [torch.tensor(caption) for caption in tokenized_captions]\n",
        "\n",
        "    # Create a new batch with tokenized captions\n",
        "    tokenized_batch = {\n",
        "        'img': torch.stack(images, dim=0),\n",
        "        'tokenized_captions': torch.stack(tokenized_captions, dim=0),\n",
        "        'filename': filenames\n",
        "    }\n",
        "\n",
        "    return tokenized_batch"
      ],
      "metadata": {
        "id": "ZhTVXCheOYm6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build Dataset"
      ],
      "metadata": {
        "id": "7TWHXZgugWOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImgCaptionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir, json_path, transform) -> None:\n",
        "        super(ImgCaptionDataset, self).__init__()\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Connect caption -> image_id -> file_name\n",
        "        self.caption_with_id = caption_with_id(json_path)\n",
        "        self.id2file_name = id2file_name(json_path)\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.caption_with_id)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        caption_id = self.caption_with_id[idx]\n",
        "        file_name = self.id2file_name[caption_id['image_id']]\n",
        "        img = Image.open(os.path.join(self.img_dir, file_name)).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        return {'img': img, 'caption': caption_id['caption'], 'filename': os.path.splitext(file_name)[0]}"
      ],
      "metadata": {
        "id": "qJq6lmbBlbaM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImgDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root: str, transform) -> None:\n",
        "        self.transform = transform\n",
        "        self.img_path = [i for i in Path(root).glob(\"*.jpg\")]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.img_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.img_path[idx]).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        return img, os.path.splitext(self.img_path[idx].name)[0]"
      ],
      "metadata": {
        "id": "lYoLLeI3gZvS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build Dataloader"
      ],
      "metadata": {
        "id": "vMk7sUiwHEPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 25\n",
        "\n",
        "train_ds = ImgCaptionDataset(\n",
        "    img_dir='/content/hw3_data/p2_data/images/train',\n",
        "    json_path='/content/hw3_data/p2_data/train.json',\n",
        "    transform=tr.Compose([\n",
        "        tr.Resize(224),\n",
        "        tr.CenterCrop(224),\n",
        "        tr.ToTensor(),\n",
        "        tr.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        ")\n",
        "val_ds = ImgCaptionDataset(\n",
        "    img_dir='/content/hw3_data/p2_data/images/train',\n",
        "    json_path='/content/hw3_data/p2_data/val.json',\n",
        "    transform=tr.Compose([\n",
        "        tr.Resize(224),\n",
        "        tr.CenterCrop(224),\n",
        "        tr.ToTensor(),\n",
        "        tr.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        ")"
      ],
      "metadata": {
        "id": "MMGD07vrHGiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0725d83-09a6-4eec-9149-8a71b9f08f32"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Config"
      ],
      "metadata": {
        "id": "Fx9an6CwEx-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "\n",
        "    def __init__(self, checkpoint=None):\n",
        "        self.n_layer = 12\n",
        "        self.n_head = 12\n",
        "        self.n_embd = 768\n",
        "        self.vocab_size = 50257\n",
        "        self.block_size = 1024\n",
        "        self.checkpoint = checkpoint"
      ],
      "metadata": {
        "id": "x39qAcvyEzgi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = Config(checkpoint='/content/hw3_data/p2_data/decoder_model.bin')"
      ],
      "metadata": {
        "id": "lLwivdnDE5eq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### timm's ViT encoder"
      ],
      "metadata": {
        "id": "HK4W-qWpE1eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = timm.create_model('vit_base_patch16_224_in21k', pretrained=True)"
      ],
      "metadata": {
        "id": "mw9GuLaaE24G",
        "outputId": "87bfa825-47bd-45c2-8746-51dc6a034f9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "b2a0d506e2e04803992bf87364ed43f6",
            "ef59e9fb421f407681f15bb750685c1d",
            "0332f6309ba74a8f9ba3d6570a738cec",
            "4dde676efdd043ce883e9f3dca791e80",
            "8a8fa90e26a74d18a750586f74deecf3",
            "0ffe39a7f7174c5fba2834cc02cbc45b",
            "5c0e5080613340f996d35dacd874e5f6",
            "412c03a4d8034774a9a5d9c243991d1c",
            "6829079eb2af4eeba9049ffa240d8362",
            "97f885edb8494c30b4d7e4763103d73e",
            "4a1dd5145f944d3d8d2589f8679effbf"
          ]
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name vit_base_patch16_224_in21k to current vit_base_patch16_224.augreg_in21k.\n",
            "  model = create_fn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/410M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2a0d506e2e04803992bf87364ed43f6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "    img = batch['img']\n",
        "    break"
      ],
      "metadata": {
        "id": "tYBaNf1TE_s2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    encoder_out = encoder.forward_features(img)"
      ],
      "metadata": {
        "id": "WP6PDoTWFBWs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_out.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKn08vpIJB-_",
        "outputId": "8957da24-b539-4872-8cd1-1c0cd681b88b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([25, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### decoder"
      ],
      "metadata": {
        "id": "w1QQ3sdc8GRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.c_attn = nn.Linear(cfg.n_embd, 3 * cfg.n_embd)\n",
        "        self.c_proj = nn.Linear(cfg.n_embd, cfg.n_embd)\n",
        "        self.n_head = cfg.n_head\n",
        "        self.n_embd = cfg.n_embd\n",
        "        size = cfg.block_size\n",
        "        self.register_buffer('bias', torch.tril(torch.ones(size, size)).view(1, 1, size, size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch, context, embedding\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        return self.c_proj((att @ v).transpose(1, 2).contiguous().view(B, T, C))\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.multihead_attn = nn.MultiheadAttention(cfg.n_embd, cfg.n_head, batch_first=True)\n",
        "\n",
        "    def forward(self, query, encoder_out):\n",
        "        \"\"\"\n",
        "        Q is the source from the decoder, K, V are the sources from the encoder.\n",
        "        Q: (N, L, Eq), where L is the target embedding dim, Eq is embed_dim and batch_first=True.\n",
        "        {K, V}: (N, L, E{k,v}), where L is the source embedding dim, E{k,v} is {k,v}_dim and batch_first=True.\n",
        "        \"\"\"\n",
        "        attn_output, attn_output_weights = self.multihead_attn(query, encoder_out, encoder_out)\n",
        "        return attn_output # , attn_output_weights\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(cfg.n_embd)\n",
        "        self.ln_2 = nn.LayerNorm(cfg.n_embd) # add\n",
        "        self.ln_3 = nn.LayerNorm(cfg.n_embd)\n",
        "        self.attn = Attention(cfg)\n",
        "        self.crs_attn = CrossAttention(cfg) # add\n",
        "        self.mlp = nn.Sequential(collections.OrderedDict([\n",
        "            ('c_fc', nn.Linear(cfg.n_embd, 4 * cfg.n_embd)),\n",
        "            ('act', nn.GELU(approximate='tanh')),\n",
        "            ('c_proj', nn.Linear(4 * cfg.n_embd, cfg.n_embd))\n",
        "        ]))\n",
        "\n",
        "    def forward(self, x, encoder_out) -> Tensor: # add\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.crs_attn(self.ln_2(x), self.ln_2(encoder_out)) # add\n",
        "        x = x + self.mlp(self.ln_3(x))\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.block_size = cfg.block_size\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(cfg.vocab_size, cfg.n_embd), # 文字投影\n",
        "            wpe = nn.Embedding(cfg.block_size, cfg.n_embd), # position\n",
        "            h = nn.Sequential(*[Block(cfg) for _ in range(cfg.n_layer)]), # Nx\n",
        "            ln_f = nn.LayerNorm(cfg.n_embd)\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(cfg.n_embd, cfg.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        # timm's ViT encoder\n",
        "        self.encoder = timm.create_model('vit_base_patch16_224_in21k', pretrained=True)\n",
        "        # load checkpoint\n",
        "        if self.cfg.checkpoint is not None:\n",
        "            state_dict = torch.load(self.cfg.checkpoint)\n",
        "            transposed = [ '.c_attn.weight', '.c_fc.weight', '.c_proj.weight' ]\n",
        "            for key, value in state_dict.items():\n",
        "                if any(key.endswith(w) for w in transposed):\n",
        "                    state_dict[key] = value.t()\n",
        "            self.transformer.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    def forward(self, x: Tensor, img: Tensor) -> Tensor: # add\n",
        "        x = torch.narrow(x, 1, 0, min(x.size(1), self.block_size))\n",
        "        pos = torch.arange(x.size()[1], dtype=torch.long, device=x.device).unsqueeze(0)\n",
        "        x = self.transformer.wte(x) + self.transformer.wpe(pos)\n",
        "        with torch.no_grad():\n",
        "            encoder_out = self.encoder.forward_features(img)\n",
        "        x = [block(x, encoder_out) for block in self.transformer.h]\n",
        "        x = self.lm_head(self.transformer.ln_f(x[-1])) # add self.transformer.h(x, encoder_out)\n",
        "        return x"
      ],
      "metadata": {
        "id": "TYyI157L8JB7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(cfg)"
      ],
      "metadata": {
        "id": "W7ItaTQyFM3y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "    img = batch['img']\n",
        "    x = batch['tokenized_captions']\n",
        "    break"
      ],
      "metadata": {
        "id": "8iZRZyyLVqTa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = decoder(x, img)"
      ],
      "metadata": {
        "id": "C9WhUVX3XUNA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred.size(), x.size(), img.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VDTQFh1jIkn",
        "outputId": "66b9c0fe-2b06-42ee-e0c4-445b28860030"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([25, 250, 50257]),\n",
              " torch.Size([25, 250]),\n",
              " torch.Size([25, 3, 224, 224]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.decode(pred[0].argmax(dim=1).tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "9rVBzjCBAMC6",
        "outputId": "041907ef-cfe0-4273-b6a4-923acd8110bc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "',,,,,,,,.,\\n,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss() # ignore_index=50256\n",
        "\n",
        "loss = [loss_fn(pred[i], x[i]) for i in range(BATCH_SIZE)]"
      ],
      "metadata": {
        "id": "mZVS0SXWk7Qh"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(loss).item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkgAS7Ll4_Eu",
        "outputId": "efb9f7ed-b5e4-47e5-ab41-050652d0800b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97.69975280761719"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(pred[1].argmax(dim=1) == x[1]).sum().item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPwwDskr7e_2",
        "outputId": "04cdaf68-3bf8-4067-e751-ae3e6b3e2cb0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "rFEjEGPIj285"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset) # number of samples\n",
        "    num_batches = len(dataloader) # batches per epoch\n",
        "\n",
        "    model.train() # to training mode\n",
        "    epoch_loss, epoch_correct = 0, 0\n",
        "    loss = 0\n",
        "    for batch_i, data in enumerate(tqdm(dataloader)):\n",
        "        data['img'] = data['img'].to(device, non_blocking=True)\n",
        "        data['tokenized_captions'] = data['tokenized_captions'].to(device, non_blocking=True)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute prediction loss\n",
        "        pred = model(data['tokenized_captions'], data['img'])\n",
        "        # loss = loss_fn(pred, x)\n",
        "        loss = [loss_fn(pred[i], data['tokenized_captions'][i]) for i in range(BATCH_SIZE)]\n",
        "        loss = sum(loss)\n",
        "\n",
        "        # Optimization by gradients\n",
        "        loss.backward() # backpropagation to compute gradients\n",
        "        optimizer.step() # update model params\n",
        "\n",
        "        # write to logs\n",
        "        epoch_loss += loss.item() # tensor -> python value\n",
        "        # (N, Class)\n",
        "        pred_correct = [(pred[i].argmax(dim=1) == data['tokenized_captions'][i]).sum().item() for i in range(BATCH_SIZE)]\n",
        "        epoch_correct += sum(pred_correct)\n",
        "\n",
        "    # return avg loss of epoch, acc of epoch\n",
        "    return epoch_loss/num_batches, epoch_correct/size"
      ],
      "metadata": {
        "id": "xH8HOqOGqM47"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testing(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset) # number of samples\n",
        "    num_batches = len(dataloader) # batches per epoch\n",
        "\n",
        "    model.eval() # model to test mode.\n",
        "    epoch_loss, epoch_correct = 0, 0\n",
        "\n",
        "    # No gradient for test data\n",
        "    with torch.no_grad():\n",
        "        for batch_i, data in enumerate(tqdm(dataloader)):\n",
        "            data['img'] = data['img'].to(device, non_blocking=True)\n",
        "            data['tokenized_captions'] = data['tokenized_captions'].to(device, non_blocking=True)\n",
        "\n",
        "            # Compute prediction loss\n",
        "            pred = model(data['tokenized_captions'], data['img'])\n",
        "            # loss = loss_fn(pred, x)\n",
        "            loss = [loss_fn(pred[i], data['tokenized_captions'][i]) for i in range(BATCH_SIZE)]\n",
        "            loss = sum(loss)\n",
        "\n",
        "            # write to logs\n",
        "            epoch_loss += loss.item()\n",
        "            pred_correct = [(pred[i].argmax(dim=1) == data['tokenized_captions'][i]).sum().item() for i in range(BATCH_SIZE)]\n",
        "            epoch_correct += sum(pred_correct)\n",
        "\n",
        "    return epoch_loss/num_batches, epoch_correct/size"
      ],
      "metadata": {
        "id": "QifGUCkCsxhE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 100\n",
        "model = Decoder(cfg).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss() # ignore_index=50256\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "# Earlystopping\n",
        "patience = 3\n",
        "counter = 0\n",
        "best_loss = np.inf\n",
        "\n",
        "# logs\n",
        "logs = {\n",
        "    'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []\n",
        "}\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    train_loss, train_acc = training(train_loader, model, loss_fn, optimizer)\n",
        "    val_loss, val_acc = testing(val_loader, model, loss_fn)\n",
        "\n",
        "    # # Note that step should be called after test_epoch()\n",
        "    # schedule.step(val_loss)\n",
        "\n",
        "    print(f'EPOCH: {epoch:04d} \\\n",
        "    train_loss: {train_loss:.4f}, train_acc: {train_acc:.3f} \\\n",
        "    val_loss: {val_loss:.4f}, val_acc: {val_acc:.3f} ')\n",
        "\n",
        "    logs['train_loss'].append(train_loss)\n",
        "    logs['train_acc'].append(train_acc)\n",
        "    logs['val_loss'].append(val_loss)\n",
        "    logs['val_acc'].append(val_acc)\n",
        "\n",
        "    # Save model\n",
        "    # torch.save(model.state_dict(), os.path.join('/content', 'last.pth'))\n",
        "\n",
        "    # chcek improvement\n",
        "    if val_loss < best_loss:\n",
        "        counter = 0\n",
        "        best_loss = val_loss\n",
        "        torch.save(model.state_dict(), os.path.join('/content', 'best.pth'))\n",
        "    else:\n",
        "        counter += 1\n",
        "    if counter >= patience:\n",
        "        print(\"Earlystop!\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEguocg8j4qa",
        "outputId": "32228834-f1f4-4c78-f8f4-c740cde54b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]\n",
            "  0%|          | 0/2122 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 1/2122 [00:11<6:31:50, 11.08s/it]\u001b[A\n",
            "  0%|          | 2/2122 [00:12<3:16:20,  5.56s/it]\u001b[A\n",
            "  0%|          | 3/2122 [00:14<2:14:26,  3.81s/it]\u001b[A\n",
            "  0%|          | 4/2122 [00:16<1:45:27,  2.99s/it]\u001b[A\n",
            "  0%|          | 5/2122 [00:17<1:29:35,  2.54s/it]\u001b[A\n",
            "  0%|          | 6/2122 [00:19<1:20:07,  2.27s/it]\u001b[A\n",
            "  0%|          | 7/2122 [00:21<1:14:00,  2.10s/it]\u001b[A\n",
            "  0%|          | 8/2122 [00:23<1:09:53,  1.98s/it]\u001b[A\n",
            "  0%|          | 9/2122 [00:24<1:07:23,  1.91s/it]\u001b[A\n",
            "  0%|          | 10/2122 [00:26<1:05:37,  1.86s/it]\u001b[A\n",
            "  1%|          | 11/2122 [00:28<1:04:29,  1.83s/it]\u001b[A\n",
            "  1%|          | 12/2122 [00:30<1:03:47,  1.81s/it]\u001b[A\n",
            "  1%|          | 13/2122 [00:32<1:03:15,  1.80s/it]\u001b[A\n",
            "  1%|          | 14/2122 [00:33<1:02:55,  1.79s/it]\u001b[A\n",
            "  1%|          | 15/2122 [00:35<1:02:47,  1.79s/it]\u001b[A\n",
            "  1%|          | 16/2122 [00:37<1:02:46,  1.79s/it]\u001b[A\n",
            "  1%|          | 17/2122 [00:39<1:02:36,  1.78s/it]\u001b[A\n",
            "  1%|          | 18/2122 [00:40<1:02:32,  1.78s/it]\u001b[A\n",
            "  1%|          | 19/2122 [00:42<1:02:40,  1.79s/it]\u001b[A\n",
            "  1%|          | 20/2122 [00:44<1:02:43,  1.79s/it]\u001b[A\n",
            "  1%|          | 21/2122 [00:46<1:02:38,  1.79s/it]\u001b[A\n",
            "  1%|          | 22/2122 [00:48<1:02:38,  1.79s/it]\u001b[A\n",
            "  1%|          | 23/2122 [00:49<1:02:45,  1.79s/it]\u001b[A\n",
            "  1%|          | 24/2122 [00:51<1:02:44,  1.79s/it]\u001b[A\n",
            "  1%|          | 25/2122 [00:53<1:02:39,  1.79s/it]\u001b[A\n",
            "  1%|          | 26/2122 [00:55<1:02:41,  1.79s/it]\u001b[A\n",
            "  1%|▏         | 27/2122 [00:57<1:02:42,  1.80s/it]\u001b[A\n",
            "  1%|▏         | 28/2122 [00:58<1:02:42,  1.80s/it]\u001b[A\n",
            "  1%|▏         | 29/2122 [01:00<1:02:46,  1.80s/it]\u001b[A\n",
            "  1%|▏         | 30/2122 [01:02<1:02:48,  1.80s/it]\u001b[A\n",
            "  1%|▏         | 31/2122 [01:04<1:02:52,  1.80s/it]\u001b[A\n",
            "  2%|▏         | 32/2122 [01:06<1:03:06,  1.81s/it]\u001b[A\n",
            "  2%|▏         | 33/2122 [01:07<1:03:40,  1.83s/it]\u001b[A\n",
            "  2%|▏         | 34/2122 [01:09<1:03:34,  1.83s/it]\u001b[A\n",
            "  2%|▏         | 35/2122 [01:11<1:03:51,  1.84s/it]\u001b[A\n",
            "  2%|▏         | 36/2122 [01:13<1:03:45,  1.83s/it]\u001b[A\n",
            "  2%|▏         | 37/2122 [01:15<1:03:53,  1.84s/it]\u001b[A\n",
            "  2%|▏         | 38/2122 [01:17<1:03:42,  1.83s/it]\u001b[A\n",
            "  2%|▏         | 39/2122 [01:19<1:03:57,  1.84s/it]\u001b[A\n",
            "  2%|▏         | 40/2122 [01:20<1:04:02,  1.85s/it]\u001b[A\n",
            "  2%|▏         | 41/2122 [01:22<1:04:08,  1.85s/it]\u001b[A\n",
            "  2%|▏         | 42/2122 [01:24<1:03:51,  1.84s/it]\u001b[A\n",
            "  2%|▏         | 43/2122 [01:26<1:03:56,  1.85s/it]\u001b[A\n",
            "  2%|▏         | 44/2122 [01:28<1:03:43,  1.84s/it]\u001b[A\n",
            "  2%|▏         | 45/2122 [01:30<1:03:46,  1.84s/it]\u001b[A\n",
            "  2%|▏         | 46/2122 [01:31<1:03:53,  1.85s/it]\u001b[A\n",
            "  2%|▏         | 47/2122 [01:33<1:03:52,  1.85s/it]\u001b[A\n",
            "  2%|▏         | 48/2122 [01:35<1:04:12,  1.86s/it]\u001b[A\n",
            "  2%|▏         | 49/2122 [01:37<1:04:09,  1.86s/it]\u001b[A\n",
            "  2%|▏         | 50/2122 [01:39<1:04:18,  1.86s/it]\u001b[A\n",
            "  2%|▏         | 51/2122 [01:41<1:04:13,  1.86s/it]\u001b[A\n",
            "  2%|▏         | 52/2122 [01:43<1:04:37,  1.87s/it]\u001b[A\n",
            "  2%|▏         | 53/2122 [01:45<1:04:50,  1.88s/it]\u001b[A\n",
            "  3%|▎         | 54/2122 [01:46<1:05:09,  1.89s/it]\u001b[A\n",
            "  3%|▎         | 55/2122 [01:48<1:05:00,  1.89s/it]\u001b[A\n",
            "  3%|▎         | 56/2122 [01:50<1:05:29,  1.90s/it]\u001b[A\n",
            "  3%|▎         | 57/2122 [01:52<1:05:29,  1.90s/it]\u001b[A\n",
            "  3%|▎         | 58/2122 [01:54<1:05:34,  1.91s/it]\u001b[A\n",
            "  3%|▎         | 59/2122 [01:56<1:05:46,  1.91s/it]\u001b[A\n",
            "  3%|▎         | 60/2122 [01:58<1:05:39,  1.91s/it]\u001b[A\n",
            "  3%|▎         | 61/2122 [02:00<1:05:52,  1.92s/it]\u001b[A\n",
            "  3%|▎         | 62/2122 [02:02<1:05:50,  1.92s/it]\u001b[A\n",
            "  3%|▎         | 63/2122 [02:04<1:05:57,  1.92s/it]\u001b[A\n",
            "  3%|▎         | 64/2122 [02:06<1:05:57,  1.92s/it]\u001b[A\n",
            "  3%|▎         | 65/2122 [02:08<1:05:37,  1.91s/it]\u001b[A\n",
            "  3%|▎         | 66/2122 [02:09<1:05:36,  1.91s/it]\u001b[A\n",
            "  3%|▎         | 67/2122 [02:11<1:05:18,  1.91s/it]\u001b[A\n",
            "  3%|▎         | 68/2122 [02:13<1:05:07,  1.90s/it]\u001b[A\n",
            "  3%|▎         | 69/2122 [02:15<1:04:46,  1.89s/it]\u001b[A\n",
            "  3%|▎         | 70/2122 [02:17<1:04:37,  1.89s/it]\u001b[A\n",
            "  3%|▎         | 71/2122 [02:19<1:04:25,  1.88s/it]\u001b[A\n",
            "  3%|▎         | 72/2122 [02:21<1:04:19,  1.88s/it]\u001b[A\n",
            "  3%|▎         | 73/2122 [02:23<1:04:09,  1.88s/it]\u001b[A\n",
            "  3%|▎         | 74/2122 [02:25<1:04:09,  1.88s/it]\u001b[A\n",
            "  4%|▎         | 75/2122 [02:26<1:04:08,  1.88s/it]\u001b[A\n",
            "  4%|▎         | 76/2122 [02:28<1:03:58,  1.88s/it]\u001b[A\n",
            "  4%|▎         | 77/2122 [02:30<1:03:41,  1.87s/it]\u001b[A\n",
            "  4%|▎         | 78/2122 [02:32<1:03:34,  1.87s/it]\u001b[A\n",
            "  4%|▎         | 79/2122 [02:34<1:03:34,  1.87s/it]\u001b[A\n",
            "  4%|▍         | 80/2122 [02:36<1:03:37,  1.87s/it]\u001b[A\n",
            "  4%|▍         | 81/2122 [02:38<1:03:37,  1.87s/it]\u001b[A\n",
            "  4%|▍         | 82/2122 [02:39<1:03:26,  1.87s/it]\u001b[A\n",
            "  4%|▍         | 83/2122 [02:41<1:03:27,  1.87s/it]\u001b[A\n",
            "  4%|▍         | 84/2122 [02:43<1:03:28,  1.87s/it]\u001b[A\n",
            "  4%|▍         | 85/2122 [02:45<1:03:21,  1.87s/it]\u001b[A\n",
            "  4%|▍         | 86/2122 [02:47<1:03:26,  1.87s/it]\u001b[A\n",
            "  4%|▍         | 87/2122 [02:49<1:03:27,  1.87s/it]\u001b[A\n",
            "  4%|▍         | 88/2122 [02:51<1:03:28,  1.87s/it]\u001b[A\n",
            "  4%|▍         | 89/2122 [02:53<1:03:34,  1.88s/it]\u001b[A\n",
            "  4%|▍         | 90/2122 [02:54<1:03:31,  1.88s/it]\u001b[A\n",
            "  4%|▍         | 91/2122 [02:56<1:03:31,  1.88s/it]\u001b[A\n",
            "  4%|▍         | 92/2122 [02:58<1:03:36,  1.88s/it]\u001b[A\n",
            "  4%|▍         | 93/2122 [03:00<1:03:22,  1.87s/it]\u001b[A\n",
            "  4%|▍         | 94/2122 [03:02<1:03:32,  1.88s/it]\u001b[A\n",
            "  4%|▍         | 95/2122 [03:04<1:03:41,  1.89s/it]\u001b[A\n",
            "  5%|▍         | 96/2122 [03:06<1:03:47,  1.89s/it]\u001b[A\n",
            "  5%|▍         | 97/2122 [03:08<1:03:58,  1.90s/it]\u001b[A\n",
            "  5%|▍         | 98/2122 [03:10<1:03:54,  1.89s/it]\u001b[A\n",
            "  5%|▍         | 99/2122 [03:11<1:03:47,  1.89s/it]\u001b[A\n",
            "  5%|▍         | 100/2122 [03:13<1:03:34,  1.89s/it]\u001b[A\n",
            "  5%|▍         | 101/2122 [03:15<1:03:43,  1.89s/it]\u001b[A\n",
            "  5%|▍         | 102/2122 [03:17<1:03:31,  1.89s/it]\u001b[A\n",
            "  5%|▍         | 103/2122 [03:19<1:03:36,  1.89s/it]\u001b[A\n",
            "  5%|▍         | 104/2122 [03:21<1:03:50,  1.90s/it]\u001b[A\n",
            "  5%|▍         | 105/2122 [03:23<1:03:45,  1.90s/it]\u001b[A\n",
            "  5%|▍         | 106/2122 [03:25<1:03:58,  1.90s/it]\u001b[A\n",
            "  5%|▌         | 107/2122 [03:27<1:03:37,  1.89s/it]\u001b[A\n",
            "  5%|▌         | 108/2122 [03:28<1:03:37,  1.90s/it]\u001b[A\n",
            "  5%|▌         | 109/2122 [03:30<1:03:29,  1.89s/it]\u001b[A\n",
            "  5%|▌         | 110/2122 [03:32<1:03:21,  1.89s/it]\u001b[A\n",
            "  5%|▌         | 111/2122 [03:34<1:03:32,  1.90s/it]\u001b[A\n",
            "  5%|▌         | 112/2122 [03:36<1:03:30,  1.90s/it]\u001b[A\n",
            "  5%|▌         | 113/2122 [03:38<1:03:24,  1.89s/it]\u001b[A\n",
            "  5%|▌         | 114/2122 [03:40<1:03:23,  1.89s/it]\u001b[A\n",
            "  5%|▌         | 115/2122 [03:42<1:03:28,  1.90s/it]\u001b[A\n",
            "  5%|▌         | 116/2122 [03:44<1:03:17,  1.89s/it]\u001b[A\n",
            "  6%|▌         | 117/2122 [03:46<1:03:13,  1.89s/it]\u001b[A\n",
            "  6%|▌         | 118/2122 [03:47<1:03:06,  1.89s/it]\u001b[A\n",
            "  6%|▌         | 119/2122 [03:49<1:03:01,  1.89s/it]\u001b[A\n",
            "  6%|▌         | 120/2122 [03:51<1:02:49,  1.88s/it]\u001b[A\n",
            "  6%|▌         | 121/2122 [03:53<1:02:50,  1.88s/it]\u001b[A\n",
            "  6%|▌         | 122/2122 [03:55<1:02:42,  1.88s/it]\u001b[A\n",
            "  6%|▌         | 123/2122 [03:57<1:02:39,  1.88s/it]\u001b[A\n",
            "  6%|▌         | 124/2122 [03:59<1:02:48,  1.89s/it]\u001b[A\n",
            "  6%|▌         | 125/2122 [04:01<1:02:29,  1.88s/it]\u001b[A\n",
            "  6%|▌         | 126/2122 [04:02<1:02:38,  1.88s/it]\u001b[A\n",
            "  6%|▌         | 127/2122 [04:04<1:02:47,  1.89s/it]\u001b[A\n",
            "  6%|▌         | 128/2122 [04:06<1:02:43,  1.89s/it]\u001b[A\n",
            "  6%|▌         | 129/2122 [04:08<1:02:40,  1.89s/it]\u001b[A\n",
            "  6%|▌         | 130/2122 [04:42<6:16:44, 11.35s/it]\u001b[A\n",
            "  6%|▌         | 131/2122 [10:17<59:58:11, 108.43s/it]\u001b[A\n",
            "  6%|▌         | 132/2122 [18:11<120:39:48, 218.29s/it]\u001b[A\n",
            "  6%|▋         | 133/2122 [18:32<87:53:11, 159.07s/it] \u001b[A\n",
            "  6%|▋         | 134/2122 [18:34<61:48:23, 111.92s/it]\u001b[A\n",
            "  6%|▋         | 135/2122 [18:36<43:33:12, 78.91s/it] \u001b[A\n",
            "  6%|▋         | 136/2122 [18:38<30:46:26, 55.78s/it]\u001b[A\n",
            "  6%|▋         | 137/2122 [18:39<21:50:05, 39.60s/it]\u001b[A\n",
            "  7%|▋         | 138/2122 [18:41<15:35:01, 28.28s/it]\u001b[A\n",
            "  7%|▋         | 139/2122 [18:43<11:12:38, 20.35s/it]\u001b[A\n",
            "  7%|▋         | 140/2122 [18:45<8:08:51, 14.80s/it] \u001b[A\n",
            "  7%|▋         | 141/2122 [18:47<6:00:13, 10.91s/it]\u001b[A\n",
            "  7%|▋         | 142/2122 [18:49<4:30:26,  8.20s/it]\u001b[A\n",
            "  7%|▋         | 143/2122 [18:51<3:27:29,  6.29s/it]\u001b[A\n",
            "  7%|▋         | 144/2122 [18:52<2:43:45,  4.97s/it]\u001b[A\n",
            "  7%|▋         | 145/2122 [18:54<2:13:04,  4.04s/it]\u001b[A\n",
            "  7%|▋         | 146/2122 [18:56<1:51:52,  3.40s/it]\u001b[A\n",
            "  7%|▋         | 147/2122 [18:58<1:36:42,  2.94s/it]\u001b[A\n",
            "  7%|▋         | 148/2122 [19:00<1:26:11,  2.62s/it]\u001b[A\n",
            "  7%|▋         | 149/2122 [19:02<1:18:44,  2.39s/it]\u001b[A\n",
            "  7%|▋         | 150/2122 [19:04<1:13:53,  2.25s/it]\u001b[A\n",
            "  7%|▋         | 151/2122 [19:06<1:10:24,  2.14s/it]\u001b[A\n",
            "  7%|▋         | 152/2122 [19:20<3:09:29,  5.77s/it]\u001b[A"
          ]
        }
      ]
    }
  ]
}