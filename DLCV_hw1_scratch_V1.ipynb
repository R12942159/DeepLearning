{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R12942159/DeepLearning/blob/main/DLCV_hw1_scratch_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{0: bike, 1: chair, 2: camel, 3: rabbit, 4: lamp, 5: clock,\n",
        "6: tree?, 7: fox, 8: hedgehog, 9: bed, 10: child,\n",
        "11: Dinosaur, 12: building, 13: tree?, 14: apple, 15: animal?,\n",
        "16: animal?, 17: kangaroo, 18: tree?, 19: snake, 20: beetle,\n",
        "21: cupboard, 22: mouse, 23: tree?, 24: house, 25: Wolf}"
      ],
      "metadata": {
        "id": "_8siHzDjt7q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VH3Xcmt3_mW",
        "outputId": "87fc9850-456e-45fc-e658-c9531531e324"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# Get cuda from GPU device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using: {device}\")"
      ],
      "metadata": {
        "id": "g9jZWEzm2ATp",
        "outputId": "e8e90753-7eb0-494c-abd4-894f33461220",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# search file paths\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "img_paths_train = glob('/content/drive/MyDrive/NTU_DLCV/p1_data/train_50/*.png') # *: all\n",
        "img_paths_train = glob('/content/drive/MyDrive/NTU_DLCV/p1_data/train_50/*.png') # *: all\n",
        "img_paths_val = glob('/content/drive/MyDrive/NTU_DLCV/p1_data/val_50/*.png')"
      ],
      "metadata": {
        "id": "9QOyVrCgmBW0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of images\n",
        "len(img_paths_train), len(img_paths_val)"
      ],
      "metadata": {
        "id": "E0HoCo-Cmusl",
        "outputId": "bd1e2a40-63d7-4e1f-9663-5c4144f49af1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22500, 2500)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_paths, img_size):\n",
        "        self.img_paths = img_paths\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"number of samples\"\"\"\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"read 1 sample\"\"\"\n",
        "        # Read img\n",
        "        path = self.img_paths[idx] # get img path\n",
        "        img = cv2.imread(path) # read img\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert to RGB\n",
        "        img = cv2.resize(img, (self.img_size, self.img_size)) # Resize iamge\n",
        "        # img = Image.open(path).convert('RGB') # read img\n",
        "        img = img / 255.0 # 0~255 -> 0.~1.\n",
        "\n",
        "        # Read class index\n",
        "        cls_idx = int((path.split('/')[-1]).split('_')[0])\n",
        "\n",
        "        # transform img to tensor\n",
        "        img = torch.tensor(img, dtype=torch.float)\n",
        "        img = img.permute(2, 0, 1) # (H, W, C) -> (C, H, W)\n",
        "        cls_idx = torch.tensor(cls_idx, dtype=torch.long)\n",
        "\n",
        "        return img, cls_idx"
      ],
      "metadata": {
        "id": "Y0m6VZ7ErBqS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build dataset\n",
        "IMG_SIZE = 32\n",
        "train_ds = MyDataset(img_paths_train, IMG_SIZE)\n",
        "val_ds = MyDataset(img_paths_val, IMG_SIZE)"
      ],
      "metadata": {
        "id": "gZXmPg4cs5kl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build DataLoaders\n",
        "BATCH_SIZE = 50\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, BATCH_SIZE, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "zZtjZU2lwDE4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Model class\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchsummary import summary # model summary\n",
        "\n",
        "\n",
        "class CNN_scratch(nn.Module):\n",
        "    def __init__(self, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=kernel_size, padding='same'),\n",
        "            nn.BatchNorm2d(num_features=32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size, padding='same'),\n",
        "            nn.BatchNorm2d(num_features=32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2), # img_size/2\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size, padding='same'),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size, padding='same'),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2), # img_size/4\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size, padding='same'),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size, padding='same'),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2), # img_size/8\n",
        "        )\n",
        "        self.flatten = nn.Flatten() # (128, 4, 4) -> (128*4*4)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128*4*4, 256),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 50),\n",
        "        )\n",
        "        # self.flatten = nn.Sequential(\n",
        "        #     nn.AdaptiveAvgPool2d(1),\n",
        "        #     nn.Flatten() # (128, 1, 1)\n",
        "        # )\n",
        "        # self.classifier = nn.Sequential(\n",
        "        #     nn.Linear(128, 50),\n",
        "        # )\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x) # img to feature maps\n",
        "        x = self.flatten(x) # feature maps -> feature vectors\n",
        "        x = self.classifier(x) # classification\n",
        "        return x"
      ],
      "metadata": {
        "id": "l28kdth_xO1b"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a model and move to GPU device\n",
        "model = CNN_scratch(kernel_size=3).to(device)"
      ],
      "metadata": {
        "id": "ewIVHhpC1w_U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "0IEGRzo_2LRT",
        "outputId": "67faa3e7-aba0-4e3b-87b2-a98f810d9c56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN_scratch(\n",
            "  (feature_extractor): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU()\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU()\n",
            "    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU()\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU()\n",
            "    (17): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU()\n",
            "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.3, inplace=False)\n",
            "    (1): Linear(in_features=2048, out_features=256, bias=True)\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=256, out_features=50, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model summary\n",
        "summary(model, (3, IMG_SIZE, IMG_SIZE))"
      ],
      "metadata": {
        "id": "3bAGGkfz2MYx",
        "outputId": "a765394b-1c56-472a-b771-eeb2368ede5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "          Flatten-22                 [-1, 2048]               0\n",
            "          Dropout-23                 [-1, 2048]               0\n",
            "           Linear-24                  [-1, 256]         524,544\n",
            "          Dropout-25                  [-1, 256]               0\n",
            "             ReLU-26                  [-1, 256]               0\n",
            "           Linear-27                   [-1, 50]          12,850\n",
            "================================================================\n",
            "Total params: 825,298\n",
            "Trainable params: 825,298\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.77\n",
            "Params size (MB): 3.15\n",
            "Estimated Total Size (MB): 5.93\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "mdYme-1m89UQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)"
      ],
      "metadata": {
        "id": "SuUhkYyc2vJk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm # (optional) progress bar\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset) # number of samples\n",
        "    num_batches = len(dataloader) # batches per epoch\n",
        "\n",
        "    model.train() # to training mode.\n",
        "    epoch_loss, epoch_correct = 0, 0\n",
        "    for batch_i, (x, y) in enumerate(tqdm(dataloader, leave=False)):\n",
        "        x, y = x.to(device), y.to(device) # move data to GPU\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute prediction loss\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Optimization by gradients\n",
        "        loss.backward() # backpropagation to compute gradients\n",
        "        optimizer.step() # update model params\n",
        "\n",
        "        # write to logs\n",
        "        epoch_loss += loss.item() # tensor -> python value\n",
        "        # (N, Class)\n",
        "        epoch_correct += (pred.argmax(dim=1) == y).sum().item()\n",
        "\n",
        "    # return avg loss of epoch, acc of epoch\n",
        "    return epoch_loss/num_batches, epoch_correct/size\n",
        "\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset) # number of samples\n",
        "    num_batches = len(dataloader) # batches per epoch\n",
        "\n",
        "    model.eval() # model to test mode.\n",
        "    epoch_loss, epoch_correct = 0, 0\n",
        "\n",
        "    # No gradient for test data\n",
        "    with torch.no_grad():\n",
        "        for batch_i, (x, y) in enumerate(dataloader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Compute prediction loss\n",
        "            pred = model(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "\n",
        "            # write to logs\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_correct += (pred.argmax(1) == y).sum().item()\n",
        "\n",
        "    return epoch_loss/num_batches, epoch_correct/size"
      ],
      "metadata": {
        "id": "ZtpUJven21Hg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the second last layer\n",
        "def last_layer(input_x):\n",
        "  input_x = model.feature_extractor(input_x)\n",
        "  input_x = input_x.view(input_x.size(0), -1) # flatten c*h*w to input classifier\n",
        "  model_csf = nn.Sequential(*list(model.classifier.children())[0:2]).eval()\n",
        "  input_x = model_csf(input_x)\n",
        "  return input_x"
      ],
      "metadata": {
        "id": "1uzREr59iYWL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "EPOCHS = 100\n",
        "plot_step = [i for i in range(100)]\n",
        "logs = {\n",
        "    'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []\n",
        "}\n",
        "\n",
        "# Earlystopping\n",
        "patience = 5\n",
        "counter = 0\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    train_loss, train_acc = train(train_loader, model, loss_fn, optimizer)\n",
        "    val_loss, val_acc = test(val_loader, model, loss_fn)\n",
        "\n",
        "    print(f'EPOCH: {epoch:04d} \\\n",
        "    train_loss: {train_loss:.4f}, train_acc: {train_acc:.3f} \\\n",
        "    val_loss: {val_loss:.4f}, val_acc: {val_acc:.3f} ')\n",
        "\n",
        "    logs['train_loss'].append(train_loss)\n",
        "    logs['train_acc'].append(train_acc)\n",
        "    logs['val_loss'].append(val_loss)\n",
        "    logs['val_acc'].append(val_acc)\n",
        "\n",
        "    # plot\n",
        "    if epoch in plot_step:\n",
        "        with torch.no_grad():\n",
        "            xs = None\n",
        "            ys = None\n",
        "            for batch_idx, (x, y) in enumerate(val_loader):\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                output_x = last_layer(x)\n",
        "                if xs is None:\n",
        "                   xs = output_x.detach().cpu().numpy() # 防止梯度回傳; run on cpu only\n",
        "                   ys = y.detach().cpu().numpy().flatten()\n",
        "\n",
        "                  #  print(type(xs), xs.shape, type(ys), ys.shape, sep='\\n')\n",
        "                else:\n",
        "                   output_x = output_x.detach().cpu().numpy()\n",
        "                   y = y.detach().cpu().numpy().flatten()\n",
        "                   xs = np.vstack((xs, output_x)) # 垂直堆疊\n",
        "                   ys = np.concatenate((ys, y), axis=0) # 合併\n",
        "\n",
        "                  #  print(type(output_x), output_x.shape, type(y), y.shape, type(xs), xs.shape, type(ys), ys.shape, sep='\\n')\n",
        "        xs = xs.reshape(xs.shape[0], -1)\n",
        "\n",
        "        # PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        pca_xs = pca.fit_transform(xs)\n",
        "        plt.figure()\n",
        "        plt.title(f\"PCA plot on epoch {epoch}\")\n",
        "        plt.scatter(pca_xs[:, 0], pca_xs[:, 1], c=ys)\n",
        "        plt.savefig(os.path.join(\"/content/drive/MyDrive/NTU_DLCV/p1_data/CNN result\", f\"PCA_epoch{epoch}\"), format='png')\n",
        "\n",
        "        # TSNE\n",
        "        tsne = TSNE(n_components=2, n_jobs=-1)\n",
        "        tsne_x = tsne.fit_transform(xs)\n",
        "        plt.figure()\n",
        "        plt.title(f\"TSNE plot on epoch {epoch}\")\n",
        "        plt.scatter(tsne_x[:, 0], tsne_x[:, 1], c=ys)\n",
        "        plt.savefig(os.path.join(\"/content/drive/MyDrive/NTU_DLCV/p1_data/CNN result\", f\"TSNE_epoch{epoch}\"), format='png')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    torch.save(model.state_dict(), \"last.pth\")\n",
        "    # chcek improvement\n",
        "    if val_loss < best_loss:\n",
        "        counter = 0\n",
        "        best_loss = val_loss\n",
        "        torch.save(optimizer.state_dict(), \"/content/drive/MyDrive/NTU_DLCV/p1_data/CNN result/CNN_best_optimizer.pth\")\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/NTU_DLCV/p1_data/CNN result/CNN_best_model.pth\")\n",
        "        print(\"Model saved ~\")\n",
        "    else:\n",
        "        counter += 1\n",
        "    if counter >= patience:\n",
        "        print(\"Earlystop!\")\n",
        "        break"
      ],
      "metadata": {
        "id": "2Se7vPci3d-L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}