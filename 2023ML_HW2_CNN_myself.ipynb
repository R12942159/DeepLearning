{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R12942159/NTU_DLCV/blob/main/2023ML_HW2_CNN_myself.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDVwVcC2ofHK"
      },
      "source": [
        "## ML HW2 sample code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "若有問題可直接寄信至助教信箱，環境部分推薦使用前述所提到之環境，若使用自己的電腦執行，請注意環境的相容性\n"
      ],
      "metadata": {
        "id": "-MFWlImM-3FM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lxEHor54-2WE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rV0FCmEOSz0"
      },
      "source": [
        "#### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMsNXtVLopQ2"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import glob\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torchvision import transforms as tr\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfyxtZsqrw-_"
      },
      "source": [
        "!gdown 1drrS7gnyzUJPPiQcDWcHdIXqzjy2n3yZ\n",
        "!unzip 'HW2.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "# search file paths\n",
        "def get_img_paths(path):\n",
        "  paths = [os.path.join(path, i) for i in os.listdir(path) if i.endswith('.jpg')]\n",
        "  return paths\n",
        "\n",
        "train_paths = sorted(get_img_paths('/content/data/train'))\n",
        "test_paths = sorted(get_img_paths('/content/data/test'))"
      ],
      "metadata": {
        "id": "QPhPDrjbdQkB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "img_labels = pd.read_csv('/content/data/train.csv')['label'].values.tolist()"
      ],
      "metadata": {
        "id": "5hm3QrxehWKJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "from PIL import Image\n",
        "from torchvision.transforms import v2\n",
        "from torchvision.transforms.functional import hflip, vflip\n",
        "# transforms = v2.Compose([\n",
        "#     v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
        "#     v2.RandomHorizontalFlip(p=0.5),\n",
        "#     v2.ToDtype(torch.float32, scale=True),\n",
        "#     v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "# ])\n",
        "\n",
        "class Hw2_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_path, img_label, transform, mode='train', augmentation=True) -> None:\n",
        "        self.img_path = img_path\n",
        "        if img_label == list:\n",
        "            self.img_label = img_label\n",
        "        else:\n",
        "            self.img_label = img_path\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "        def origine(x): return x\n",
        "        if augmentation:\n",
        "            self.augmentation = [origine, hflip, vflip]\n",
        "        else:\n",
        "            self.augmentation = [origine]\n",
        "\n",
        "        assert len(self.img_path) == len(self.img_label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # read img\n",
        "        path = self.img_path[idx]\n",
        "        img = Image.open(path).convert('RGB') # img = (64,64,3)\n",
        "\n",
        "        # transform/normalize img\n",
        "        img = self.transform(img)\n",
        "\n",
        "        if self.mode != 'test':\n",
        "            # augmentation img\n",
        "            augment = random.choice(self.augmentation)\n",
        "            img = augment(img)\n",
        "\n",
        "            # get label\n",
        "            label = self.img_label[idx]\n",
        "\n",
        "            return img, label\n",
        "        else:\n",
        "\n",
        "            return img, re.split(r'[./]', path)[-2]\n",
        "\n"
      ],
      "metadata": {
        "id": "rjji7KijikOU",
        "outputId": "1aab5ff9-c41d-451d-eb94-8066c6734a13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean=[0.485, 0.456, 0.406]\n",
        "std=[0.229, 0.224, 0.225]\n",
        "\n",
        "img_ds = Hw2_dataset(train_paths,\n",
        "                     img_labels,\n",
        "                     transform = tr.Compose([\n",
        "                          tr.ToTensor(),\n",
        "                          tr.Normalize(mean=mean, std=std),\n",
        "                          ]),\n",
        "                     mode = 'train',\n",
        "                     augmentation = True,)\n",
        "\n",
        "test_ds = Hw2_dataset(test_paths,\n",
        "                     None,\n",
        "                     transform = tr.Compose([\n",
        "                          tr.ToTensor(),\n",
        "                          tr.Normalize(mean=mean, std=std),\n",
        "                          ]),\n",
        "                     mode = 'test',\n",
        "                     augmentation = False,)"
      ],
      "metadata": {
        "id": "U0vcD84ssHtV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "# Randomly divided into a training and validation dataset.\n",
        "dataset_size = len(img_ds)\n",
        "val_size = int(0.2 * dataset_size)\n",
        "train_ds, val_ds = random_split(img_ds, [dataset_size - val_size, val_size])"
      ],
      "metadata": {
        "id": "43CrLsArvXUf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build dataloders\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, BATCH_SIZE*2, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(val_ds, BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "id": "syCZJQYr2uBD",
        "outputId": "4324f867-50dd-43e5-eb46-95c9574457e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_scratch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_scratch, self).__init__()\n"
      ],
      "metadata": {
        "id": "JWnmbmOWPQu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLA_KBidRF73"
      },
      "source": [
        "#### Define module class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QwewjIWP39y"
      },
      "source": [
        "class FaceExpressionNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FaceExpressionNet, self).__init__()\n",
        "        # TODO\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64, eps=1e-05, affine=True),\n",
        "            nn.LeakyReLU(negative_slope=0.05),\n",
        "            nn.MaxPool2d((2, 2)),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * 32 * 32, 7),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #image size (64,64)\n",
        "        x = self.conv(x) #(32,32)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyyjQS2eOSz7"
      },
      "source": [
        "#### Define training and testing process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFCMvUe0OSz7"
      },
      "source": [
        "def train(train_loader, model, loss_fn, use_gpu=True):\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    for (img, label) in train_loader:\n",
        "        if use_gpu:\n",
        "            img = img.to(device)\n",
        "            label = label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(img)\n",
        "        loss = loss_fn(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            predict = torch.argmax(output, dim=-1)\n",
        "            acc = np.mean((label == predict).cpu().numpy())\n",
        "            train_acc.append(acc)\n",
        "            train_loss.append(loss.item())\n",
        "    print(\"Epoch: {}, train Loss: {:.4f}, train Acc: {:.4f}\".format(epoch + 1, np.mean(train_loss), np.mean(train_acc)))\n",
        "\n",
        "def valid(valid_loader, model, loss_fn, use_gpu=True):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_loss = []\n",
        "        valid_acc = []\n",
        "        for idx, (img, label) in enumerate(valid_loader):\n",
        "            if use_gpu:\n",
        "                img = img.to(device)\n",
        "                label = label.to(device)\n",
        "            output = model(img)\n",
        "            loss = loss_fn(output, label)\n",
        "            predict = torch.argmax(output, dim=-1)\n",
        "            acc = (label == predict).cpu().tolist()\n",
        "            valid_loss.append(loss.item())\n",
        "            valid_acc += acc\n",
        "\n",
        "        valid_acc = np.mean(valid_acc)\n",
        "        valid_loss = np.mean(valid_loss)\n",
        "        print(\"Epoch: {}, valid Loss: {:.4f}, valid Acc: {:.4f}\".format(epoch + 1, valid_loss, valid_acc))\n",
        "    return valid_acc\n",
        "\n",
        "def save_checkpoint(valid_acc, acc_record, epoch, prefix='model'):\n",
        "    # you can define the condition to save model :)\n",
        "    if valid_acc >= np.mean(acc_record[-5:]):\n",
        "        checkpoint_path = f'{prefix}.pth'\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print('model saved to %s' % checkpoint_path)\n",
        "\n",
        "def better(acc_record):\n",
        "    if max(acc_record) == acc_record[-1]: return True\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBHN_XSsTF6p"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    model = FaceExpressionNet()\n",
        "    if use_gpu:\n",
        "        model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    acc_record = []\n",
        "\n",
        "    for epoch in range(NUM_ECPOCH):\n",
        "        train(train_loader, model, loss_fn, use_gpu)\n",
        "        valid_acc = valid(valid_loader, model, loss_fn, use_gpu=True)\n",
        "        acc_record.append(valid_acc)\n",
        "\n",
        "\n",
        "        if better(acc_record):\n",
        "            save_checkpoint(valid_acc, acc_record, epoch, prefix='model')\n",
        "\n",
        "        print('########################################################')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqMcl44nOSz9"
      },
      "source": [
        "def test(test_loader, model, file_name='predict.csv'):\n",
        "    with torch.no_grad():\n",
        "        predict_result = []\n",
        "        predict_name = []\n",
        "        for img, name in test_loader:\n",
        "            if use_gpu:\n",
        "                img = img.to(device)\n",
        "            output = model(img)\n",
        "            predict = torch.argmax(output, dim=-1).tolist()\n",
        "            predict_result += predict\n",
        "            predict_name += name\n",
        "\n",
        "    with open(file_name, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['id', 'label'])\n",
        "        for id, r in zip(predict_name, predict_result):\n",
        "            writer.writerow([id, r])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqtDLxvkOSz9"
      },
      "source": [
        "del model\n",
        "model = FaceExpressionNet()\n",
        "model.load_state_dict(torch.load('model.pth'))\n",
        "model = model.cuda()\n",
        "test(test_loader, model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}